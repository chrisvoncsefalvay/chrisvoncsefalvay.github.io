<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris von Csefalvay">
<meta name="dcterms.date" content="2025-12-31">
<meta name="description" content="The bubble everyone’s watching for isn’t the one that’s coming.">

<title>Five wild guesses for 2026 – Chris von Csefalvay</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a4d8066ab99c821fadc425098389dfee.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=395640625"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', '395640625', { 'anonymize_ip': true});
</script>
<script type="application/ld+json">{"@context":"http://www.schema.org","@type":"person","name":"Chris von Csefalvay","jobTitle":"Director of Biomedical AI/ML","height":"74 inches","gender":"male","description":"Chris von Csefalvay is a computational epidemiologist and data scientist working at the intersection of AI/ML, computational dynamics and public health. He is the author of Computational Modeling of Infectious Disease and a number of research papers.","url":"https://chrisvoncsefalvay.com","image":"https://chrisvoncsefalvay.com/img/IMG_5986.jpeg","address":{"@type":"PostalAddress","addressLocality":"Denver","addressRegion":"CO","postalCode":"80204","addressCountry":"United States"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University of Oxford","sameAs":"https://en.wikipedia.org/wiki/University_of_Oxford"},{"@type":"CollegeOrUniversity","name":"Cardiff University","sameAs":"https://en.wikipedia.org/wiki/Cardiff_University"}],"worksFor":[{"@type":"Organization","name":"HCLTech"}],"birthDate":"1986-07-15","birthPlace":"Budapest, Hungary","memberOf":[{"@type":"Organization","name":"Royal Society for Public Health"},{"@type":"Organization","name":"TOPRA"},{"@type":"Organization","name":"IEEE"}],"nationality":[{"@type":"Country","name":"United Kingdom"},{"@type":"Country","name":"Hungary"}]}</script>


<meta property="og:title" content="Five wild guesses for 2026 – Chris von Csefalvay">
<meta property="og:description" content="The bubble everyone’s watching for isn’t the one that’s coming.">
<meta property="og:image" content="https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/header.png">
<meta property="og:site_name" content="Chris von Csefalvay">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1024">
<meta name="twitter:title" content="Five wild guesses for 2026 – Chris von Csefalvay">
<meta name="twitter:description" content="The bubble everyone’s watching for isn’t the one that’s coming.">
<meta name="twitter:image" content="https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/header.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1024">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Five wild guesses for 2026">
<meta name="citation_author" content="Chris von Csefalvay">
<meta name="citation_publication_date" content="2025-12-31">
<meta name="citation_cover_date" content="2025-12-31">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-12-31">
<meta name="citation_fulltext_html_url" content="https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/">
<meta name="citation_language" content="en-GB">
<meta name="citation_reference" content="citation_title=Training compute-optimal large language models;,citation_author=Jordan Hoffmann;,citation_author=Sebastian Borgeaud;,citation_author=Arthur Mensch;,citation_author=Elena Buchatskaya;,citation_author=Trevor Cai;,citation_author=Eliza Rutherford;,citation_author=Diego Las Casas;,citation_author=Lisa Anne Hendricks;,citation_author=Johannes Welbl;,citation_author=Aidan Clark;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2203.15556;">
<meta name="citation_reference" content="citation_title=A path towards autonomous machine intelligence;,citation_author=Yann LeCun;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_publisher=https://openreview.net/pdf?id=BZ5a1r-kVsf;">
<meta name="citation_reference" content="citation_title=Americans have mixed views of AI and an appetite for regulation;,citation_author=Searchlight Institute;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_publisher=https://www.searchlightinstitute.org/research/americans-have-mixed-views-of-ai-and-an-appetite-for-regulation/;">
<meta name="citation_reference" content="citation_title=SEO poisoning LLMs;,citation_author=ZeroFox;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_publisher=https://www.zerofox.com/blog/seo-poisoning-llms/;">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Chris von Csefalvay</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers"> 
<span class="menu-text">Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../media"> 
<span class="menu-text">Media</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">The Notebook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://computationalinfectiousdisease.com"> 
<span class="menu-text">Computational Modeling of Infectious Disease</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://craftofposttraining.com"> 
<span class="menu-text">The Craft of Post-Training</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-pace-we-cannot-keep" id="toc-the-pace-we-cannot-keep" class="nav-link active" data-scroll-target="#the-pace-we-cannot-keep">The pace we cannot keep</a></li>
  <li><a href="#beyond-the-dialogue" id="toc-beyond-the-dialogue" class="nav-link" data-scroll-target="#beyond-the-dialogue">Beyond the dialogue</a></li>
  <li><a href="#the-last-fixed-screen" id="toc-the-last-fixed-screen" class="nav-link" data-scroll-target="#the-last-fixed-screen">The last fixed screen</a></li>
  <li><a href="#what-were-actually-running-out-of" id="toc-what-were-actually-running-out-of" class="nav-link" data-scroll-target="#what-were-actually-running-out-of">What we’re actually running out of</a></li>
  <li><a href="#the-retrieval-delusion" id="toc-the-retrieval-delusion" class="nav-link" data-scroll-target="#the-retrieval-delusion">The retrieval delusion</a></li>
  <li><a href="#three-battles-in-the-background" id="toc-three-battles-in-the-background" class="nav-link" data-scroll-target="#three-battles-in-the-background">Three battles in the background</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Five wild guesses for 2026</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLMs</div>
    <div class="quarto-category">AI</div>
    <div class="quarto-category">tech predictions</div>
  </div>
  </div>

<div>
  <div class="description">
    The bubble everyone’s watching for isn’t the one that’s coming.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris von Csefalvay <a href="mailto:chris@chrisvoncsefalvay.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-3131-0864" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">31 December 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>There’s an old parable about a drunk searching for his keys under a streetlamp. A passerby stops to help, and after some fruitless searching asks, “Are you sure you lost them here?” The drunk replies, “No, I lost them in the alley. But the light’s better here.”</p>
<p>I think about this story a lot when I read AI predictions. <a href="../../posts/five-wild-guesses/index.html">Last year, I made five predictions for 2025</a>, and they held up reasonably well.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> This year, the crystal ball feels murkier, and maybe a little darker.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The productisation of AI governance happened faster than even I expected. The insurance industry is still figuring out its role, but the actuarial interest is undeniable.</p></div></div><p>Everyone is looking for an AI bubble. And in a sense, they’re right to look – something is unsustainable, something will give. But I’m increasingly convinced that the bubble hunters are searching under the streetlamp. The crisis they’re watching for — the valuation crash, the hype cycle deflation, the “AI winter” — is not the crisis that’s coming, but the result of looking at what’s most measurable, a McNamara fallacy of supreme proportions. The real bubble is quieter, more insidious and almost entirely ignored: we are depleting finite human resources faster than any compute buildout can compensate.</p>
<p>And so, in my view, the most important AI developments of 2026 won’t be about what AI can do. They’ll be about what <em>we</em> must change — how we think, how we interact, how we organise, and how we understand what these systems even are.</p>
<p>Here are my five guesses. None of them are about model capabilities.</p>
<section id="the-pace-we-cannot-keep" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-pace-we-cannot-keep">The pace we cannot keep</h2>
<p>There’s a reason military doctrine distinguishes between the <em>tempo</em> of operations and the <em>speed</em> of individual actions. You can have blazingly fast units that nonetheless lose wars because they outrun their supply lines, their communications, their ability to coordinate. The German Blitzkrieg worked not because the tanks were fastest, but because the entire system — infantry, air support, logistics, command — could maintain coherent tempo together.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;The later failures on the Eastern Front were, in part, failures of exactly this coherence. Speed without synchronisation is just chaos with velocity.</p></div></div><p>We are building AI agents that can operate at extraordinary speed. What we have not solved — what we have barely begun to think about — is the tempo problem. Agents can draft, iterate, execute and evaluate faster than any human can meaningfully supervise. And yet, for any task that matters, a human must remain in the loop. Not because regulations demand it (though they do), but because the unique contribution humans make — judgement, context, accountability — cannot yet be delegated.</p>
<p>This creates an odd inversion. The bottleneck is no longer the machine. It’s us.</p>
<p>Vibe coders realised this pretty early, encountering a sort of fatigue that sounded an awful lot like mental overwhelm to me. Even I’ve noticed it in my own work this year: I am restructuring how I think. Not consciously at first, but unmistakably. I batch decisions differently. I’ve developed heuristics for when to trust AI output (simple heuristic: more or less never) and when to inspect it. I find myself pre-formatting my intentions in ways that minimise back-and-forth.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> I am, in some small way, becoming more machine-compatible.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;This is not prompt engineering but something closer to cognitive ergonomics — reshaping one’s own mental workflow to mesh with a non-human collaborator.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;And inevitably, a LinkedIn influencer will brand it “AgileThink 2.0” or something equally ghastly.</p></div></div><p>This is, I suspect, the cognitive prelude to something larger. In 2026, we’ll see this adaptation become conscious and widespread. People will talk openly about “agent-compatible thinking”. There will be courses, frameworks, perhaps even certifications.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> And somewhere, quietly, someone will start asking whether the adaptation should run the other way — whether we might need to enhance human cognition, chemically or otherwise, to be more biocompatible with the systems we’ve built.</p>
<p>That conversation is coming. 2026 may be when it begins.</p>
</section>
<section id="beyond-the-dialogue" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="beyond-the-dialogue">Beyond the dialogue</h2>
<p>The word “dialogue” comes from the Greek <em>dialogos</em> — a conversation between two. Not three, not many. Two. Almost everything we’ve built in conversational AI assumes the dyadic frame. One human, one model. A prompt, a response. Even the terminology betrays us: we speak of “chat”, of “assistants”, of “conversations” — all implicitly two-party structures. The entire architecture of modern LLM interaction is built around this assumption, from context windows to RLHF to the very notion of “alignment.”</p>
<p>But real human collaboration is rarely dyadic. Meetings have multiple participants. Decisions emerge from group deliberation. The most interesting intellectual work happens in seminars, not tutorials. And the social dynamics of a three-person conversation are fundamentally different from a two-person one — any parent who has watched playground friendships knows this.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;The mathematician Alfréd Rényi once remarked that a mathematician is a machine for turning coffee into theorems. He neglected to mention — as Paul Erdős was fond of recounting — that the best theorems usually emerged from <em>conversations</em> over that coffee, and rarely with just one other person.</p></div></div><p>We do not yet know how to build AI systems that can participate meaningfully in polyadic conversation. The problems are legion: turn-taking, attention management, theory of mind about multiple interlocutors, maintaining coherent context across divergent threads, knowing when to speak and when to listen. Multi-agent frameworks attempt this from one direction — multiple AI agents coordinating — but the harder problem is mixed groups: two humans and an agent, or a team with an AI participant, or a board meeting with a non-human advisor.</p>
<p>I suspect 2026 will see the first serious attempts to crack this. Not solutions, but recognitions: papers naming the problem (the field is extremely niche – there are fewer than a dozen major papers dealing with the overall problem of it all), frameworks attempting to address it, products that fail instructively. The dyadic assumption will start to feel like what it is — a limitation we backed into, not a design choice we made.</p>
<p>When we solve this — and eventually we will — AI stops being a tool you <em>use</em> and becomes a participant you <em>include</em>. That is a category shift with implications we’ve barely begun to think through.</p>
</section>
<section id="the-last-fixed-screen" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-last-fixed-screen">The last fixed screen</h2>
<p>Every interface you have ever used was a compromise. Not between you and the machine, but between you and every other user who might encounter the same software. The button is <em>there</em> because it had to be <em>somewhere</em>, and designers made a choice that would be tolerable for most people most of the time. Your preferences, your workflows, your particular way of thinking — these were never the point. You adapted to the interface. The interface did not adapt to you.</p>
<p>We have naturalised this so completely that we barely notice it. But consider how strange it is: the same spreadsheet layout for the accountant and the clinical scientist, the same email client for the executive and the intern, the same dashboard for the expert and the novice. We call this “consistency” and treat it as a virtue.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;And it was a virtue, once, when the alternative was chaos. But virtues have contexts, and contexts change.</p></div></div><p>Generative AI dissolves this compromise. If a model can understand what you’re trying to accomplish, it can construct an interface suited to exactly that task, for exactly your preferences, in exactly this moment. The interface becomes ephemeral — not a fixed artifact but a generated surface, conjured on demand and discarded when done. This is not a minor evolution in UX but the end of UI determinism as we’ve known it.</p>
<p>The implications cascade. Design systems become less about specifying interfaces and more about specifying <em>constraints</em> on generated interfaces. Accessibility transforms from retrofitting fixed layouts to declaring capabilities and letting the system adapt. Documentation becomes nearly impossible — how do you write a manual for a screen that exists only once?<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> And debugging becomes archaeological: reconstructing what the user saw at the moment something went wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;The enterprise software industry will have a collective aneurysm. I look forward to it. But just as UIs can be on-the-fly, so can documentation.</p></div></div><p>In 2026, we’ll see the first serious experiments with ephemeral UI. They will be partial, tentative, probably somewhat broken. But they will demonstrate something important: you don’t interact with interfaces. You interact with <em>data</em>. Everything else is — was always — just an abstraction. And abstractions, it turns out, can be adaptive.</p>
</section>
<section id="what-were-actually-running-out-of" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="what-were-actually-running-out-of">What we’re actually running out of</h2>
<p>If you want to know where a bubble is, don’t look at what’s abundant. Look at what’s scarce.</p>
<p>The popular narrative says we’re running out of… what, exactly? Investor patience? Reasonable valuations? Adult supervision? These are the scarcities the bubble-watchers monitor. But they’re looking under the streetlamp again.</p>
<p>Here’s what we’re actually running out of: talent. And data.</p>
<p>Let me start with the harder one — harder for me personally. This has been a year of too many farewells. I have watched colleagues burn out, leave the field, leave the <em>industry</em>, leave more than that. The pace is not sustainable. The always-on, always-shipping, always-pivoting tempo that frontier AI demands is chewing through the few thousand people in the world who can actually do this work, a resource that is nowhere near renewable. You can always build more GPUs, raise another round or find another senator to awkwardly pose next to you as you break ground on another multi-gigawatt data centre. But you cannot mint senior ML researchers, manufacture institutional knowledge or shortcut the decade it takes to develop the intuition that distinguishes good research from promising-looking noise.</p>
<p>No foundation lab wants to talk about this. But I’ve seen the faces. I’ve had the conversations, the ones that don’t make it into the proceedings or the news articles. There are endless news about nine-figure pay packages and very little about the fact that people are <em>leaving</em> those jobs. As always, journalism managed to miss the wood for the trees. The talent crisis is real, it’s accelerating, and 2026 will be the year it becomes undeniable.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;And if you’re running a lab, a startup or even a larger company: look out for your people. Regardless of what you pay them, it’s small change compared to what it will cost to replace them on an increasingly tight market.</p></div><div id="ref-Hoffmann2022Chinchilla" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>‘Training Compute-Optimal Large Language Models’</span>. <em>arXiv Preprint arXiv:2203.15556</em>.
</div></div><p>The data crisis is, if anything, more fundamental. I keep going on about this like those crazy street preachers you would see on a street corner, except my version of the end times talks a lot about tokens, scaling laws and South American rodents. LLMs, or at least the paradigm we have for their pre-training, obey certain laws the way a marble obeys the laws of gravity rolling down a slope. To train a model of a given size optimally, you need a proportional amount of high-quality data <span class="citation" data-cites="Hoffmann2022Chinchilla">(<a href="#ref-Hoffmann2022Chinchilla" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>. And we, as a species, do not produce quality information fast enough. We have already strip-mined the public web. We are negotiating access to private archives, licensed corpora, proprietary datasets. But the math doesn’t lie: pre-training at scale, as we’ve known it, is approaching exhaustion.</p>
<p>This is why the future belongs to smaller, focal models with intelligent routing. Not because small is philosophically superior, but because the Chinchilla constraint gives us a choice: grow our models or maintain quality.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> We cannot do both indefinitely. The era of monolithic giants may already be ending — not with a bang, but with a quiet recognition that we’ve run out of food to feed them.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;And by quality, I also include resilience. One offshoot of these scaling laws is that as we need to reduce our quality standards, our vulnerability to intentional contamination of the data supply chain increases vastly.</p></div></div></section>
<section id="the-retrieval-delusion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-retrieval-delusion">The retrieval delusion</h2>
<p>A Searchlight Institute survey from August 2025 asked Americans what they thought happened when they queried tools like ChatGPT <span class="citation" data-cites="SearchlightAI2025">(<a href="#ref-SearchlightAI2025" role="doc-biblioref">Searchlight Institute 2025</a>)</span>. Forty-five percent said the tool “looks up an exact answer in a database.” Another twenty-one percent believed it “follows a script of prewritten responses.”<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Two-thirds of users, in other words, believe LLMs are either filing cabinets or chatbots circa 2015. Not generating. Not constructing. <em>Retrieving</em>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-SearchlightAI2025" class="csl-entry" role="listitem">
Searchlight Institute. 2025. <span>‘Americans Have Mixed Views of AI and an Appetite for Regulation’</span>. <a href="https://www.searchlightinstitute.org/research/americans-have-mixed-views-of-ai-and-an-appetite-for-regulation/" class="uri">https://www.searchlightinstitute.org/research/americans-have-mixed-views-of-ai-and-an-appetite-for-regulation/</a>.
</div><div id="fn10"><p><sup>10</sup>&nbsp;The survey, conducted by Tavern Research with 2,301 American adults, buried this finding in a section titled “Dustbin” — noting it “didn’t lead to a larger conclusion.” I would respectfully submit that it <em>is</em> the conclusion.</p></div></div><p>This is not a minor misconception. This is the original sin from which nearly every dysfunction in the LLM ecosystem flows.</p>
<p>If you believe LLMs retrieve information, you use them as search engines. You ask factual questions and expect factual answers. You are confused when they “hallucinate” — because filing cabinets don’t hallucinate. You optimise for short, snappy responses, because that’s what search results look like. And the models, trained on human feedback, learn to provide exactly that: terse, confident, citation-shaped outputs that pattern-match to “retrieved information” whether or not the underlying content is sound.</p>
<p>The result? LLMs are terrible at generating good prose — because we’ve trained them to generate query responses. They are unreliable as knowledge sources — because they were never knowledge sources. And worst of all, the retrieval delusion creates an incentive structure for poisoning the data supply chain: if people believe these systems retrieve rather than generate, then manipulating what gets “retrieved” becomes a vector for information warfare.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;This is already happening. SEO for AI (GEO or AEO) is a growth industry. The goal is not to inform the model but to manipulate its outputs.</p></div></div><p>But the retrieval delusion has a human-side corollary — what I think of as the assistance gap. We have not figured out how to work <em>with</em> these systems. Not because they’re bad at collaboration, though they often are. But because most people have never managed anyone. They’re newly minted kings of enormous kingdoms, with no idea of how to rule.</p>
<p>Think about what LLMs require: clear intent, structured delegation, graceful error handling, iterative refinement. These are management skills. Most of us have never learned to command in a way that gets results. We have no training in delegation. We are, in effect, first-time managers with no onboarding, managing an employee whose capabilities we fundamentally misunderstand.</p>
<p>2026 will be the year we start naming this gap. Not just on the AI side — how to build better assistants — but on the human side: how to become better at being assisted.</p>
</section>
<section id="three-battles-in-the-background" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="three-battles-in-the-background">Three battles in the background</h2>
<p>These five predictions unfold against a backdrop of deeper tensions — three battles for the soul of AI that will shape which future we’re predicting <em>for</em>.</p>
<p><strong>The architecture wars.</strong> Yann LeCun keeps insisting that LLMs are a dead end, that Joint Embedding Predictive Architectures (JEPA) and world models are the path forward <span class="citation" data-cites="LeCun2022JEPA">(<a href="#ref-LeCun2022JEPA" role="doc-biblioref">LeCun 2022</a>)</span>.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Others maintain that scale and emergent capabilities will carry transformer architectures wherever we need to go. This is not merely an academic dispute. It’s a question of whether the billions poured into LLM infrastructure are investments or sunk costs. I confess I find the “superintelligence” framing that we seem to be resorting to rather tedious — but the underlying question matters: are we building toward something general, or optimising a local maximum?</p>
<div class="no-row-height column-margin column-container"><div id="ref-LeCun2022JEPA" class="csl-entry" role="listitem">
LeCun, Yann. 2022. <span>‘A Path Towards Autonomous Machine Intelligence’</span>. <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" class="uri">https://openreview.net/pdf?id=BZ5a1r-kVsf</a>.
</div><div id="fn12"><p><sup>12</sup>&nbsp;His argument, roughly: LLMs predict tokens, but intelligence requires predicting <em>states of the world</em>. Autoregressive text generation is a parlour trick, not a path to understanding. He is not wrong.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;Anthropic remains the notable exception. Claude is still everyone’s quirky uncle.</p></div></div><p><strong>The business model reckoning.</strong> The SaaS LLM model — pay per token, API as product — worked when models were scarce and differentiated. But commoditisation is coming. Open-weight models are closing the gap and starting to look more or less the same.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> Inference costs are falling. If the moat was capability, and capability is converging, what exactly are customers paying for? The death of scaling, if it comes, is also the death of a particular business logic. What replaces it? Vertical integration? Specialised fine-tunes? Model-as-loss-leader for something else entirely? Nobody knows yet. But the current model won’t survive contact with 2026’s economics.</p>
<p><strong>The locus of intelligence.</strong> Here’s a question that keeps me up at night: when an agentic system does something impressive, where does the intelligence <em>live</em>? In the model? In the scaffolding — the prompts, the tools, the orchestration logic? In the interplay between them? We tend to attribute capability to the model, but increasingly the clever work happens in the framework: the routing, the decomposition, the error recovery. Do we need better models, better agentic architectures, or both — and if both, should they be developed in isolation or in concert? This, too, is a very practical puzzle. A lot of players have made certain bets in one way or another, and this town is not going to be big enough for all the different visions even for simple problems like NL2SQL.</p>
<p>These battles won’t conclude in 2026. But they’re the gravitational forces bending everything else. There’s of course also geopolitics – the story that wants to be the story (US v EU, a generally lukewarm conflict considering EU spending on AI is almost laughably small compared to the US), and the story that actually is (China). We’ve spent the last months of an undoubted American advantage earlier last year, and we may be in the last few months of a rapidly closing decision window when well-conceived initiatives like <a href="https://atomproject.ai/">the ATOM Project</a> can still make a difference. For those intelligent or knowledgeable enough not to be taken in by the fashionable meme that AI is a ‘scam’, the realisation that this is one of the most consequential technologies of our time is inescapable. Someone will control it. I dislike that idea, but I’m enough of a realist to accept it. 2026 will be the year when we learn whether we have enough of a belief in things we claim to hold dear – freedom, democracy, academic liberty, the right of the individual to self-determination and knowledge – to actually defend them in practice.</p>
<hr>
<p>As I was pulling my thoughts together for this post, what struck me was how few of these predictions were about AI itself. They’re about us, in the end: our cognition, our interfaces, our institutions, our misconceptions. The most important variable in 2026 may not be what the models can do, but whether we can adapt quickly enough to work with what they already do. The story of AI is fundamentally a human story, not a technological one. Technology comes from the Greek word <em>techne</em> – something made, something created. It’s the product, not the producer. And so, we can’t talk about AI the way we talk about the weather. We have to accept that its shortcomings and its glories both are products of our work, reflections of our choices. This is not always a pleasant thought, but the alternative is not only factually false to the point of delusion, it’s also disempowering.</p>
<p>The bubble everyone’s watching for — the valuation crash, the hype collapse — may come or may not. I don’t know. But the bubble I see is quieter and more certain: we are depleting reserves that infrastructure cannot replace. Talent burns out. Data runs dry. Conceptual confusion compounds. These are the scarcities that will shape what’s possible.</p>
<p>And yet. I’ve been in this field long enough to know that constraints breed creativity. The shift to smaller models is also an opportunity to build more thoughtfully. The human bottleneck is also an invitation to build systems that meet us where we are. The retrieval misconception, ultimately, is a reflection of just how far we have grown apart conceptually and linguistically, and we can always choose to bridge that gap.</p>
<p>I don’t know if these guesses will age well. Last year’s held up better than I expected. These feel riskier. But the point of predicting the future was never certainty — it was being present to change. Walking alone in the dark, alert to signs.</p>
<p>Here’s to 2026. May we find our keys where we actually lost them.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Five Wild Guesses for 2026},
  date = {2025-12-31},
  url = {https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/},
  langid = {en-GB}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Chris von Csefalvay. 2025. <span>“Five Wild Guesses for 2026.”</span> <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/">https://chrisvoncsefalvay.com/posts/five-wild-guesses-2026/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrisvoncsefalvay\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<ol start="3" type="a">
<li>Chris von Csefalvay, 2011–. <a href="disclaimer">Disclaimer</a></li>
</ol>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>