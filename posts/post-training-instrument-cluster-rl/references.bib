@article{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  journal={Proceedings of the 40th International Conference on Machine Learning},
  year={2023},
  url={https://proceedings.mlr.press/v202/gao23h.html}
}

@misc{schulman2020kl,
  title={Approximating {KL} divergence},
  author={Schulman, John},
  year={2020},
  url={http://joschu.net/blog/kl-approx.html}
}

@inproceedings{chen2024catastrophic,
  title={Catastrophic Goodhart: Regularizing {RLHF} with {KL} Divergence Does Not Mitigate Heavy-Tailed Reward Misspecification},
  author={Chen, Thomas and He, Jie and Kifer, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024},
  url={https://proceedings.neurips.cc/paper_files/paper/2024/file/1a8189929f3d7bd6183718f42c3f4309-Paper-Conference.pdf}
}

@article{huang2024n,
  title={The {N}+ Implementation Details of {RLHF} with {PPO}: A Case Study on {TL;DR} Summarization},
  author={Huang, Shengyi and Liu, Michael and Zhong, Qinyi and others},
  journal={arXiv preprint arXiv:2403.17031},
  year={2024},
  url={https://huggingface.co/papers/2403.17031}
}

@article{shao2024deepseekmath,
  title={{DeepSeekMath}: Pushing the Limits of Mathematical Reasoning in Open Language Models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024},
  url={https://huggingface.co/papers/2402.03300}
}

@article{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  url={https://huggingface.co/papers/2305.18290}
}

@article{schulman2017proximal,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://huggingface.co/papers/1707.06347}
}

@article{zhu2024inform,
  title={{InfoRM}: Mitigating Reward Hacking in {RLHF} via Information-Theoretic Reward Modeling},
  author={Zhu, Yuchun and Chen, Jianxin and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024},
  url={https://proceedings.neurips.cc/paper_files/paper/2024/file/f25d75fc760aec0a6174f9f5d9da59b8-Paper-Conference.pdf}
}

@misc{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  year={2022},
  url={https://arxiv.org/abs/2203.02155}
}

@article{deepseek2025r1,
  title={{DeepSeek-R1}: Incentivizing Reasoning Capability in {LLMs} via Reinforcement Learning},
  author={{DeepSeek-AI}},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025},
  url={https://huggingface.co/papers/2501.12948}
}
