<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris von Csefalvay">
<meta name="dcterms.date" content="2025-12-28">
<meta name="description" content="Monitoring reward curves, KL divergence, and policy health when training with DPO, PPO, and GRPO.">

<title>The post-training instrument cluster – Part II – Chris von Csefalvay</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a4d8066ab99c821fadc425098389dfee.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=395640625"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', '395640625', { 'anonymize_ip': true});
</script>
<script type="application/ld+json">{"@context":"http://www.schema.org","@type":"person","name":"Chris von Csefalvay","jobTitle":"Director of Biomedical AI/ML","height":"74 inches","gender":"male","description":"Chris von Csefalvay is a computational epidemiologist and data scientist working at the intersection of AI/ML, computational dynamics and public health. He is the author of Computational Modeling of Infectious Disease and a number of research papers.","url":"https://chrisvoncsefalvay.com","image":"https://chrisvoncsefalvay.com/img/IMG_5986.jpeg","address":{"@type":"PostalAddress","addressLocality":"Denver","addressRegion":"CO","postalCode":"80204","addressCountry":"United States"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University of Oxford","sameAs":"https://en.wikipedia.org/wiki/University_of_Oxford"},{"@type":"CollegeOrUniversity","name":"Cardiff University","sameAs":"https://en.wikipedia.org/wiki/Cardiff_University"}],"worksFor":[{"@type":"Organization","name":"HCLTech"}],"birthDate":"1986-07-15","birthPlace":"Budapest, Hungary","memberOf":[{"@type":"Organization","name":"Royal Society for Public Health"},{"@type":"Organization","name":"TOPRA"},{"@type":"Organization","name":"IEEE"}],"nationality":[{"@type":"Country","name":"United Kingdom"},{"@type":"Country","name":"Hungary"}]}</script>
<style>
img, .cell-output-display img {
  max-width: 100%;
  height: auto;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="The post-training instrument cluster – Part II – Chris von Csefalvay">
<meta property="og:description" content="Monitoring reward curves, KL divergence, and policy health when training with DPO, PPO, and GRPO.">
<meta property="og:image" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/rl_failure_patterns.png">
<meta property="og:site_name" content="Chris von Csefalvay">
<meta property="og:image:height" content="1484">
<meta property="og:image:width" content="1635">
<meta name="twitter:title" content="The post-training instrument cluster – Part II – Chris von Csefalvay">
<meta name="twitter:description" content="Monitoring reward curves, KL divergence, and policy health when training with DPO, PPO, and GRPO.">
<meta name="twitter:image" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/rl_failure_patterns.png">
<meta name="twitter:image-height" content="1484">
<meta name="twitter:image-width" content="1635">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="The post-training instrument cluster -- Part II">
<meta name="citation_author" content="Chris von Csefalvay">
<meta name="citation_publication_date" content="2025-12-28">
<meta name="citation_cover_date" content="2025-12-28">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-12-28">
<meta name="citation_fulltext_html_url" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/">
<meta name="citation_language" content="en-GB">
<meta name="citation_reference" content="citation_title=Scaling laws for reward model overoptimization;,citation_author=Leo Gao;,citation_author=John Schulman;,citation_author=Jacob Hilton;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://proceedings.mlr.press/v202/gao23h.html;,citation_journal_title=Proceedings of the 40th International Conference on Machine Learning;">
<meta name="citation_reference" content="citation_title=Approximating KL divergence;,citation_author=John Schulman;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=http://joschu.net/blog/kl-approx.html;">
<meta name="citation_reference" content="citation_title=Catastrophic goodhart: Regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification;,citation_author=Thomas Chen;,citation_author=Jie He;,citation_author=Daniel Kifer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://proceedings.neurips.cc/paper_files/paper/2024/file/1a8189929f3d7bd6183718f42c3f4309-Paper-Conference.pdf;,citation_conference_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=The N+ implementation details of RLHF with PPO: A case study on TL;DR summarization;,citation_author=Shengyi Huang;,citation_author=Michael Liu;,citation_author=Qinyi Zhong;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://huggingface.co/papers/2403.17031;,citation_journal_title=arXiv preprint arXiv:2403.17031;">
<meta name="citation_reference" content="citation_title=DeepSeekMath: Pushing the limits of mathematical reasoning in open language models;,citation_author=Zhihong Shao;,citation_author=Peiyi Wang;,citation_author=Qihao Zhu;,citation_author=Runxin Xu;,citation_author=Junxiao Song;,citation_author=Mingchuan Zhang;,citation_author=Y. K. Li;,citation_author=Y. Wu;,citation_author=Daya Guo;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://huggingface.co/papers/2402.03300;,citation_journal_title=arXiv preprint arXiv:2402.03300;">
<meta name="citation_reference" content="citation_title=Direct preference optimization: Your language model is secretly a reward model;,citation_author=Rafael Rafailov;,citation_author=Archit Sharma;,citation_author=Eric Mitchell;,citation_author=Stefano Ermon;,citation_author=Christopher D. Manning;,citation_author=Chelsea Finn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://huggingface.co/papers/2305.18290;,citation_volume=36;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Proximal policy optimization algorithms;,citation_author=John Schulman;,citation_author=Filip Wolski;,citation_author=Prafulla Dhariwal;,citation_author=Alec Radford;,citation_author=Oleg Klimov;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://huggingface.co/papers/1707.06347;,citation_journal_title=arXiv preprint arXiv:1707.06347;">
<meta name="citation_reference" content="citation_title=InfoRM: Mitigating reward hacking in RLHF via information-theoretic reward modeling;,citation_author=Yuchun Zhu;,citation_author=Jianxin Chen;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://proceedings.neurips.cc/paper_files/paper/2024/file/f25d75fc760aec0a6174f9f5d9da59b8-Paper-Conference.pdf;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Training language models to follow instructions with human feedback;,citation_author=Long Ouyang;,citation_author=Jeff Wu;,citation_author=Xu Jiang;,citation_author=Diogo Almeida;,citation_author=Carroll L. Wainwright;,citation_author=Pamela Mishkin;,citation_author=Chong Zhang;,citation_author=Sandhini Agarwal;,citation_author=Katarina Slama;,citation_author=Alex Ray;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2203.02155;">
<meta name="citation_reference" content="citation_title=DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning;,citation_author=DeepSeek-AI;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://huggingface.co/papers/2501.12948;,citation_journal_title=arXiv preprint arXiv:2501.12948;">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Chris von Csefalvay</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers"> 
<span class="menu-text">Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../media"> 
<span class="menu-text">Media</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">The Notebook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://computationalinfectiousdisease.com"> 
<span class="menu-text">My book</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-rl-monitoring-problem" id="toc-the-rl-monitoring-problem" class="nav-link active" data-scroll-target="#the-rl-monitoring-problem">The RL monitoring problem</a></li>
  <li><a href="#instrument-9-reward-dynamics" id="toc-instrument-9-reward-dynamics" class="nav-link" data-scroll-target="#instrument-9-reward-dynamics">Instrument 9: reward dynamics</a></li>
  <li><a href="#instrument-10-kl-divergence-health" id="toc-instrument-10-kl-divergence-health" class="nav-link" data-scroll-target="#instrument-10-kl-divergence-health">Instrument 10: KL divergence health</a></li>
  <li><a href="#instrument-11-policy-ratio-and-clip-fraction" id="toc-instrument-11-policy-ratio-and-clip-fraction" class="nav-link" data-scroll-target="#instrument-11-policy-ratio-and-clip-fraction">Instrument 11: policy ratio and clip fraction</a></li>
  <li><a href="#instrument-12-generation-length-dynamics" id="toc-instrument-12-generation-length-dynamics" class="nav-link" data-scroll-target="#instrument-12-generation-length-dynamics">Instrument 12: generation length dynamics</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting it all together</a></li>
  <li><a href="#the-failure-pattern-gallery" id="toc-the-failure-pattern-gallery" class="nav-link" data-scroll-target="#the-failure-pattern-gallery">The failure pattern gallery</a></li>
  <li><a href="#practical-recommendations" id="toc-practical-recommendations" class="nav-link" data-scroll-target="#practical-recommendations">Practical recommendations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The post-training instrument cluster – Part II</h1>
  <div class="quarto-categories">
    <div class="quarto-category">AI</div>
    <div class="quarto-category">LLMs</div>
    <div class="quarto-category">LLMOps</div>
    <div class="quarto-category">post-training</div>
    <div class="quarto-category">reinforcement learning</div>
    <div class="quarto-category">RLHF</div>
  </div>
  </div>

<div>
  <div class="description">
    Monitoring reward curves, KL divergence, and policy health when training with DPO, PPO, and GRPO.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris von Csefalvay <a href="mailto:chris@chrisvoncsefalvay.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-3131-0864" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">28 December 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Hey, I’m writing a book about this!
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m actually writing a book about this stuff. It turns out there isn’t a lot of literature on how to do post-training at the level too big for single-GPU laptop-sized hobby projects and requiring enterprise reliability on one hand, but not quite at the scale of multi-team distributed post-training you’d get in foundation labs. That’s a problem, because a lot of the current value in fine-tuning applications comes exactly out of that large, crucial market. I am in the last phases of putting together the manuscript for <em>The Frontier Playbook</em>, a set of curated tactics and techniques for real world operationalisation of LLMs. <a href="https://aifrontierplaybook.substack.com">Sign up for updates here</a>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This is Part II of a series
</div>
</div>
<div class="callout-body-container callout-body">
<p>This post builds on <a href="../../posts/post-training-instrument-cluster/index.html">Part I</a>, which covered the eight instruments for supervised fine-tuning. If you haven’t read that yet, I recommend starting there – the instruments in this post assume familiarity with the basics of loss landscape monitoring, gradient health and the general philosophy of comprehensive training instrumentation.</p>
</div>
</div>
<p>In Part I, we built an instrument cluster for supervised fine-tuning: eight monitors that together paint a comprehensive picture of what’s happening during a LoRA training run. But as I hinted at the end of that post, the story doesn’t end with SFT.</p>
<p>The moment you move from supervised fine-tuning to reinforcement learning from human feedback (RLHF) or its variants – Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), Proximal Policy Optimization (PPO) and so on – you enter a different regime altogether. The failure modes multiply. The metrics become more subtle. And the consequences of getting it wrong become considerably more expensive.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I’m being restrained here. In my experience, a failed RL run is typically 3-5x more expensive than a failed SFT run, not just in compute cost but in human time spent diagnosing what went wrong. The failure modes are less obvious, the feedback loops are longer and the pathologies can be quite subtle until they suddenly aren’t.</p></div></div><p>The fundamental shift is philosophical: in SFT, we’re teaching the model to imitate. In RL, we’re teaching it to <em>optimise</em>. And optimisation, as anyone who has spent time with reward systems (or golden retrievers) knows, is a double-edged sword. The model will find a way to maximise the reward – the question is whether that way aligns with what we actually wanted.</p>
<p>This post introduces four additional instruments for your post-training dashboard, specifically designed for preference optimisation and reinforcement learning methods. Together with the eight from Part I, they form a twelve-instrument cluster that should catch most of the pathologies you’ll encounter in production RL training.</p>
<section id="the-rl-monitoring-problem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-rl-monitoring-problem">The RL monitoring problem</h2>
<p>Before we dive into the instruments, it’s worth understanding why RL training is harder to monitor than SFT.</p>
<p>In supervised fine-tuning, the objective is clear: minimise the cross-entropy loss between the model’s predictions and the training labels. The loss curve tells you almost everything you need to know about whether learning is happening. Yes, there are subtle failure modes (as we covered in Part I), but the basic diagnostic picture is straightforward.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The reward model is itself a trained system with its own failure modes. We’re optimising against a proxy, not the true objective. This, by the way, is the source of most RL pathologies.</p>
</div></div><p>In RL, we’re optimising against a <em>reward signal</em> – typically produced by a reward model trained on human preferences, or in the case of DPO, implicitly derived from preference pairs. This introduces several complications:</p>
<ol type="1">
<li><strong>The reward is a proxy</strong>: The reward model is our best guess at what humans want, not what they actually want. Optimising too hard against this proxy leads to reward hacking – the model exploiting quirks in the reward model rather than genuinely improving <span class="citation" data-cites="gao2023scaling">(<a href="#ref-gao2023scaling" role="doc-biblioref">Gao, Schulman, and Hilton 2023</a>)</span>.</li>
<li><strong>The policy can drift</strong>: Unlike SFT, where the model stays close to its initialisation, RL can push the model relatively far from its starting point. This drift can destroy capabilities that the base model had. RL can legitimately break your base model’s back.</li>
<li><strong>The optimisation landscape is non-stationary</strong>: As the policy changes, the distribution of data it generates changes, which in turn changes the effective reward landscape.</li>
<li><strong>Multiple objectives are in tension</strong>: We want high reward <em>and</em> low divergence from the reference policy <em>and</em> preserved capabilities <em>and</em> no degenerate behaviours. These can conflict.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-gao2023scaling" class="csl-entry" role="listitem">
Gao, Leo, John Schulman, and Jacob Hilton. 2023. <span>‘Scaling Laws for Reward Model Overoptimization’</span>. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v202/gao23h.html">https://proceedings.mlr.press/v202/gao23h.html</a>.
</div></div><p>The instruments in this post are designed to catch failures in each of these dimensions.</p>
</section>
<section id="instrument-9-reward-dynamics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="instrument-9-reward-dynamics">Instrument 9: reward dynamics</h2>
<p>The reward curve is to RL what the loss curve is to SFT – the primary indicator of whether learning is happening. But unlike loss, reward requires considerably more nuance to interpret correctly.</p>
<p>The TRL library, for example, logs several reward-related metrics by default, but understanding what they mean – and what healthy versus pathological patterns look like – requires some unpacking.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>For DPO, the implicit reward for a response is typically <span class="math inline">\(\beta (\log \pi_\theta(y \mid x) - \log \pi_{\text{ref}}(y \mid x))\)</span>, rather than a chosen-vs-rejected ratio. The <code>rewards/margins</code> metric tracks the difference between chosen and rejected rewards, which should increase during training.</p>
</div></div><div id="1427311b" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Enhanced reward monitoring callback</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainerCallback</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RewardDynamicsCallback(TrainerCallback):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Track reward statistics beyond simple averages."""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.margin_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Track reward metrics (names vary by trainer)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        reward_keys <span class="op">=</span> [</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rewards/chosen"</span>, <span class="st">"rewards/rejected"</span>, <span class="st">"rewards/margins"</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"objective/scores"</span>, <span class="st">"objective/rlhf_reward"</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> reward_keys:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">in</span> logs:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                value <span class="op">=</span> logs[key]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"margin"</span> <span class="kw">in</span> key <span class="kw">or</span> <span class="st">"chosen"</span> <span class="kw">in</span> key:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.reward_history.append(value)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.reward_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>                        rewards <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.reward_history)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Trend analysis</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">_trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                            <span class="bu">range</span>(<span class="bu">len</span>(rewards)), rewards, <span class="dv">1</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                        )[<span class="dv">0</span>]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Variance tracking (early warning for instability)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">_variance"</span>] <span class="op">=</span> np.var(rewards)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Saturation detection</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                        recent <span class="op">=</span> rewards[<span class="op">-</span><span class="dv">20</span>:] <span class="cf">if</span> <span class="bu">len</span>(rewards) <span class="op">&gt;=</span> <span class="dv">20</span> <span class="cf">else</span> rewards</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">_saturation"</span>] <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> np.std(recent) <span class="op">/</span> (np.std(rewards) <span class="op">+</span> <span class="fl">1e-8</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>For <strong>DPO</strong>, the key metrics are:</p>
<ul>
<li><code>rewards/chosen</code>: The implicit reward for chosen responses (higher is better)</li>
<li><code>rewards/rejected</code>: The implicit reward for rejected responses (lower is better)</li>
<li><code>rewards/margins</code>: The difference between chosen and rejected (should increase)</li>
<li><code>rewards/accuracies</code>: How often the model prefers chosen over rejected (should approach 1.0)</li>
</ul>
<div id="cell-fig-dpo-metrics-relationship" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-dpo-metrics-relationship" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dpo-metrics-relationship-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-dpo-metrics-relationship-output-1.png" width="822" height="519" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dpo-metrics-relationship-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The relationship between DPO reward metrics: chosen and rejected rewards define the margin, which determines accuracy through the sigmoid function.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The margin is simply the difference between the chosen and rejected rewards; the preference probability is the sigmoid of that margin (scaled by beta).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The logged accuracy is the empirical fraction of pairs where the chosen reward exceeds the rejected reward, so it tracks the sigmoid in expectation. As training progresses, we want the margin to increase – either by pushing the chosen reward up, the rejected reward down, or both. A larger margin means higher accuracy, which means the model more reliably prefers the chosen response over the rejected one.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;This follows from the Bradley-Terry model underlying DPO. The model assumes the probability of preferring response A over B is <span class="math inline">\(\sigma(r_A - r_B)\)</span>, where <span class="math inline">\(r\)</span> is the implicit reward. DPO’s loss function directly optimises this probability, so the accuracy metric – how often the model assigns higher probability to the chosen response – naturally follows a sigmoid relationship with the reward margin.</p></div></div><p>For <strong>PPO</strong>, the key metrics are:</p>
<ul>
<li><code>objective/scores</code>: The raw reward from the reward model</li>
<li><code>objective/rlhf_reward</code>: The effective RLHF reward (<code>scores + non_score_reward</code>)</li>
<li><code>objective/non_score_reward</code>: The KL penalty contribution (negative, computed as <code>-kl_coef * kl</code>)</li>
<li><code>objective/kl</code>: The mean KL divergence between current and reference policy</li>
</ul>
<p>For <strong>GRPO</strong>, the metrics follow a different naming convention:</p>
<ul>
<li><code>reward</code>: The overall average reward after applying reward weights</li>
<li><code>reward_std</code>: The standard deviation of rewards (per-group or batch-level, depending on <code>scale_rewards</code>)</li>
<li><code>kl</code>: The average KL divergence (logged only if <code>beta &gt; 0</code>)</li>
<li><code>entropy</code>: Average entropy of token predictions across completions</li>
<li><code>clip_ratio/region_mean</code>: The fraction of token probabilities clipped within the trust region</li>
<li><code>clip_ratio/low_mean</code> and <code>clip_ratio/high_mean</code>: Mean clipping on lower and upper bounds</li>
</ul>
<div id="cell-fig-reward-dynamics" class="cell" data-fig-height="8" data-fig-width="10" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-reward-dynamics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reward-dynamics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-reward-dynamics-output-1.png" width="952" height="759" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reward-dynamics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Reward dynamics diagnostics: healthy learning vs common pathologies in DPO.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Margin not increasing</strong>: If <code>rewards/margins</code> plateaus early, your model has stopped learning from the preference data. Either the learning rate is too low, or you’ve exhausted the signal in your dataset. As a general rule, people overestimate just how much signal <em>is</em> in a given preference dataset.</li>
<li><strong>Unbounded reward growth</strong>: If chosen rewards keep climbing without limit, you’re likely seeing reward hacking. The model is exploiting the reward function rather than genuinely improving. Check your KL divergence – it’s probably too high.</li>
<li><strong>Rewards converging</strong>: If chosen and rejected rewards approach each other, the model is losing its ability to distinguish preferences. This can indicate catastrophic forgetting or a learning rate that’s too high.</li>
<li><strong>High reward variance</strong>: Unstable rewards suggest unstable optimisation. Consider reducing the learning rate or increasing the KL penalty.</li>
</ul>
</section>
<section id="instrument-10-kl-divergence-health" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="instrument-10-kl-divergence-health">Instrument 10: KL divergence health</h2>
<p>The KL divergence between your training policy and the reference policy is perhaps the single most important diagnostic for RL training. It measures how far your model has drifted from its starting point – and managing this drift is the central challenge of preference optimisation.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The reference policy is typically the SFT model you started with. In DPO, this is implicit in the loss function. In PPO, you maintain an explicit reference model and compute KL divergence at each step.</p>
</div></div><p>The mathematics are straightforward. For a policy <span class="math inline">\(\pi_\theta\)</span> and reference policy <span class="math inline">\(\pi_{\text{ref}}\)</span>, the KL divergence is:</p>
<p><span class="math display">\[D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) = \mathbb{E}_{x \sim \pi_\theta}\left[\log \frac{\pi_\theta(x)}{\pi_{\text{ref}}(x)}\right]\]</span></p>
<p>In practice, we estimate this over batches of generated text, summing the per-token log probability differences. The TRL library uses the Schulman approximator for efficiency <span class="citation" data-cites="schulman2020kl">(<a href="#ref-schulman2020kl" role="doc-biblioref">Schulman 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schulman2020kl" class="csl-entry" role="listitem">
Schulman, John. 2020. <span>‘Approximating <span>KL</span> Divergence’</span>. <a href="http://joschu.net/blog/kl-approx.html">http://joschu.net/blog/kl-approx.html</a>.
</div><div id="ref-chen2024catastrophic" class="csl-entry" role="listitem">
Chen, Thomas, Jie He, and Daniel Kifer. 2024. <span>‘Catastrophic Goodhart: Regularizing <span>RLHF</span> with <span>KL</span> Divergence Does Not Mitigate Heavy-Tailed Reward Misspecification’</span>. In <em>Advances in Neural Information Processing Systems</em>. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1a8189929f3d7bd6183718f42c3f4309-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2024/file/1a8189929f3d7bd6183718f42c3f4309-Paper-Conference.pdf</a>.
</div></div><p>The relationship between KL divergence and reward optimisation is nuanced. Recent work has shown that KL regularisation alone may not prevent reward hacking when reward model errors are heavy-tailed – a phenomenon called “catastrophic Goodhart” <span class="citation" data-cites="chen2024catastrophic">(<a href="#ref-chen2024catastrophic" role="doc-biblioref">Chen, He, and Kifer 2024</a>)</span>. The practical implication: don’t rely solely on KL to prevent overoptimisation. Monitor the other instruments too.</p>
<div id="7c8674e9" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>KL divergence monitoring callback</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KLDivergenceCallback(TrainerCallback):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor KL divergence and detect drift pathologies."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>, drift_threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drift_threshold <span class="op">=</span> drift_threshold</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kl_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># KL metrics (names vary by trainer)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        kl_keys <span class="op">=</span> [<span class="st">"objective/kl"</span>, <span class="st">"kl"</span>, <span class="st">"kl_divergence"</span>, <span class="st">"policy/approxkl_avg"</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> kl_keys:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">in</span> logs:</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                kl <span class="op">=</span> logs[key]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.kl_history.append(kl)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.kl_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>                    kl_values <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.kl_history)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Trend: is KL growing?</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"kl/trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">range</span>(<span class="bu">len</span>(kl_values)), kl_values, <span class="dv">1</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                    )[<span class="dv">0</span>]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Acceleration: is growth accelerating?</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">len</span>(kl_values) <span class="op">&gt;=</span> <span class="dv">20</span>:</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>                        first_half <span class="op">=</span> kl_values[:<span class="bu">len</span>(kl_values)<span class="op">//</span><span class="dv">2</span>]</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>                        second_half <span class="op">=</span> kl_values[<span class="bu">len</span>(kl_values)<span class="op">//</span><span class="dv">2</span>:]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="st">"kl/acceleration"</span>] <span class="op">=</span> np.mean(second_half) <span class="op">-</span> np.mean(first_half)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Warning flag for runaway KL</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> kl <span class="op">&gt;</span> <span class="va">self</span>.drift_threshold:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="st">"kl/drift_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"WARNING: KL divergence (</span><span class="sc">{</span>kl<span class="sc">:.4f}</span><span class="ss">) exceeds threshold (</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>drift_threshold<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="cell-fig-kl-divergence" class="cell" data-fig-height="10" data-fig-width="6" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-kl-divergence" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-kl-divergence-output-1.png" width="568" height="951" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: KL divergence patterns: the art of managing policy drift in preference optimisation.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>KL growing without bound</strong>: Runaway policy drift. Your model is moving too far from the reference. Increase the KL penalty (beta) or reduce the learning rate. In PPO, check that the clipping is working.</li>
<li><strong>KL near zero throughout training</strong>: Your KL penalty (beta) is too high. The model is barely updating. Reduce beta to allow more policy drift.</li>
<li><strong>KL noisy with high variance</strong>: Unstable optimisation. The learning rate is probably too high for your current KL penalty setting.</li>
<li><strong>KL suddenly spiking</strong>: Often indicates a batch of data that the model handles very differently from the reference. Investigate that batch.</li>
</ul>
<p>A useful heuristic for PPO: the <code>val/ratio</code> metric (ratio of current to old policy probabilities) should stay close to 1.0. If it regularly exceeds 2.0 or drops below 0.5, the policy updates are too aggressive <span class="citation" data-cites="huang2024n">(<a href="#ref-huang2024n" role="doc-biblioref">Huang et al. 2024</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-huang2024n" class="csl-entry" role="listitem">
Huang, Shengyi, Michael Liu, Qinyi Zhong, et al. 2024. <span>‘The <span>N</span>+ Implementation Details of <span>RLHF</span> with <span>PPO</span>: A Case Study on <span>TL;DR</span> Summarization’</span>. <em>arXiv Preprint arXiv:2403.17031</em>. <a href="https://huggingface.co/papers/2403.17031">https://huggingface.co/papers/2403.17031</a>.
</div></div></section>
<section id="instrument-11-policy-ratio-and-clip-fraction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="instrument-11-policy-ratio-and-clip-fraction">Instrument 11: policy ratio and clip fraction</h2>
<p>For PPO and GRPO (but not DPO), the clipping mechanism is central to training stability. PPO’s “proximal” nature comes from clipping the policy ratio to prevent too-large updates.</p>
<p>The policy ratio is defined as:</p>
<p><span class="math display">\[r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}\]</span></p>
<p>PPO clips this ratio to the range <span class="math inline">\([1 - \epsilon, 1 + \epsilon]\)</span> (typically <span class="math inline">\(\epsilon = 0.2\)</span>), ensuring that the policy doesn’t change too drastically in a single update. The <code>clip_fraction</code> metric tells you how often this clipping is triggered.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>GRPO, developed by DeepSeek, uses a similar clipping mechanism but computes advantages relative to the group of completions for each prompt <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span>. The monitoring principles are the same.</p>
<div id="ref-shao2024deepseekmath" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. <span>‘<span>DeepSeekMath</span>: Pushing the Limits of Mathematical Reasoning in Open Language Models’</span>. <em>arXiv Preprint arXiv:2402.03300</em>. <a href="https://huggingface.co/papers/2402.03300">https://huggingface.co/papers/2402.03300</a>.
</div></div></div><div id="aea79b9d" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Policy ratio monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PolicyRatioCallback(TrainerCallback):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor policy ratio and clipping behaviour in PPO/GRPO."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, target_clip_fraction: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.15</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_clip_fraction <span class="op">=</span> target_clip_fraction</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.clip_history <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ratio metrics</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"val/ratio"</span> <span class="kw">in</span> logs:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            ratio <span class="op">=</span> logs[<span class="st">"val/ratio"</span>]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            ratio_var <span class="op">=</span> logs.get(<span class="st">"val/ratio_var"</span>, <span class="dv">0</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Flag extreme ratios</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ratio <span class="op">&gt;</span> <span class="fl">2.0</span> <span class="kw">or</span> ratio <span class="op">&lt;</span> <span class="fl">0.5</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"policy/ratio_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"WARNING: Extreme policy ratio: </span><span class="sc">{</span>ratio<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Clip fraction analysis</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        clip_keys <span class="op">=</span> [<span class="st">"policy/clipfrac_avg"</span>, <span class="st">"clip_ratio/low"</span>, <span class="st">"clip_ratio/high"</span>]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> clip_keys:</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">in</span> logs:</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                clip_frac <span class="op">=</span> logs[key]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.clip_history.append(clip_frac)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Too much clipping = updates too aggressive</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> clip_frac <span class="op">&gt;</span> <span class="fl">0.3</span>:</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"policy/clipping_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>                <span class="co"># No clipping = updates might be too small</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> clip_frac <span class="op">&lt;</span> <span class="fl">0.01</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"policy/underclipping_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="cell-fig-clip-fraction" class="cell" data-fig-height="8" data-fig-width="10" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-clip-fraction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clip-fraction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-clip-fraction-output-1.png" width="952" height="759" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clip-fraction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Policy ratio and clip fraction diagnostics for PPO/GRPO training.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Policy ratio far from 1.0</strong>: The ratio should hover around 1.0 with small fluctuations. Values consistently above 1.5 or below 0.7 indicate that consecutive policy updates are too drastic. Reduce the learning rate.</li>
<li><strong>Clip fraction too high (&gt; 30%)</strong>: Most of your updates are being clipped. The learning rate is too high relative to your epsilon. Either reduce learning rate or increase epsilon (carefully).</li>
<li><strong>Clip fraction too low (&lt; 1%)</strong>: Almost no clipping is happening. Either your learning rate is very conservative, or something else is wrong.</li>
<li><strong>Clip fraction growing over time</strong>: The optimisation is becoming increasingly aggressive. This often precedes a KL explosion.</li>
</ul>
</section>
<section id="instrument-12-generation-length-dynamics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="instrument-12-generation-length-dynamics">Instrument 12: generation length dynamics</h2>
<p>The final instrument is deceptively simple but catches some of the most common RL failures: tracking how long your model’s responses are. Generation length is one of the earliest indicators of reward hacking, and changes in length distribution often precede more obvious failures by hundreds of steps.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Length-based reward hacking is so common that many practitioners now add explicit length penalties or normalisation to their reward functions. The TRL library’s <code>missing_eos_penalty</code> parameter exists specifically to combat length collapse.</p>
</div></div><p>Why does length matter so much? Because length is one of the easiest things for a model to optimise. If longer responses tend to score slightly higher (perhaps because they’re more detailed, or because the reward model has a subtle length bias), the model will learn to be verbose. Conversely, if the model discovers that short, safe responses avoid negative rewards, it will collapse to terse outputs.</p>
<p>The key metrics to track (in GRPO/PPO) are:</p>
<ul>
<li><code>completions/mean_length</code>: Average token count across generations</li>
<li><code>completions/mean_terminated_length</code>: Average length of completions that end with EOS</li>
<li><code>completions/min_length</code> and <code>completions/max_length</code>: The range of lengths</li>
<li><code>completions/clipped_ratio</code>: How often generations hit the maximum length limit</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>The distinction between <code>mean_length</code> and <code>mean_terminated_length</code> is subtle but diagnostic. The former counts all completions including those truncated at <code>max_length</code>; the latter only counts those that properly terminated with an EOS token. A large gap between these two metrics indicates frequent truncation – the model is generating responses that exceed the length limit before naturally concluding.</p>
</div></div><div id="3db0cd3b" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Generation length monitoring callback</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GenerationLengthCallback(TrainerCallback):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor generation length dynamics for reward hacking detection."""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, baseline_length: <span class="bu">float</span> <span class="op">=</span> <span class="va">None</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.baseline_length <span class="op">=</span> baseline_length</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        length_keys <span class="op">=</span> [<span class="st">"completions/mean_length"</span>, <span class="st">"response_length"</span>, <span class="st">"gen_len"</span>]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> length_keys:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">in</span> logs:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                length <span class="op">=</span> logs[key]</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.length_history.append(length)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.length_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                    lengths <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.length_history)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Trend detection</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"length/trend"</span>] <span class="op">=</span> np.polyfit(<span class="bu">range</span>(<span class="bu">len</span>(lengths)), lengths, <span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Variance (low variance = mode collapse)</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"length/variance"</span>] <span class="op">=</span> np.var(lengths)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Compare to baseline if available</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.baseline_length:</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>                        ratio <span class="op">=</span> np.mean(lengths) <span class="op">/</span> <span class="va">self</span>.baseline_length</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>                        logs[<span class="st">"length/baseline_ratio"</span>] <span class="op">=</span> ratio</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> ratio <span class="op">&lt;</span> <span class="fl">0.5</span>:</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                            <span class="bu">print</span>(<span class="ss">f"WARNING: Length collapsed to </span><span class="sc">{</span>ratio<span class="sc">:.1%}</span><span class="ss"> of baseline"</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">elif</span> ratio <span class="op">&gt;</span> <span class="fl">2.0</span>:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                            <span class="bu">print</span>(<span class="ss">f"WARNING: Length exploded to </span><span class="sc">{</span>ratio<span class="sc">:.1%}</span><span class="ss"> of baseline"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="cell-fig-generation-length" class="cell" data-fig-height="10" data-fig-width="6" data-execution_count="9">
<div class="cell-output cell-output-display">
<div id="fig-generation-length" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generation-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-generation-length-output-1.png" width="568" height="951" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generation-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Generation length dynamics: healthy stability vs common pathologies.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Length steadily increasing</strong>: The model is learning that verbosity is rewarded. This is often a sign that your reward model has a length bias. Consider adding length normalisation or a brevity penalty.</li>
<li><strong>Length steadily decreasing</strong>: The model is learning that short responses are safer. This often happens when the reward model penalises certain content – the model learns to say less to avoid penalties. Check your reward distribution.</li>
<li><strong>Length variance collapsing</strong>: Even if mean length stays stable, collapsing variance indicates mode collapse. The model is converging to a single “template” response style. Increase temperature or add entropy bonuses.</li>
<li><strong>High clipped ratio</strong>: If <code>completions/clipped_ratio</code> is high (&gt; 20%), many responses are hitting the maximum length limit. Either increase <code>max_completion_length</code> or investigate why the model wants to generate such long responses.</li>
</ul>
<p>A useful baseline: measure your SFT model’s generation length distribution before RL training begins. Any significant deviation (&gt; 30% change in mean or &gt; 50% change in variance) during RL deserves investigation.</p>
</section>
<section id="putting-it-all-together" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="putting-it-all-together">Putting it all together</h2>
<p>The four RL instruments we’ve covered complement the eight from Part I:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 33%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Instrument</th>
<th>What it tells you</th>
<th>Failure mode it catches</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Reward dynamics</td>
<td>Preference learning progress</td>
<td>Reward hacking, saturation, collapse</td>
</tr>
<tr class="even">
<td>KL divergence</td>
<td>Policy drift from reference</td>
<td>Overoptimisation, capability loss</td>
</tr>
<tr class="odd">
<td>Clip fraction</td>
<td>Update magnitude</td>
<td>Unstable optimisation</td>
</tr>
<tr class="even">
<td>Generation length</td>
<td>Output distribution health</td>
<td>Length hacking, mode collapse</td>
</tr>
</tbody>
</table>
<p>Together with the SFT instruments, you now have a twelve-gauge dashboard for monitoring any post-training run.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Not to be confused with an equally potent firearm of the same specification, <em>twelve</em> here quantifies rather than specifies.</p></div></div><p>For production RL training, I recommend the following dashboard layout:</p>
<ol type="1">
<li><strong>Primary RL metrics</strong>: Reward margin, KL divergence, RLHF reward. The overview.</li>
<li><strong>Policy health</strong>: Clip fraction, policy ratio, policy entropy. The vital signs.</li>
<li><strong>Value function</strong>: Value loss, value clipping. (PPO only.)</li>
<li><strong>Fundamentals</strong>: Loss, gradients, attention entropy. Don’t forget these still matter!</li>
</ol>
</section>
<section id="the-failure-pattern-gallery" class="level2">
<h2 class="anchored" data-anchor-id="the-failure-pattern-gallery">The failure pattern gallery</h2>
<p>Let me conclude with a visual summary of the most common RL training failures I encounter in practice. Each of these can be caught by monitoring the right combination of instruments. In RL, “wtf even” is a valid diagnostic category: it’s rare for one thing to go wrong in neat isolation. Rather, what you get is a picture of multiple interacting failures that create a tableau of pathology.</p>
<div id="cell-fig-rl-failure-patterns" class="cell" data-fig-height="10" data-fig-width="11" data-execution_count="10">
<div class="cell-output cell-output-display">
<div id="fig-rl-failure-patterns" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-failure-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-rl-failure-patterns-output-1.png" width="1047" height="951" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-failure-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Common RL training failures: each requires monitoring multiple instruments simultaneously.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="practical-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="practical-recommendations">Practical recommendations</h2>
<p>If you take away nothing else from this post, remember these heuristics:</p>
<ol type="1">
<li><p><strong>Never trust the reward alone</strong>. Always cross-reference with KL divergence and generation quality samples. Reward hacking is the default failure mode.</p></li>
<li><p><strong>Set up alerts for KL divergence thresholds</strong>. In my experience, KL &gt; 0.5 is a warning and KL &gt; 1.0 is a stop sign. Your thresholds may vary, but have thresholds.</p></li>
<li><p><strong>Log generation samples throughout training</strong>. This catches qualitative failures that no metric will show you. Schedule periodic human evaluation if you can.</p></li>
<li><p><strong>For PPO, keep an eye on the ratio and clip fraction</strong>. If the ratio regularly exceeds 1.5 or clip fraction exceeds 30%, reduce your learning rate.</p></li>
<li><p><strong>Don’t forget the basic instruments</strong>. Gradient health, attention entropy and the other fundamentals still matter.</p></li>
</ol>
<p>In the next post, we’ll tackle the third domain: function calling and tool use, where the metrics of success involve not just natural language quality but format compliance, execution accuracy and production reliability. We’ve devoted relatively little attention to the outcome in general so far – perplexity, for example, is a metric that I haven’t commented on a lot. Mainly, that’s because people who swear by it cannot be convinced otherwise and people who don’t already use it don’t care. Function calling, on the other hand, is a different animal. There, we have those magical ‘verifiable rewards’, so we have no excuse not to be rather assiduous about identifying very clear outcome metrics rather than the more procedural statistical metrics that have occupied our attention so far.</p>
<p>Until then, happy training!</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The Post-Training Instrument Cluster -\/- {Part} {II}},
  date = {2025-12-28},
  url = {https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/},
  langid = {en-GB}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Chris von Csefalvay. 2025. <span>“The Post-Training Instrument Cluster
-- Part II.”</span> <a href="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/">https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-rl/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrisvoncsefalvay\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<ol start="3" type="a">
<li>Chris von Csefalvay, 2011–. <a href="disclaimer">Disclaimer</a></li>
</ol>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>