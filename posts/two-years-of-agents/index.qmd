---
categories:
- LLMs
- AI
- agentic AI
- personal
citation: true
date: 2025-11-01
description: Two years on from helping to articulate a concept that changed everything, some thoughts on where we are, where we're going and the things we lost along the way.
google-scholar: true
title: What I talk about when I talk about agentic AI
bibliography: references.bib
image: header.png
---


The Latin verb _agere_ means "to do, to act, to drive forward". From it we get not only "agent" but also "action", "actor", and curiously enough, "agile". The Romans understood what we seem to have forgotten: that agency is fundamentally about motion, about transformation, about the capacity to change the world. I've been thinking about this a lot lately as I mark roughly two years since I sat in a Berkeley coffee shop, overcaffeinated and sleep-deprived, trying to articulate what would come after chatbots. It was late October 2023, my book on computational epidemiology -- with a whole chapter devoted to agents^[My editor will kill me, but I have to admit: I _do_ have a favourite chapter, and it _is_ the one about agents. In no other place do you get Dwarf Fortress and STDs within a few pages of each other.] -- had been published earlier that year, and I was looking at an unfolding crisis as the initial hype around language models began to fade and the bandaid patch of [prompt engineering](../prompt-engineering/index.qmd) quickly showed it wasn't going anywhere. Earlier that year, I spent a blissful few days driving across the country with Oliver, my golden retriever, moving ourselves from Northern Virginia to Denver, and in order to procrastinate away having to unpack boxes, I went on to devote most of the summer to injecting LLMs into agent-based simulations of public health. The connection seemed obvious: these models needed the same kind of connectionistic compositionality that made agent-based models and neural networks powerful -- big structures of relatively trivial parts rather than desperate chases after ever-larger monoliths -- and they needed to encounter the world, to learn from interaction the way a child learns from play.

The idea that language models could _do_ things was, in those late days of 2023, still relatively novel. Tool use was of course more or less established, but far from mainstream, where the focus was on [coaxing the right words out of LLMs](../prompt-engineering/index.qmd). A key step was Shunyu Yao's brilliant ReAct [@Yao2022ReAct] paper, which presented the first proper reconciliation of action and language generation. This put his work beyond other papers that primarily focused on ways of prompting, such as Wei's 2022 CoT [@Wei2022ChainOfThought] paper, even though the latter did lay the groundwork for task decomposition, a key feature of agentic AI. [Lilian Weng](https://lilianweng.github.io/posts/2023-06-23-agent/)'s blog is perhaps the first proper early summary of what an agent properly so called would comprise, and I think that when the history of all of this is written someday, it will show she fully deserves to be considered one of the parents of the agentic idea. Finally, [Harrison Chase](https://blog.langchain.com/agents-round/)'s post and [mine](../team-of-rivals/index.qmd) were among the earliest on conceptualising the kind of more expansive notion of agents that I think are the proper domain of the subject.^[One day, the lot of us need to have dinner and/or a Highlander style battle.]

This idea was picked up by an incredible community of builders, hackers, developers and popularisers, who took and turned it into the global phenomenon it has become. Companies are deploying agent orchestration platforms, VCs are writing nine-figure cheques for "agentic" startups and every consultancy worth its salt now has an agentic AI practice. Agentic AI has also come at the right time to save us from the spectre of an AI winter as moats around simple LLM wrappers were collapsing and prompt engineering came up short. It changed the tone of the discourse that once dismissed LLMs as stochastic parrots,^["Call me when your parrot can buy plane tickets", I once quipped on a call. A friend turned this into a t-shirt I wear with pride to this day.] and the birth of agentic multimodality has enhanced that. In just a year, it's become a household name and a boardroom buzzword. In short, agentic AI has been wildly successful.

And that raises the uncomfortable question I've been wrestling with: what if it had been _too_ successful too fast? What if agentic AI has become a victim of its own success?


## The trap of inchoacy

My fear is that the success of agentic AI has landed us in a local maximum we may not be able to easily escape. Agentic AI is such a powerful idea that even in its current, limited form, it is capable of generating incredible business value -- enough to essentially underwrite the whole long thesis on AI as it stands. But paradoxically, we've fallen into what I call the trap of inchoacy: we've built systems good enough to be profitable, which prevents them from becoming what they could be.

When I first conceptualised agents, I hat I envisioned wasn't an agent-centric approach of 'boxes and arrows', the comforting organisational chart but now supplemented with LLMs. Indeed, to me, that's the worst of two worlds: all the risk of stochasticity with none of its benefits. Coming from epidemiological agent-based modeling, where we routinely have agents in the thousands up to millions, this view feels like we're succumbing to the notion that because 'agent' is in the name, agents should be the protagonists of our story. To me, agentic AI has always been about large scale emergence and self-organisation. This isn't a matter of degree but qualitatively different. If you can draw it on a whiteboard, it may well be useful, possibly make decent dinner reservations, but it will not ever exhibit meaningful emergence (if you don't believe me, try to draw every neuron of a modern convolutional neural network). It may be useful, but it's not what I mean when I talk about agentic AI.

We understand this principle everywhere else. Markets work through distributed decision-making, not central planning. Cities evolve through countless individual choices, not master blueprints. Ecosystems maintain stability through interaction, not instruction. Yet with AI agents, we insist on micromanaging the very tool we have created to escape having to do so.

I'm not chasing some validation of my preferred -- more expansive -- definition here, but rather a degree of sustainability. I worry that we're getting too comfortable with what agents can do for us today in order to harness their potential tomorrow, and it will leave us without a harvest once the low-hanging fruit has all been picked -- and no help will come the way agentic AI came to rescue LLMs from their early crisis like a gentle breeze reawakening a campfire about to go out, not unless we jolt ourselves out of this comfortable stagnation. There's only so much you can automate away with n8n-style node builders and simple decisional flows over LLMs, which is what much of current early agentic AI is about. By the tim we run out of that runway, will we have created the intellectual and theoretical ammunition to reason about the complexities? More importantly: will we have developed the language?


## The word set free

Much of what is wrong with the way we think about agentic AI is intrinsically connected to the way we speak about it. Wittgenstein wrote that the limits of one's language were the limits of their world, and he was altogether correct. In that sense, I must accept some responsibility for my part in all that's gone wrong, however unwitting. Agency is a loaded term -- perhaps more so than most of us realised initially. I can't speak for the others, but I remember what I meant the first time I used it. Agents as long-running bits of code that do some background function have been pretty well established since... well, the dawn of modernish computing. Agents in a philosophical sense, too, were on my mind. But I'm also a recovering lawyer, and even though it's been about twenty years since I last practised, I can recite case law on agency when woken from sleep. A lot of things come with that word: delegation, responsibility, authority -- notions that we need for the entire agentic project to work, and which we do not necessarily as yet have in place. My hope was that that terminological reference would import some of these.

In a life I have mostly lived without regrets, my use of this word has been the cause of some. If I could go back, I'd fervently argue for any other term we could come up with. I believe the word itself has been harmful on both a human and an intellectual level.

Most people of course conceive of agents as persons: real estate agents, customer service agents, airport ticketing agents. English being my fourth language or so, this wasn't really at the forefront of my mind. For right or wrong, many of these jobs were already at a high risk of replacement by AI, and I wonder whether our early and enthusiastic adoption of this term might have created a subconscious bias towards such human agents being seen as fundamentally replaceable by AI. When [Salesforce cuts 4,000 customer service agents to replace them with AI](https://www.cnbc.com/2025/09/02/salesforce-ceo-confirms-4000-layoffs-because-i-need-less-heads-with-ai.html) and companies from UPS to Amazon are laying off their human agents by the thousands, it is hard not to wonder how many redundancy notices one's terminological slippage had helped justify, how much human pain it might have contributed to.

And it's not buying us any semantic clarity, either. It's ultimately an anthropomorphism that leads to exactly the kind of fundamental misunderstanding holding us back from embracing the scale and stochastic connectionism of agentic AI. By seeing them as 'virtual employees' (a truly harmful analogy if there ever was one), we're painting little organisational charts in our heads -- no wonder many of the current so-called agentic architectures resemble one, with a salutary 90 degree rotation to make it maybe a little less obvious. We are confining ourselves to our comfort zones of manageable complexity, clinging to the org chart formalism in hopes that it will help us navigate the volatility, uncertainty, confusion and ambiguity of the world we live in -- and in the process, forfeiting our very best tool against it: the ability of a true non-deterministically designed agentic system to self-organise and adapt to problems. We're micromanaging ourselves into the overwhelming complexity we set out to escape.


## The widening gyre

What probably worries me more than any of this is the rapidly accelerating divide I am seeing that will fracture our society in ways we are barely beginning to understand. We do not have the vocabulary to reason about true agentic AI. Well-informed, experienced and smart leaders of major global companies are still only starting to get their heads around the profound underlying differences that such systems would entail, and the vast majority of their peers are much further behind. You cannot reason about frontier agentic AI in the language of SDLC, of deterministic governance and prescriptive processes. True frontier agentic AI is something no enterprise in the history of humanity has experienced to any appreciable degree. The top maybe 1% of leaders recognise that they need to fundamentally redraw their approach to doing business. The top 0.1% realise that that's not going to be remotely enough -- they'll need to redraw their own way of thinking, and their organisation's. You cannot treat agentic AI like a new, innovative technology: it is quite literally without a useful analogy from any point in history,^[My favourite analogy for this is Star Wars aliens _vs._ Baxter's Xeeleeverse. Star Wars aliens are all basically mostly humanoids, just weird-looking ones. The Xeeleeverse's aliens aren't even based on the same _physical_ principles, never mind the same biological templates, as we are. The closest to something we understand are the Squeem, who are basically space squid. It only gets weirder from there. Qax are cluster organisms of hexagonal convection cells that need turbulence to survive. The main antagonists, the Photino Birds, are made of antimatter. The Xeelee aren't really described. Even very new and disruptive technology like the internet was alien at best in the way Star Wars aliens are. Agentic AI is Xeeleeverse alien.] and -- even with our valiant efforts -- very few enterprises (and leaders) are at this stage equipped with the organisational and cognitive tools to reason about it, never mind harness it effectively.

At some point a few hundred thousand years ago, a gene called FOXP2 underwent a mutation in our early ancestors. We don't really understand what FOXP2 does. The simplistic explanation is that it has to do with language, the better explanation is that it's a motor learning gene that is necessary for effective language acquisition. In either case, it has been one of the greatest and most rapid success stories of evolution, essentially sweeping away wild-type FOXP2 in what in evolutionary terms is a blink of an eye. The evolutionary edge that language ability and speech conferred on our ancestors was so vast that it essentially eliminated the non-speakers. Society, coordination, story, culture, law, lore, history -- we owe almost all our organising principles, from the smallest scale of two hunters planning a way to encircle their prey to the largest scale of social coordinated communication like elections or participatory democracy, to our ability to communicate.

When this breaks down, we're in trouble. When one of the dominant technologies of our age are not so much incomprehensible as defying description due to a lack of a vocabulary, we lose the ability to have anything resembling democratic discourse about the subject itself. There are maybe 2-300 people I'd consider 'frontier thinkers' on agents, and the discussion we're having is drifting apart from the wider social discourse on the topic. Nobody has found a really good way to bridge this gap. History teaches us that this sort of stuff does not tend to end well.

Nowhere is this more evident than in popular commentary on the subject. For about three months now, we've been living in the 'bubble bubble': the ever-inflating mass of LinkedIn posts, Youtube videos and breathless articles proclaiming that the bubble is going to burst _any day now_. Much of it is premised on a superficial understanding of the technology. I don't think you need to be able to explain group-relative policy optimisation in order to have a solid opinion on the economics of AI, but maybe something more profound than pointing at circular financing then concluding AI is the next Enron/Theranos/Lehman Brothers is needed. You cannot understand data center investment policy without understanding the pipeline of products for inference, you cannot understand that pipeline unless you understand what new inference-specific hardware like the prefill optimised Vera Rubin CPX is, and you cannot understand that unless you actually understand transformer architectures and K,V caching. This is one of a thousand things that you need to have at least some grounding on in order to have the context to make sense of this phenomenon. And if you're, say, Hank Green, bless his heart, you probably don't. This is a niche subject and it's okay not to have an opinion on it -- while at the same time, it is of course something that has downstream effects that impact everyone in very human terms. You do need to know transformers in order to be able to reason about data centre spend-o-nomics, you don't need to know any of it to see mass layoffs, rising inequality and the social consequences of technological disruption and comment on it from a human perspective.^[This is a good time to give Stephen Jay Gould's non-overlapping magisteria a shout-out.] Bad commentary even with the best of intentions only widens the divide.^[The other side is not much better. Panic-mongering about superintelligence, about AI destroying the world, about existential risks from systems that still can't reliably count the letters in "strawberry".]

There's an abdication of nuanced thinking here that bothers me a great deal. Agentic AI is neither a bubble about to pop nor an existential threat. It's a genuine technological shift that's creating real value whilst simultaneously falling short of its potential and creating genuine social problems. That's a more complicated story than either the boosters or the doomers want to tell, and definitely a more differentiated one. Much of my day job is shepherding some of the world's smartest business leaders through exactly this quandary -- the messy middle we have to navigate, the safe route we must plot between Scylla and Charybdis. Working with them taught me that world class leaders can take in, indeed welcome, nuance over unhelpful oversimplifications that become cognitive straitjackets. They're comfortable with life in the land of "yes, but also" if it gives them an edge over the Procrustean 'simple stories' of AI as a fraudulent bubble or AI on the road to giving us Skynet any day now. This nuance may be widespread in boardrooms at least at the level we're working at, but is sorely missing from public discourse. If we as a society are to reason about the agentic revolution and its consequences, we must do so without needing to paint a picture of heaven on earth or a straight, do-not-collect-$200 road to hell. To quote a line from McGeorge Bundy,^[That he stole from his brother William, who stole it from Andre Gide.] "gray is the colour of truth".


## Coda

Being early doesn't give me any ownership of the idea or some privileged position in its formulation. Least of all can I or any single individual claim credit for its success, which has built on the efforts of so many. At the same time, in a world where millisecond differences are worth millions, being half a year early makes one incredibly sought after, complete with the kinds of job offers that NFL players used to get. As far as I can see these as an acknowledgment that every leading player in this industry is now recognising the value of agentic AI, it's gratifying. But I've also never forgotten where I came from. 

It's been a long, long journey from writing my first neural networks from a 13th floor hospital room in the Royal London over a decade ago, to waking up to a stuffed mailbox that reads like a Who's Who of Silicon Valley. Every morning I log into my Github, I see the status message I set years ago and never changed: "_doing science and still alive!_", reminding me that just being here is a minor miracle I wasn't guaranteed. I've already won the only lottery that really matters, against fearful odds, and every morning I get to wake up to the snoring of the world's best dog (sorry, not sorry) and watch the sun rise over the Denver skyline, I'm reminded of just how fortunate I am that I get to be here and do the things I live for: do cool
frontier stuff, help my clients, mentor the next generation, give back to the community. There is so much to the future of agentic AI, and AI in general, that I rarely look back because looking ahead keeps me occupied. And so when I revisit the past, it's to explain the origins of my ideas, not to claim credit or authority. I'm shy and reclusive by nature -- seeking either is constitutionally alien to me. But the ideas we help bring into the world create a responsibility for their future we cannot so easily disclaim. We carry them into being as tender sprouts with no guarantees they will grow into mature trees that bear fruit. We only get to give them our hopes and dreams and fears and passions, and pray that it will all turn out for the best. And so as I look at agentic AI's unlikely success story, I worry the way anyone would about the fate of a powerful idea in an uncertain, violent and imperfect world. But more than fear, I have hope.

Because while the agentic revolution may be over, the journey hasn't even  begun in earnest. This is at best the end of the beginning. There are big questions to be tackled. Deep Agents, multi-agent coordination [@Becker2025MALLM], model-native agents/agent-native models, the tricky issues that surround agentic alignment esp. in view of evaluation awareness, embodied agents [@Fu2025ROSBag] and the way we secure and authenticate agents are just a few of them. World simulation agents, which to a great degree [implement the notion I have been advocating for here](../agentic-simulation/index.qmd), are seeing increased use in the social sciences [@Srinivasan2025Democracy] but also, closer to my principal domain of work, in clinical simulation [@Schmidgall2024AgentClinic]. And of course that's all on top of the the challenges we're facing on the wider social issues that I've touched on above.

The promise remains. Systems of agents, properly conceived, could exhibit the kind of emergent intelligence that no individual model can achieve: learn, adapt, task-decompose, recompose. But realising this potential requires moving beyond the current boxes-and-arrows paradigm to something more fluid, more evolutionary, more genuinely agentic. Even it doesn't come with the comfort of being able to draw it on a whiteboard.

The challenge of the road ahead is not primarily technical. No new models, no new GPUs, no new architectures are coming to save us. The issues are on a cognitive, linguistic and social plane, and so must the solutions be. For all my worries, for all my misgivings about terminology, I do believe that the best is yet to come. But I also know that we're not promised those outcomes. When we set sail on this journey, we did so without the guarantee that there will be land at the end of it. That we have found safe harbour and profitable shores is occasion to be grateful -- and carry on.
