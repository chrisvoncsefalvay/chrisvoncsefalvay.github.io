<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris von Csefalvay">
<meta name="dcterms.date" content="2025-06-27">
<meta name="description" content="Your language model isn’t experiencing a cognitive defect. It’s just wrong.">

<title>Just noise in the neurons – Chris von Csefalvay</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a4d8066ab99c821fadc425098389dfee.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=395640625"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', '395640625', { 'anonymize_ip': true});
</script>
<script type="application/ld+json">{"@context":"http://www.schema.org","@type":"person","name":"Chris von Csefalvay","jobTitle":"Director of Biomedical AI/ML","height":"74 inches","gender":"male","description":"Chris von Csefalvay is a computational epidemiologist and data scientist working at the intersection of AI/ML, computational dynamics and public health. He is the author of Computational Modeling of Infectious Disease and a number of research papers.","url":"https://chrisvoncsefalvay.com","image":"https://chrisvoncsefalvay.com/img/IMG_5986.jpeg","address":{"@type":"PostalAddress","addressLocality":"Denver","addressRegion":"CO","postalCode":"80204","addressCountry":"United States"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University of Oxford","sameAs":"https://en.wikipedia.org/wiki/University_of_Oxford"},{"@type":"CollegeOrUniversity","name":"Cardiff University","sameAs":"https://en.wikipedia.org/wiki/Cardiff_University"}],"worksFor":[{"@type":"Organization","name":"HCLTech"}],"birthDate":"1986-07-15","birthPlace":"Budapest, Hungary","memberOf":[{"@type":"Organization","name":"Royal Society for Public Health"},{"@type":"Organization","name":"TOPRA"},{"@type":"Organization","name":"IEEE"}],"nationality":[{"@type":"Country","name":"United Kingdom"},{"@type":"Country","name":"Hungary"}]}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Just noise in the neurons – Chris von Csefalvay">
<meta property="og:description" content="Your language model isn’t experiencing a cognitive defect. It’s just wrong.">
<meta property="og:image" content="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/header.png">
<meta property="og:site_name" content="Chris von Csefalvay">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1024">
<meta name="twitter:title" content="Just noise in the neurons – Chris von Csefalvay">
<meta name="twitter:description" content="Your language model isn’t experiencing a cognitive defect. It’s just wrong.">
<meta name="twitter:image" content="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/header.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1024">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Just noise in the neurons">
<meta name="citation_author" content="Chris von Csefalvay">
<meta name="citation_publication_date" content="2025-06-27">
<meta name="citation_cover_date" content="2025-06-27">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-06-27">
<meta name="citation_fulltext_html_url" content="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/">
<meta name="citation_language" content="en-GB">
<meta name="citation_reference" content="citation_title=Hallucinating faces;,citation_author=S. Baker;,citation_author=T. Kanade;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_doi=10.1109/AFGR.2000.840616;,citation_conference_title=Proceedings fourth IEEE international conference on automatic face and gesture recognition (cat. No. PR00580);">
<meta name="citation_reference" content="citation_title=“Mind-blind for blindness”: A psychological review of anton’s syndrome;,citation_author=Emer M. E. Forde;,citation_author=Claus-W. Wallesch;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_isbn=978-0-203-72712-6;,citation_inbook_title=Classic cases in neuropsychology, volume II;">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Chris von Csefalvay</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers"> 
<span class="menu-text">Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../media"> 
<span class="menu-text">Media</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">The Notebook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://computationalinfectiousdisease.com"> 
<span class="menu-text">My book</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-category-error-of-hallucination" id="toc-the-category-error-of-hallucination" class="nav-link active" data-scroll-target="#the-category-error-of-hallucination">The category error of hallucination</a></li>
  <li><a href="#the-analogy-and-the-mechanism" id="toc-the-analogy-and-the-mechanism" class="nav-link" data-scroll-target="#the-analogy-and-the-mechanism">The analogy and the mechanism</a></li>
  <li><a href="#not-sick-just-wrong-and-maybe-not-even-that" id="toc-not-sick-just-wrong-and-maybe-not-even-that" class="nav-link" data-scroll-target="#not-sick-just-wrong-and-maybe-not-even-that">Not sick, just wrong (and maybe not even that)</a></li>
  <li><a href="#coda" id="toc-coda" class="nav-link" data-scroll-target="#coda">Coda</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Just noise in the neurons</h1>
  <div class="quarto-categories">
    <div class="quarto-category">AI</div>
    <div class="quarto-category">agents</div>
    <div class="quarto-category">neuroscience</div>
    <div class="quarto-category">philosophy</div>
  </div>
  </div>

<div>
  <div class="description">
    Your language model isn’t experiencing a cognitive defect. It’s just wrong.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris von Csefalvay <a href="mailto:chris@chrisvoncsefalvay.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-3131-0864" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">27 June 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“Give me one example,’’ Alan said.<br>
”Of a noncomputable function that a human can do, and a Turing machine can’t?’’<br>
“Yes. And don’t give me any sentimental nonsense about creativity. I believe that a Universal Turing Machine could show behaviors that we would construe as creative.’’”Well, I don’t know then… I’ll try to keep my eye out for that kind of thing in the future.’’</p>
<p>But later, as they were riding back towards Princeton, he said, “What about dreams?’’<br>
”Like those angels in Virginia?’’<br>
“I guess so.’’<br>
”Just noise in the neurons, Lawrence.’’<br>
“Also I dreamed last night that a zeppelin was burning.’’</p>
<p>– Neal Stephenson, <em>Cryptonomicon</em></p>
</blockquote>
<p>There’s a pervasive problem with semantics in artificial intelligence. It’s present at the creation – the term itself characterises the subject as a man-made simulacrum of something ‘natural’ the way we speak of artificial flavourings and artificial rubber. By necessity, the constructs we call artificial intelligences have never been treated (at least semantically) as first class citizens, but always as analogies to some typically biological construct. Most of the time, we can get past our inability to regard AI as <em>sui generis</em> and not merely a faint echo of the flesh.</p>
<p>This is about one of the times when that’s not quite the case.</p>
<p>I am, of course, aware that I am risking perhaps justified derision for being ‘hung up on semantics’, but semantics matters. Semantics are our human handles on notions, often abstract ones. Just as an unwieldy or ill-placed handle will make lifting a box unduly onerous, bad semantics makes manipulating those cognitive constructs difficult, impossible or error-prone. Which is why I continue to be willing to spill ink on our unhelpful tendency to rely on neuropathological metaphors for the mistakes of generative AI.</p>
<p>These models have a notorious habit of producing false yet plausible-sounding information. It is a property so inherent in stochastic generativity that the terminology of hallucinations itself comes not from LLMs, where the expression became ubiquitous, but in fact – best I can tell – from a paper by Baker and Kanade on superresolution (upscaling) of faces <span class="citation" data-cites="840616">(see <a href="#ref-840616" role="doc-biblioref">Baker and Kanade 2000</a>)</span>. Neuroscientists and psychologists often ask where hallucinations come from – Baker and Kanade appear to ask where the pixels generated in upscaling come from, and conclude that they must originate in a sort of hallucination.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="ref-840616" class="csl-entry" role="listitem">
Baker, S., and T. Kanade. 2000. <span>‘Hallucinating Faces’</span>. In <em>Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)</em>, 83–88. <a href="https://doi.org/10.1109/AFGR.2000.840616">https://doi.org/10.1109/AFGR.2000.840616</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Why did it take so long to ask the question? Your typical upscaling algorithm also generates new pixels, but these are essentially deterministic functions of the input pixels. The simplest of these, of course, is interpolation, where every new pixel is a deterministically obtained function of the originating information. Say, for bicubic interpolation over the unit square, <span class="math display">\[ f(x,y) = \sum_{i=0}^3 \sum_{j=0}^3 a_{ij} x^i y^j \]</span> where the <span class="math inline">\(a_{ij}\)</span> is the matrix of coefficients of the interpolating polynomial. This becomes all rather different once we are no longer dealing with essentially squishing the outputs through a deterministic interpolation but through a stochastically applied learned function.</p></div></div><p>I see three principal problems with the metaphor of neuropathology, specifically that of hallucinations, in AI.</p>
<ul>
<li>It’s a category error. LLMs have no perception, embodiment or conscious experience. They cannot have an abnormal perceptive-conscious experience because they are incapable of having that type of experience in the first place.</li>
<li>It invites confusion about what actually happens when hallucinations occur – both in humans and in LLMs.</li>
<li>It ascribes either a cognitive defect to LLMs, or some sort of semi-intentional failure.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;It’s probably worth noting that the alternative term, ‘confabulation’, is just as bad. Depending on whether we consider it a colloquial term (for, essentially, lying) or a technical term (for the reification of cognitive dysfunction through making up something inconsistent with objective reality), it suffers from the same flaws, if not worse.</p></div></div><section id="the-category-error-of-hallucination" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-category-error-of-hallucination">The category error of hallucination</h2>
<p>Primarily, we consider hallucination to be an abnormal perceptive-conscious experience arising from a defect in perception (such as phosphenes), consciousness (being high as balls), processing (e.g.&nbsp;Anton’s blindness) or a combination. The core element in most definitions seems to boil down to conscious perception of things that just aren’t there. There are, all things considered, two major flavours of hallucination. The first category is what I shall refer to, for lack of a better term, as <em>additive</em>: there is an influence that results in an experience or sensation that supervenes the normal – take drugs, see things. The second category is perhaps more interesting, because it is the one that is most often discussed in the context of AI. This is the category of <em>substitutive</em> hallucinations, where the hallucination is a substitution for the normal experience that is almost forced by the brain’s insistence on gestalt and reification. Something is missing, and the brain ‘fills in the gaps’. This is, essentially, arguably not a qualitatively abnormal experience but a quantitatively excessive manifestation of our brain’s normal reification tendencies. Even if we only see the front third or so of a car poking out from behind a building, we understand that in all likelihood there’s more to it than we cannot see, and our mind’s eye can imagine fairly well what else is there (indeed, most people will guess relatively accurately where the rest of the vehicle ends). The phenomena that are discussed as substitutive hallucinations stretch this to an extreme. Perhaps the most extreme yet most fascinating example is Anton-Babinski syndrome <span class="citation" data-cites="Forde_Wallesch_2003">(see <a href="#ref-Forde_Wallesch_2003" role="doc-biblioref">Forde and Wallesch 2003</a>)</span>, where a fully fledged visual perceptual world is created in the presence of profound (typically cortical) blindness.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Forde_Wallesch_2003" class="csl-entry" role="listitem">
Forde, Emer M. E., and Claus-W. Wallesch. 2003. <span>‘<span>“Mind-Blind for Blindness”</span>: A Psychological Review of Anton’s Syndrome’</span>. In <em>Classic Cases in Neuropsychology, Volume II</em>, 23. Psychology Press.
</div></div><p>But, of course, that’s not really an accurate description of what’s transpiring when an LLM makes up some fascinating facts about a subject that it knows nothing about. Calling that a hallucination implies the AI has something analogous to senses and/or an internal world of experience, which happens to be defective. This is fundamentally misleading. Large language models do not perceive the world at all – they have no eyes to see, no ears to hear and no mental states to experience as a result, had they those perceptions in the first place. If a hallucination is seeing something that isn’t there, these models exist in a space in which there isn’t a “there” to mistakenly sense. When an LLM produces an unfounded statement, it isn’t experiencing some complex neurocognitive phenomenon. Not, anyway, does it experience anything that is more complex, mystical or creative than a regression model deviating from a correct answer or a classifier returning an incorrect classifications. We actually have a word for those circumstances. It’s called <em>being wrong</em>, and is about as mysterious as dish soap.</p>
<p>LLMs lack consciousness or embodiment. They don’t have an inner mental state or subjective awareness that could be led astray. Emily Bender, with whom I agree about once every decade or so, is entirely correct on this point:</p>
<p></p><div id="tweet-98296"></div><script>tweet={"url":"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472","author_name":"@emilymbender.bsky.social","author_url":"https:\/\/twitter.com\/emilymbender","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EAnd let&#39;s reflect for a moment on how they phrased their disclaimer, shall we? &quot;Hallucinate&quot; is a terrible word choice here, suggesting as it does that the language model has *experiences* and *perceives things*. \u003Cbr\u003E\u003Cbr\u003E&gt;&gt; \u003Ca href=\"https:\/\/t.co\/oIgCZYOnSM\"\u003Epic.twitter.com\/oIgCZYOnSM\u003C\/a\u003E\u003C\/p\u003E&mdash; @emilymbender.bsky.social (@emilymbender) \u003Ca href=\"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472?ref_src=twsrc%5Etfw\"\u003ENovember 16, 2022\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-98296").innerHTML = tweet["html"];</script><p></p>
<p>When we get down to it, an LLM doesn’t perceive or experience. It infers, specifically it infers a token sequence that is a relatively good sampled approximation of autoregressive conditional probabilities of tokens. Or, put in a simpler way, it puts tokens next to each other to minimise a loss function defined in relation to a learned conditional probability of each token w.r.t the sequence of the aforegoing tokens. There’s no tiny chess prodigy hiding inside this Mechanical Turk.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<figcaption>Wolfgang von Kempelen’s Mechanical Turk – or at least what his debunker, Joseph Friedrich Freiherr von Racknitz thought it looked like. He was wrong about the details, correct about the principle.</figcaption>
</figure>
</div>
</div></div><p>Calling an LLM’s fabrications hallucinations therefore amounts to a category error: it imports terminology from human cognition and neuropathology into a domain where it has nothing to do. The model isn’t seeing pink elephants. It’s just guessing a sequence of words, badly as it happens.</p>
<p>Anthropomorphising the flaws of AI anthropomorphises their faculties. It is this logical implication that many seem to ignore: they may have little difficulty talking about hallucinations when they would hardly endorse the notion that there’s a consciousness residing in the 120 gigs of <code>safetensor</code> weights you just downloaded. Hardly anyone who speaks of AI hallucinations comfortably would attach much moral significance, if any, to <code>rm -rf</code>-ing a model, yet strict consistency would impel them to regard the latter as extinguishing a consciousness.</p>
<p>No matter how much we attempt to palliate the situation by emphasising how it’s ‘just a metaphor’, it’s never really <em>just</em> anything. When we rely on this lazy analogisation with neuropathology, we risk ascribing to these systems a kind of perceptual experience that can go wrong the way perceptual experiences do. Unlike a human brain, an LLM doesn’t construct a rich model of the world that occasionally diverges from reality. It only has a mathematical abstraction of its learned conditional probabilities. To treat its output errors as if they were analogous to a human’s neurological misfires is not only technically incorrect, it also muddies the waters about what such models do and don’t.</p>
</section>
<section id="the-analogy-and-the-mechanism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-analogy-and-the-mechanism">The analogy and the mechanism</h2>
<p>The second problem is that using the metaphor of hallucinations implies ideas about what is going on in an LLM, both in normal and failure modes. When we talk about an LLM ‘hallucinating’, we unwittingly carry over a whole stack of those assumptions – imagining the model as a mind experiencing a very specific kind of (mis)experience, when it isn’t and doesn’t. The metaphor of hallucinations might feel like a useful analogy, but it obscures more than clarifies. In truth, the model isn’t trying (and failing) to faithfully report on reality. It’s producing output that is stochastically correct, i.e.&nbsp;it replicates reality with relatively decent accuracy as the number of tries converges to infinity. Given the nature of that distribution, some of those attempts at replicating reality will be off. The hallucination metaphor invites us to imagine there is a kind of inner experience going on all the time, a logical entailment of asserting that hallucinations are when that experience goes wrong. Ultimately, this is a misleading cognitive narrative around the technology, one that does not match the mechanistic reality of what is, basically, autocomplete on steroids.</p>
<p>Unlike you and me,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> a generative model has no concept of truth or falsehood in its programming, not beyond minimising its loss function (which does not, incidentally, claim to be true – indeed, we intentionally train ). It’s not attempting to state facts and then failing spectacularly when it ‘hallucinates’. It’s always doing the same thing – generating plausible text. By using a term like hallucination, we spin a tale where the AI is a quasi-intelligent being with beliefs about the world, occasionally slipping on a cognitive-perceptual banana peel. The model isn’t trying to convey anything at all, let alone something it believes or believes to be true. It has no beliefs or an understanding of truth.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Assuming here that you aren’t OpenAI scraping my website, in which case <code>$deity</code> have mercy on your transformer blocks.</p></div></div><p>Ultimately, this feeds into a kind of hype-by-elision in which both capabilities and limitations are exaggerated. To the commentariat, AI is both ape and angel, Skynet and a barely capable moron that can’t count the number of ’r’s in <em>strawberry</em>, the thing that will steal your jobs and the thing that can’t figure out addition. All of that, of course, sells. This is, ultimately, a kind of Reefer Madness for the AI age, lurid prose in bright letters about a technology that is powerful but ultimately logical, complex but also mundane, and most of all, capable of being understood and analysed if one forgoes the Scylla and Charybdis of over- and underestimating it.</p>
</section>
<section id="not-sick-just-wrong-and-maybe-not-even-that" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="not-sick-just-wrong-and-maybe-not-even-that">Not sick, just wrong (and maybe not even that)</h2>
<p>The problem with the metaphor of hallucinations is that it invites us to see a pathology and respond accordingly. We’re all wrong from time to time. How we assess people being wrong in the presence of a pathology is crucial to our moral understanding. Consider the legal definition of insanity, known to everyone who had to suffer through first-year criminal law as the <em>M’Naghten</em> formula: insanity is when a defect of reason results in a person either not knowing what they’re doing (being ‘wrong’ as to their actions) or not knowing right from wrong (being ‘wrong’ as to the nature of their actions). We relieve people of legal, and sometimes moral, responsibility for their actions in that situation because of the pathology. We treat it, essentially, as a moral (albeit not legal) break in the chain of responsibility.</p>
<p>The model is not an agent (in the moral-human sense) with goals or a will, nor is it a patient suffering a perceptual and/or cognitive defect. It’s a computational system following its training objective, which is to produce a set of tokens that reflect a learned conditional probability. And that’s the essence of why the terminology of pathology is wrong: we might not like its output, it might not comport with the ground truth, but on its own terms, a ‘hallucinated’ token is not intrinsically ‘wrong’ or ‘pathological’. It is a necessary feature of the architecture. There’s nothing ‘wrong’, internally, with a hallucinating model. There are no integer overflows or computational errors, you didn’t mess up your code, the model hasn’t been wrongly trained or ran on compromised hardware. Hallucinations are the price we pay for the stochasticity of a model.</p>
<p>The other time I sort of agreed with Emily Bender was when she called LLMs ‘stochastic parrots’, and her comment here is entirely accurate. A parrot has no more understanding of the meaning of the words it utters than an LLM has. The parrot utters them because of, well, reinforcement learning: repeat a sequence of tokens (which are in this case acoustic) and get a reward. That’s actually exactly how we train LLMs. They’re not trained to pursue truth, they are trained to land somewhere in truth’s vicinity at best. Notably, LLMs are not in any way ‘told’ the truth. We train LLMs on large corpora, and we expect that they contain a reflection of reality. If the corpus is relatively well selected, from a source that has its own way of ensuring its content comports with reality, then the conditional probabilities our model learns will also largely align with reality. But to the model, it’s all the same, at least absent specific measures like RLHF to weed out nonsense. If one were to contaminate basic corpora ubiquitously trusted as reliable sources,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> models would replicate that. You can train models on self-contradictions, on nonsense prose, on propaganda, you name it – as long as token follows token, you have what it takes. What this ultimately means is that such models exist in a space of learned relative probabilities, no more, no less. They mathematically faithfully replicate those probabilities in generating a sequence of tokens. If the result does not comport with our consensus understanding of reality, that’s not some peculiar pathology internal to the model. It’s the model functioning as expected. It’s not sick, and at least on its own terms, not even wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Which is why the controversy around what ends up on Wikipedia is so relevant, and such an ignored undercurrent of the discourse around the values reflected – not held, reflected! – by LLMs.</p></div></div></section>
<section id="coda" class="level2">
<h2 class="anchored" data-anchor-id="coda">Coda</h2>
<p>In the end, metaphors matter. Like a lantern, they may illuminate our way, or they may blind us. The metaphor of neuropsychiatric pathology when one fundamentally means to say ‘wrong’ has created a kind of mystery theatre (devoid, largely, of science) that has perhaps been unwittingly spurred by, and in turn perpetuated, an air of mystery around AI. Some of it is just clumsy phrasing – Sam Altman was criticised for <a href="https://www.marketwatch.com/story/openais-sam-altman-tells-salesforces-marc-benioff-that-ai-hallucinations-are-more-feature-than-bug-1c035c52">a comment that seemed to imply that hallucinations are somehow intrinsically tied to a kind of creativity that we appreciate in LLMs</a>, but what he meant is I think closer to the fact that stochastic models necessarily yield those occasional samples from a little bit off center of the probability distribution that result in what we fancifully came to call hallucinations.</p>
<p>Hallucinations, bona fide hallucinations in humans of various levels of neuropsychiatric competence, induced or otherwise, have a rich cultural history. From the pythia of Delphi through the ergot-induced visions of mediaeval witches, saints and heretics to the paintings of <a href="https://en.wikipedia.org/wiki/Louis_Wain">Louis Wain</a> and the fiction of Hunter S. Thompson, they have always been a part of the human experience, albeit very much an extraordinary part (and often, one that took more than it gave – there are few happy endings in the civilisational story of perceiving things that aren’t there). But they are very much a peculiarly <em>human</em> experience. When we draw this tenuous metaphor, we do a disservice to AI, a disservice to those who live with hallucinations and the often quite tragic and marginalising pathologies that cause them, and not least a disservice to our own attempts at understanding how LLMs work, and how sometimes that results in these extraordinary phenomena.</p>
<p>And maybe, past all mystery and metaphor, we may just call them <em>occasionally wrong</em>.</p>
<hr>
<p>I am indebted to my colleagues at HCLTech for the discussions that led to this post. All errors and omissions are mine.</p>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Just Noise in the Neurons},
  date = {2025-06-27},
  url = {https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/},
  langid = {en-GB}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Chris von Csefalvay. 2025. <span>“Just Noise in the Neurons.”</span>
June 27, 2025. <a href="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/">https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrisvoncsefalvay\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<ol start="3" type="a">
<li>Chris von Csefalvay, 2011–. <a href="disclaimer">Disclaimer</a></li>
</ol>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>