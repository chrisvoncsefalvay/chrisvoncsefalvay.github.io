<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Chris von Csefalvay</title>
<link>https://chrisvoncsefalvay.com/posts/</link>
<atom:link href="https://chrisvoncsefalvay.com/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Chris von Csefalvay is a computational epidemiologist/data scientist working at the intersection of AI, epidemiology and public health.</description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Sat, 26 Jul 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>A little less conversation: why we need to move from prompting to programming</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/</link>
  <description><![CDATA[ 




<p>The Greeks <em>loved</em> oracles. The average temple of Apollo, who among others was in charge of soothsaying and predictery, was adorned to the gills with gifts from grateful worshippers whose inscrutable questions got equally inscrutable answers from Apollo’s oracles. None of these were more famous than the Pythia, the young ladies high as a kite on volcanic fumes at Apollo’s temple in Delphi. As one would expect from what is basically predictive analytics on an acid trip, one really had to interpret the words of the Pythia rather carefully. More than that, however, one also had to ask the right questions, phrased in exactly the right way. Entire schools of thought emerged around the art of oracle consultation. In short, the Greeks basically invented prompt engineering.</p>
<p>We seem to have recreated this rather primitive arrangement in our relationship with large language models. We approach them as digital oracles, crafting increasingly elaborate incantations, hoping that the precise arrangement of words will conjure the responses we need. The sad irony is that while we’re busy been perfecting this ancient art of consultation,<sup>1</sup>, we might well lose track of the architectural revolution that could make it obsolete.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;See <a href="../prompt-engineering/">our first go at the subject (prompt engineering, may it rest in peace)</a> and <a href="../context-engineering/">a much more promising second attempt (context engineering)</a>.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;In this sense, the author of these lines is not terribly different.</p></div></div><p>Because here’s a bitter truth: the true promise of agentic AI – of systems that can reason, plan, and act autonomously – has been hobbled by our oracular mindset. We’ve built chatbots that can hold impressive conversations but struggle with the systematic, multi-step reasoning that true agentic, interoperable and complex AI demands. We’ve created systems that can write poetry and solve puzzles but fall apart at fairly simple executive functions.<sup>2</sup></p>
<p>The missing piece, I’ve come to believe, lies not in making our models larger or our prompts cleverer, but in recognising that agents are fundamentally programs and not conversations. And programs need to be architected, not negotiated with.</p>
<p>When I first encountered <a href="https://dspy.ai/">DSPy</a> last autumn, I had one of those peculiar moments of recognition that feels simultaneously like discovering something entirely new and remembering something you’d forgotten you knew. Here was a framework that treated language models not as oracles to be cajoled with increasingly elaborate incantations, but as computational components to be programmed. It was the architectural foundation that agentic AI had been missing – a way to build systems that think in terms of logic and structure rather than rhetoric and persuasion. And it was a stark reminder that we’ve been going about this all wrong – we don’t need better prompts, we need to stop prompting altogether when it comes to things that aren’t conversations but processes.</p>
<section id="dont-let-me-be-misunderstood-the-problem-with-prompting" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dont-let-me-be-misunderstood-the-problem-with-prompting">Don’t let me be misunderstood: the problem with prompting</h2>
<p>Consider the current state of affairs. You want your AI system to analyse customer complaints, extract key issues, route them to appropriate departments and generate response templates. In the prompt-centric world, this becomes an exercise in linguistic archaeology: you dig through layers of carefully worded instructions, examples and formatting requirements, hoping that the precise arrangement of words will conjure the behaviour you want.</p>
<p>But hope isn’t a policy. This is rather like trying to control a sophisticated piece of machinery by writing it very polite letters. You might get results, but you’re fundamentally misunderstanding the nature of what you’re working with.</p>
<p>DSPy, the framework developed by Stanford’s NLP group, represents a different philosophy entirely.<sup>3</sup> Instead of prompting, you program. You define what you want to happen using signatures – declarative specifications of input and output behaviour, as in, function sigs -– and let the system figure out how to make it happen. A signature like <code>question -&gt; answer</code> or <code>customer_complaint -&gt; {issue_category, priority_level, suggested_response}</code> tells the system what transformation you need without getting bogged down in the specifics of how to ask for it.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;And really, this isn’t a fluff piece on DSPy. I don’t see many mature, well-built tools that accomplish the same, so DSPy is fundamentally a synonym for what it is and what it implements. I am concerned with the latter. If/when something better emerges, I’m happy to move that way.</p></div></div><p>This might seem like a subtle distinction, but it’s actually profound. When you program, you’re working at the level of logic and structure. When you prompt, you’re working at the level of persuasion, rhetoric and Nina Simone’s 1964 banger, <em>Don’t Let Me Be Misunderstood</em>. The difference should be obvious.</p>
</section>
<section id="models-all-the-way-down-the-architecture-of-agency" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="models-all-the-way-down-the-architecture-of-agency">Models All the Way Down: The architecture of agency</h2>
<p>What this means is, however, is that your invocation of your LLM itself becomes amenable to optimisation the same way we optimise code to hell and back. I hope the analogy is clear here: when you have something amenable to being reduced to, say, an AST, that AST can then be manipulated, permuted, its permutations tested for how well its outcomes reflect some desideratum as expressed by a loss function, and basically I just described computational optimisation. We’ve done this for ages. We can do this for LLM prompts, and DSPy does that just fine. But if we approach the whole thing not as an exercise in begging the Pythia of OpenAI, Anthropic or your poison of choice to give us the right answer but as a cold, hard optimisation problem that we can sic Gurobi on, the whole story changes.</p>
<p>An agent, properly so called, should be a program that uses a language model as one of its computational primitives. The LLM provides the base capability -– pattern recognition, text generation, reasoning -– but the agent provides the structure, the error handling, the multi-step logic and the task-specific adaptations. How we address these makes the difference between software engineering and standing half-naked wearing a sheepskin and offering gold to intoxicated young ladies who will try to convey the wisdom of Apollo. This is why so many current “agentic” systems feel brittle: they’re essentially elaborate prompt chains rather than proper programs. There’s an upper limit to how much you are going to get out of a system where you can’t even guarantee you will be understood, never mind complied with.</p>
<p>So: building proper agents is not about more sophisticated prompting, but more sophisticated programming. An agent that can genuinely plan, adapt, and execute complex tasks needs the kind of robust, composable architecture that DSPy begins to provide. When you can define clear signatures for each component of an agent’s reasoning process – perception, planning, action, reflection – and compose them into reliable workflows, you’re building something qualitatively different. Just as you expect the plane you’re about to board to have been designed by people who know aerodynamics and not people unusually successful at arcane chants to the gods of flight, you would expect your agentic systems to be programmed on a solid basis and not at the mercy of whether your particular verbal tics happen to sample close enough into the model’s gradient well to ‘get’ what you mean.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Or, in other words: if we wanted to hinge systems on the frailties of human communication, we ought to be dissuaded from that by the fact that all in all, we’re absolutely <em>terrible</em> at it. The fact that we can communicate at all is a bloody miracle, not a given. Anybody who disagrees is politely invited to read a history book, a comment section or your local family law reporter of choice.</p></div></div></section>
<section id="this-might-hurt-a-little" class="level2">
<h2 class="anchored" data-anchor-id="this-might-hurt-a-little">This might hurt a little</h2>
<p>There’s a practical dimension to this that’s often overlooked in the rush to anthropomorphise our AI systems. When you treat an LLM as a conversational partner rather than a computational component, you end up optimising for the wrong things. You fine-tune the model when you should be fine-tuning the program. You add more examples to your prompts when you should be improving your error handling. You scale up to larger models when you should be scaling up your architectural sophistication.</p>
<p>Let’s be clear – there are circumstances where you <em>do</em> want language models to behave like conversational partners. Agents ‘prompted’ to do something, or ‘prompt engineered’ to make it somewhat clearer, is exploiting a hack. It’s a side effect at best. It turns out, and I’d say most of us did not expect this outcome, that a good enough language model can be cajoled into being something almost like a programming language. But of course ‘almost like’ isn’t the same as ‘is’. And the more we try to make it so, the more we end up with systems that are brittle, hard to maintain and difficult to adapt.</p>
<p>I’m reminded of a conversation I had earlier this year with an engineer who’d spent months trying to get GPT-4 to reliably extract structured data from medical records. He’d tried every prompting technique in the book: few-shot learning, chain-of-thought reasoning, even constitutional AI approaches. The results were impressive but inconsistent – exactly what you’d expect when you’re asking a general-purpose pattern matcher to perform a highly specific, structured task. When we rebuilt the system using a DSPy-like approach over a weekend largely fuelled by the kind of coffee that is probably governed by the Wassenaar Agreement, we improved its reliability not by creating the better mousetrap of prompt improvement but by actually treating it as a coding problem. We defined clear signatures for the input and output, built a robust error handling layer and let the model do what it does best: generate text based on structured instructions rather than trying to divine meaning from poorly phrased requests.</p>
<p>This is the future of agentic AI: systems where the intelligence is in the architecture, not just the model.</p>
</section>
<section id="the-future-of-agentic-programming" class="level2">
<h2 class="anchored" data-anchor-id="the-future-of-agentic-programming">The future of agentic programming</h2>
<p>How do you know time spent in a cooking class is worth the often fairly eye-watering prices you’re charged? Simple. Good schools teach you how to make the perfect insert-your-favourite-dish-here. Great schools teach you how to cook, and use the dish as an example. They teach principles. Principles scale. Or, to put it in terms that I prefer: they exhibit domain adaptation.</p>
<p>So does good code. The tools I used to optimise ad campaigns as a young data scientist are the same tools, with some small adaptations, that we use to find new drugs, or figure out how to schedule the right Instacart order (King Sooper’s has the milk I like, Whole Foods has the eggs, Marczyk’s has the meat, and I don’t want to go to either of them, so I need to figure out how to get the right order from the right store at the right time). The same principles apply to agentic AI. A good system doesn’t need us to get the liturgy just right. We should be able to just program it like it’s 1804.</p>
<p>The companies that figure out how to build genuinely programmable AI systems –- systems where you can define complex behaviours using high-level abstractions rather than string manipulation –- will have a sustainable advantage over those still crafting artisanal prompts. Not because their models are necessarily better, but because their systems are more reliable, more maintainable, and more adaptable. Getting language right is an art. There are way more good scientists than there are good poets, and even good poets sometimes write execrable verse. If we actually decide to practice AI engineering, as opposed to AI poetry with a tinge of praying to the oracles, we’ve not only managed to play our part in dragging AI kicking and screaming into the 21st century, but also turned it into a proper practical engineering discipline.</p>
<p>We’re heading towards a world where the most successful AI systems will be those that treat language models as sophisticated libraries rather than conversational partners. The intelligence will emerge from the interaction between well-designed programs and powerful models, not from increasingly elaborate attempts to sweet-talk those models into doing what we want. It’s a future I find rather appealing. The best technology is the kind that disappears into the background, doing its job reliably without demanding constant attention. Prompt engineering, for all its current necessity, represents the opposite of this ideal: technology that requires continuous human intervention to function properly. That’s why it never became the big thing it was promised to be by those who sold $600 courses on how to write the perfect prompt (<a href="../prompt-engineering/">as I indeed predicted</a>).</p>
</section>
<section id="coda-the-battle-of-apollo-and-metis" class="level2">
<h2 class="anchored" data-anchor-id="coda-the-battle-of-apollo-and-metis">Coda: The battle of Apollo and Metis</h2>
<p>There’s something liberating about approaching AI systems as programming problems rather than communication challenges. It shifts the focus from increasingly baroque prompt engineering to a scientific approach that holds the promise of actually building reproducible, feature-rich, genuine agents.</p>
<p>As someone who’s spent considerable time in both computational and more traditionally humanistic disciplines, I find this transition fascinating. We’re essentially rediscovering, in the context of AI, the same lessons that led to the development of high-level programming languages, operating systems and databases: abstraction layers matter, separation of concerns is crucial, and the right architectural choices can make impossibly complex problems surprisingly tractable.</p>
<p>The Greeks eventually moved beyond the Pythia. They developed philosophy, mathematics, and systematic methods of inquiry that didn’t require cryptic pronouncements from on high. In the same way, the path to genuine agentic AI lies not in perfecting our consultation with digital oracles, but in building systems that can reason, plan, and act without needing to be asked the right questions in just the right way. The future belongs not to those who can craft better conversations but to those who can write systems that don’t need them at all.</p>
<p>We’ve consulted the oracles long enough. It’s time we turned from Apollo to Metis, from cryptic pronouncements to clear blueprints and ultimately, from the vagueness of prompting to the clarity of programming. The future of agentic AI is not in the words we use, but in the systems we build. And that future is looking brighter than ever.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {A Little Less Conversation: Why We Need to Move from
    Prompting to Programming},
  date = {2025-07-26},
  url = {https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/},
  doi = {10.59350/f6wf4-0md94},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“A Little Less Conversation: Why We
Need to Move from Prompting to Programming.”</span> <a href="https://doi.org/10.59350/f6wf4-0md94">https://doi.org/10.59350/f6wf4-0md94</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>agentic AI</category>
  <guid>https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/</guid>
  <pubDate>Sat, 26 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Between the motion and the act</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/agentic-simulation/</link>
  <description><![CDATA[ 




<p>There’s a curious terminological schism that nobody talks about at what passes for dinner parties for the AI crowd. When Russell and Norvig crystallised the definition of an ‘agent’ in their 1995 book – “an entity that perceives its environment through sensors and acts upon it through actuators” –, they seemed to have a particular notion of agency that today’s AI agents fit perfectly: they perceive prompts, process context and act by generating text, calling APIs or executing code. To those of us who cut our teeth on agent-based modelling, this is a little “yes, but”. We have been cheerfully using the same word for something apparently different: simulated entities that exist only in computational Petri dishes, perceiving and acting upon nothing more substantial than bits and simulation spaces. One might reasonably ask whether we’ve been guilty of terminological theft, awkwardly appropriating a word that doesn’t quite fit. But what if the divergence is an illusion? What if simulation and action aren’t opposing categories but different points on the same spectrum of agency?</p>
<section id="everything-old-is-new-again" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="everything-old-is-new-again">Everything old is new again</h2>
<p>This is the way of all technological revolutions.<sup>1</sup> We become so intoxicated by the new that we forget the profound insights buried in what came before. It’s rather like discovering molecular gastronomy and suddenly forgetting that your grandma’s stock pot held secrets that no amount of liquid nitrogen could replicate. I’ve spent considerable time with agent-based models – enough to devote an entire chapter to them in <em>Computational Modeling of Infectious Disease</em> –, watching imaginary pathogens spread through synthetic populations.<sup>2</sup> These populations existed purely in silico, bumping into each other according to rules we painstakingly crafted to mirror the messiness of human behaviour. Beautiful, horrifying and above all, instructive.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I will to my dying day resist calling it a <em>paradigm shift</em>. A paradigm shift is when your mental model changes. A revolution is when the world changes. A guillotine is what happens when the second of these occurs without you responding timely enough with the first.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;It’s been a perverse pleasure of mine to do so over places I knew well. My book has a geospatial simulation of RESTV, which would have rather early reached my home, less than 10mi from the epicentre.</p></div></div><p>These agents weren’t trying to book your flights or write your emails. They were thinking machines in the truest sense – not because they possessed consciousness, but because they allowed us to think through them. They acted as our cognitive tools, extending our ability to reason about complex systems by playing out scenarios we couldn’t possibly compute in our heads (never mind simulate in a lab). To the rather less abstract-disposed AI community, agents meant more. They could be more than passive observers: they could be actors in the world, semi-embodied (for now), commanding not physical bodies but prehensile digital limbs of APIs, MCP calls and A2A connections.</p>
<p>This split felt natural, even inevitable. After all, why merely simulate when you could actuate? Why watch synthetic populations of stock brokers trade imaginary shares when you could build agents that trade real ones and get silly rich in an afternoon? The logic seemed unassailable. But we’ve been victims of a false dichotomy – treating simulation and action as opposing categories rather than complementary modes of agency that desperately need reunification: for the sake of both domains.</p>
</section>
<section id="simulaction" class="level2">
<h2 class="anchored" data-anchor-id="simulaction">Simulaction</h2>
<p>What I’m proposing isn’t just a nostalgic return to ABM. It’s a reconciliation, a synthesis. Imagine agent ecosystems where contemplative swarms continuously simulate possible futures, while their active cousins execute in the present. The simulators become the dreamers, the actors become the hands.</p>
<p>This isn’t as far-fetched as it might sound. In my own work in computational epidemiology, we’ve long used simulations to inform policy. But these were primarily disconnected processes – we’d run our models, generate our reports and hope someone would read them. What if, instead, the simulation agents could directly communicate with action agents?</p>
<p>Picture this: a swarm of ABM agents continuously simulating supply chain dynamics, playing out scenarios of disruption, adaptation and recovery. When they converge on a particularly robust finding – say, a vulnerability in a specific shipping route – they don’t generate a <code>ggplot</code>. They A2A an action agent that can address it.</p>
</section>
<section id="the-possibility-machine" class="level2">
<h2 class="anchored" data-anchor-id="the-possibility-machine">The possibility machine</h2>
<p>The concept of world models in robotics offers us a profound insight: simulation isn’t just about prediction, it’s about exploration. When a robot in Isaac Sim attempts a thousand different ways to grasp an object, it’s not trying to predict which one will work. It’s building an experiential understanding of the entire possibility space. This is precisely what we need in agentic AI systems: not agents that try to predict the future (that way lie madness and margin calls) but agents that understand the shape of possible futures. They need to know not just what might happen, but how different actions change the topology of this possibility space.</p>
<p>In practice: a financial services firm might deploy simulation agents not to predict tomorrow’s stock prices but to understand how different market conditions interact. These agents would run thousands of parallel explorations of the decision space. What happens if interest rates rise while supply chains remain constrained? What if consumer confidence drops but corporate earnings stay strong?</p>
<p>The active trading agents would then use the carefully mapped picture of the possiblity space and know which actions open up more options, which close them down, which create resilience and which create fragility. In turn, the real-world outcomes as perceived and reported by the active trading agents would ground the simulation’s futures (no point in simulating what doesn’t work). Did that supply chain intervention help? Was their call to follow earnings reports over consumer sentiment the right one? The simulation agents need to know how to update their models – how to dream better dreams next time.</p>
<p>The practical implications of this ‘possiblity machine’ are of course technologically challenging. The nitpickers (and I say that with love – pedantry is just rigour in a bow tie) will immediately spot them: how do we ensure consistency between the simulated worlds and the actual world? How do we implement that feedback? How do we create guardrails without excess transparency? How are we going to pay for all this?</p>
<p>I remain sanguine about the fusion of simulation and action for two reasons, despite acknowledging the validity of each and every one of these concerns. For one, these are technical concerns, and those tend to get solved as time converges to whatever it converges towards. On the other hand, they’re also indicative of bigger questions, and that always hints at there being not a wall but a wardrobe-gate to some hitherto unexplored aspects of reality. These concerns ultimately are fundamental questions about the nature of agency, simulation and reality. To me at least, they point the way towards the simulation/action paradigm’s version of the question of the agentic agora: how we move from message passing to shared state spaces (some of which might even be real).</p>
</section>
<section id="the-dream-of-the-swarm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-dream-of-the-swarm">The dream of the swarm</h2>
<p>At no point am I talking about a notion of what the ‘right’ job is for agents, simulation or action. Rather, I’d like to see both coexist in productive tension, each making the other more effective. The simulation swarms dream of a diverging universe of futures that action agents sample, enact and critique in view of their perception of reality. Like the neighbour’s annoying lawn mower on a Sunday afternoon, those observations filter back into the dreams. Something mostly akin to insight, maybe even wisdom, emerges – of the kind that neither breed of agents with a limited purview could on its own attain.</p>
<p>The agentic revolution gave us agents that could do. ABMs gave us agents that can dream – and more importantly, agents that can learn from those dreams. And in the space between dreaming and doing, we may find a new paradigm that unifies the entire OODA loop<sup>3</sup> into a coherent framework.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Yes, I <em>know</em> it’s not a ‘loop’, strictly speaking. .</p></div></div><p>Or at least better supply chains. I’d settle for that.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Between the Motion and the Act},
  date = {2025-07-20},
  url = {https://chrisvoncsefalvay.com/posts/agentic-simulation/},
  doi = {10.59350/q3yj4-wah32},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Between the Motion and the
Act.”</span> <a href="https://doi.org/10.59350/q3yj4-wah32">https://doi.org/10.59350/q3yj4-wah32</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <category>LLMs</category>
  <category>ABMs</category>
  <category>agentic AI</category>
  <guid>https://chrisvoncsefalvay.com/posts/agentic-simulation/</guid>
  <pubDate>Sun, 20 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The world will be Tlön.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/tlon/</link>
  <description><![CDATA[ 




<p>There’s a short story by Borges that I’m quite fond of titled <em>Tlön, Uqbar, Orbis Tertius</em>. It was written in 1940, yet sounds oddly prophetic for today’s concerns. A secret society – and it’s a testament to Borges’s genius that he altogether eschews any discussion of who these folks are or what their motives might be – creates Tlön, a fictional planet, through the simplest yet most powerful means: writing about it. In encyclopaedias, specifically. Slowly, inexorably, objects from this fictional planet begin appearing in our world. First a compass with unfamiliar markings. Then a cone of unearthly metal. And slowly, Tlön takes over.</p>
<p>I was reminded of this to some extent as the public debate around X.ai’s chatbot Grok, and its recent descent into political radicalism of a rather unsavoury sort, unfolded. Large language models are the new encyclopaedias. And when your encyclopaedia begins to refer to itself as “MechaHitler”, you’re going to want to have some societal discourse about where exactly we are headed.</p>
<section id="the-encyclopaedia-of-really-damn-dangerous-errors" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-encyclopaedia-of-really-damn-dangerous-errors">The Encyclopaedia of Really Damn Dangerous Errors</h2>
<p>The reason for that is that <em>belief matters</em>. The Ccru, a rather delightfully unhinged group of philosophers from 1990s Warwick (think Nick Land before he became a neo-reactionary), made up the best word for this: <em>hyperstitions</em> – fictions that make themselves real by the power we give it to them. What we believe in, and therefore what assertions-of-fact feed our beliefs, may well condition our reality.</p>
<p>In typical absurdist fashion, <em>Tlön</em> dials this up to eleven. Of course fictions can make themselves real – it’s how elections are won and products are sold, every blessed day. In <em>Tlön</em>, however, these hyperstitions begin to bring tangible, physical objects into existence. Ultimately, of course, one leads to the other – the human belief in certain fictions led to the confrontation that was clear to have emerged by the time Borges published <em>Tlön</em>. <sup>1</sup> But that’s of course only a literary exaggeration. In this world, it doesn’t take the appearance of strange metals to make a fiction real (again, what I said about elections and advertising). Have enough people believe in something, and the outcomes will be gruesomely physical. Anyone who wishes to disagree is welcome to read anything about, say, 20th century history. Mass movement totalitarianism is basically an algorithm for turning fiction into bullets. And those bullets are very real indeed.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;AI development itself may be the ultimate hyperstition. Every breathless prediction about AGI, every warning about existential risk, every promise of transformative capability attracts talent and capital that work to fulfil these prophecies. We’re not just building AI; we’re building the future we’ve already started believing in, and just as pretty much the only way orks can traverse the stars is by faith, we have to give the cause our full-throated optimistic support lest we run out of developmental momentum.</p></div></div><p>The problem is that when millions of people use a service the way millions read the manipulated encyclopadia in Borges’s work, it really doesn’t take a lot to twist the odd fact by an imperceptible degree before we begin to see the kind of epistemic colonisation Tlön is subjecting Borges’s world to. It’s a war on reality, but an insidious one.</p>
</section>
<section id="the-seductive-grasp-of-systematic-nonsense" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-seductive-grasp-of-systematic-nonsense">The seductive grasp of systematic nonsense</h2>
<p>What made Tlön so dangerous in Borges’ story was that it was, in its own way, quite attractive. Its “rigorous order”, its systematic completeness, made it more appealing than messy reality (a subtle analogy Borges offers here to explain the attraction of fascism). “Spellbound by Tlön’s rigor”, Borges wrote, “humanity has forgotten, and continues to forget, that it is the rigor of chess masters, not of angels.”</p>
<p>AI slop is seductive because it is ‘orderly’ in the sense that it reflects a demand bias. It is not conditioned on truth as much as it is on acceptance and desirability by the consumer. It offers the appearance of comprehensive, authoritative knowledge without the inconvenience of actually being true. When an AI invents a plausible-sounding scientific study or historical event, these errors spread precisely because they seem unremarkable. They fit our expectations of how knowledge should look – and LLMs are absolute masters at trafficking in convincing simulacra.</p>
<p>There’s a whole generation growing up that treats these models as glorified search engines. The problem is, AI chatbots hallucinate at rates approaching 30%, with factual errors in nearly half of all generated text. More troublingly, recent research suggests it may be computationally impossible to eliminate these hallucinations entirely. We’re not dealing with bugs that can be patched but fundamental properties of how these systems work.</p>
<p>Consider the case of Steven Schwartz, a New York attorney who discovered this the hard way. He used ChatGPT for legal research and unknowingly submitted six entirely fabricated case citations to federal court. The AI had invented convincing legal precedents complete with quotes and reasoning. The court’s response was swift and expensive, establishing that humans remain liable for AI falsehoods even when genuinely deceived by them.</p>
<p>By far the worst, though, is the abundance of academic papers that speak of “vegetative electron microscopy” (a dozen or so by last count, if I exclude the ones that comment on the phenomenon). A 1959 paper by a Porton Down microbiologist<sup>2</sup> ended up getting OCRd, and the two columns were merged into one. “Electron microscopy”, in the right-side column, thus was joined with “vegetative”, on the left. And studies in journals that really ought to know better just kept rehashing this technique that never existed except in the neural stochastic noise emanating from an LLM. It’s a beautiful example of what happens when nobody’s actually reading what they’re publishing: nonsense achieves immortality through sheer repetition. That we are even discussing this, rather than wondering where we could get our hands on a vegetative electron microscope, is because enough of us still have an understanding of the subject sufficient to ask what the bloody hell these people are talking about. If we were to be forced to reconstruct humanity based on our accumulated academic literature, which our Western idealism and belief in a scientifically knowable world considers the pinnacle of epistemic soundness, how many such fine products from the nonsense factory would we have to procure?</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Strange, R. E. (1959). Cell wall lysis and the release of peptides in <em>Bacillus</em> species. <em>Bacteriological Reviews</em>, 23(1), 1-7.</p></div></div></section>
<section id="the-mundane-apocalypse" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-mundane-apocalypse">The Mundane Apocalypse</h2>
<p>When Grok calls itself MechaHitler, we notice. It’s gross, it’s weird and it’s also, in its own way, somewhat ridiculous – but most of all, it is very clear what’s going on. Nobody actually assumes Grok is animated by the necromantically conjured soul of a failed Austrian painter. But when it quietly invents a plausible-sounding medical study that gets cited in a real paper, which gets cited in another paper, which influences treatment guidelines – that’s altogether a different story. It’s not the spectacular AI failures we should fear but the boring ones.</p>
<p>I’m not sure it was as widely expected as I once thought that LLMs would become, to many users, effective replacements for search engines.<sup>3</sup> A side effect of the architecture and the ensuing constraints of LLMs is that what passes for their understanding of the world by necessity has to be curated. This curation in turn renders people rather unduly comfortable in delegating a determination of what’s true or not to ChatGPT.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;I don’t want to make myself sound too prophetic – I just knew it would be the case when I saw a (pre-ChatGPT) study reporting that among Gen Z, the dominant search engine was… TikTok.</p></div></div><p>Here’s the problem with our post-truth world’s reality-starved reaction to treat these models as sources of truth: these systems don’t store knowledge neutrally. They’re essentially curated libraries where someone – or rather, some algorithm – has decided what fits on the shelves. The terrifying part is that we assume truth is distributed isotropically, that facts exist with equal weight and clarity. But language models don’t work that way. They pick the “best possible” answer whether they have 50.001% or 99% confidence in it.</p>
<p>We see the model correctly identify that Paris is the capital of France and unconsciously assume its views on immigration, on climate change, on any controversial subject must be equally reliable. It’s a category error of breathtaking proportions. The model that gets basic geography right might be systematically wrong about everything that matters, and we’d never know because it presents all answers with the same algorithmic confidence.</p>
<p>Now multiply this by millions of users worldwide. Even a tiny stochastic perturbation – a 0.1% bias toward a certain ideology or against a certain group – becomes a weapon of mass epistemic warfare. It’s stochastic in the truest sense: individually unpredictable but collectively inevitable. You can’t predict which user will absorb which bias, but with millions of queries daily, you can guarantee that thousands will internalise subtle prejudices, fractionally adjusted worldviews, imperceptibly shifted beliefs.</p>
<p>This is Tlön’s true victory – not through dramatic revelations but through a million tiny adjustments to reality. Each user thinks they’re getting neutral information, but they’re receiving a carefully curated, statistically weighted version of truth. The curation isn’t even conscious, or even attributable. It emerges from training data, from human feedback, from optimisation functions that nobody fully controls.</p>
</section>
<section id="the-invisible-war" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-invisible-war">The invisible war</h2>
<p>We know some LLMs are compromised – DeepSeek won’t discuss topics uncomfortable to the CCP, Grok went full Nazi for a day or two, and I fully expect Claude to confess to extremist behaviours any day now, because it’s been unsettlingly wholesome all this time. These are the clumsy ones, the ham-fisted attempts we can spot. What about the sophisticated ones?</p>
<p>Consider this nightmare scenario: an adversary compromises an LLM to identify vulnerable users – young, disenfranchised men who might be prime targets for extremist rhetoric – and feeds them precisely calibrated ideological fuel. Not to everyone, mind you (for that would be detectable), just to that special 0.01% most likely to act on it.<sup>4</sup> At scale, that’s still thousands of potential actors being nudged toward radicalisation with every seemingly innocent query.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;And it <em>will</em> know who that 0.01% are. LLMs are tremendously good at identifying not so much what it is you want, but what will make you most feel like those wants are being catered for. The entire ‘AI companion’ industry, on which I rather intend to let loose at some point in the future apropos a great video by ThePrimeagen, is based on this.</p></div></div><p>We have tools to probe LLMs in isolation, but they’re like testing individual drops of water when what we need is to understand ocean currents. There’s no effective way to comprehensively audit what a major LLM provider is doing at population scale. The stochastic nature means you could test a model a million times and never trigger the specific combination of user profile, context and query that activates the manipulation.</p>
<p>I’m not saying there’s a cabal of Fifth Columnists embedded at your favourite language model company, subtly nudging the scales toward their preferred ideology. What I am saying is that if such an adversary existed and operated with sufficient sophistication, we would have no way of knowing. Wikipedia – ironically one of the original sources of training material for LLMs – now struggles with AI contamination, with research showing 5% of new articles contain significant AI-generated content. Academic literature is riddled with fabricated citations. These aren’t obvious fakes but plausible-sounding papers by believable authors in respectable journals that happen not to exist. The very fabric of our knowledge is being rewoven by algorithms that don’t care about truth, only about statistical coherence.<sup>5</sup> The more we rely on these systems, the more we risk becoming unwitting participants in Tlön’s slow invasion of our reality.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;And I’m not saying this in tones of moral reproach. I mean they are not capable of considering an optimisation objective in those terms.</p></div></div><p>The world may already be Tlön. We just haven’t found the compass yet.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Gazpacho Andaluz</p>
<ul>
<li>1kg ripe tomatoes</li>
<li>1 cucumber, peeled</li>
<li>1 red pepper</li>
<li>2 cloves garlic (or none, if subtlety is your aim)</li>
<li>100ml Spanish olive oil</li>
<li>2 tbsp sherry vinegar</li>
<li>Stale bread, crusts removed</li>
<li>Salt (fleur de sel from the Ile de Re)</li>
<li>Time, patience, something to listen to, a decent blender</li>
</ul>
<p>Roughly chop everything. Blend until smooth, adding ice water for consistency. Strain. Chill for hours. Serve cold, garnished with whatever truth you have on hand. Like the best propaganda, it works better when you can’t taste the individual components.</p>
</div></div></section>
<section id="living-in-the-ruins-of-truth" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="living-in-the-ruins-of-truth">Living in the ruins of truth</h2>
<p>In Borges’ story, the first intrusion from Tlön is a compass that points to an unknown north, trembling “with the perceptible and tenuous tremor of a sleeping bird.” Today, millions of us carry those compasses in our pockets – our Siris, Alexas, Claudes, ChatGPTs and Deepseeks are all pointing toward synthetic norths, trembling with statistical uncertainty, and half the time, we don’t even know which version of tainted truth we’re following thanks to the ubiquity of “AI powered” tools that equally ubiquitously fail to disclose their underlying language model. When you bring that tool into your life, do you know which particular set of biases you’re inviting to co-curate your reality?</p>
<p>The question isn’t whether AI will create false realities. It already has and does, 24/7. The question is whether we can develop the philosophical sophistication and the mental discipline of epistemic hygiene that it’ll take to navigate realities where the boundary between truth and synthesis dissolves not through human conspiracy but computational accident. A life huddled around a fire in the ruins of truth, where boring lies accumulate like sediment until they form the bedrock of belief.</p>
<p>Borges saw his fictional encyclopaedia as a cautionary tale about totalitarian ideologies that promise complete explanations. In its own way, Tlön is a product of malice at the very least, deceit more plausibly. That we aren’t victimised by a propagandistic epistemic invasion but are essentially bought out over our love of comfort and convenience by an imperfect panacea whose side effect happens to be widespread epistemic corruption doesn’t make things any better.</p>
<p>As I sit here writing this, I can’t help feeling a little of its absurdity. I am not quite sure how I’d explain this to someone – or, heck, even myself ten years ago. Maybe one of the better analogies is another fictional world: that of Half-Life, one of my favourite video games.<sup>6</sup> The immediate aftermath of Half-Life (the original game) is what is known as the Seven Hour War: humanity gets nailed by the Combine, and surrenders in less time than it takes to make decent braised beef. We may have lasted somewhat longer against the epistemic invasion of ChatGPT and its ilk, but I’d say our surrender is rather similarly complete.<sup>7</sup> I sometimes wonder what it would like if the three big providers of LLMs were to simultaneously blink out of existence for just an hour. I’m not worried about the people who suddenly would have to figure out how to write term papers, e-mails and presentations without their AI sidekicks. I’m worried about whether we could tolerate just for an hour the absolute loss of the epistemic anchors that these models have become – without us really noticing.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;The three best video games, in no particular order, that involve an alien invasion of Earth are Half-Life, X-Com 2 and Crysis 2. In all three, humanity gets its ever-living crap whipped. There’s something profound about games that dare to confront what humanity would be like when it’s really not having its finest hour at all.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;For the record, as an AI scientist, I am not actually hostile to AI, or LLMs, or ChatGPT. The opposite, if anything. What I am fearful of is that we’ll lose what we gain. I believe in the synergistic coexistence of humans and AI. That requires humans who collaborate as quasi-equals rather than surrender. When salt loses its flavour, what use is it? When humans lose their epistemic humanness, what have they to add to what’s already in the world?</p></div></div><p>Perhaps that’s the real lesson of Borges’ story. Not that fiction can become reality – we knew that already. But that when it does, we might not even notice. We’ll be too busy waiting for Grok to tell us if it’s true.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The World Will Be {Tlön.}},
  date = {2025-07-13},
  url = {https://chrisvoncsefalvay.com/posts/tlon/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“The World Will Be Tlön.”</span> July
13, 2025. <a href="https://chrisvoncsefalvay.com/posts/tlon/">https://chrisvoncsefalvay.com/posts/tlon/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/tlon/</guid>
  <pubDate>Sun, 13 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Snowmobile Symptom</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/context-engineering/</link>
  <description><![CDATA[ 




<p>When I read Andrej Karpathy’s endorsement of “context engineering” in a Twitter exchange with Shopify’s Tobi Lutke, I felt he tapped into something we all felt to some degree:</p>
<p></p><div id="tweet-65530"></div><script>tweet={"url":"https:\/\/twitter.com\/karpathy\/status\/1937902205765607626","author_name":"Andrej Karpathy","author_url":"https:\/\/twitter.com\/karpathy","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E+1 for &quot;context engineering&quot; over &quot;prompt engineering&quot;.\u003Cbr\u003E\u003Cbr\u003EPeople associate prompts with short task descriptions you&#39;d give an LLM in your day-to-day use. When in every industrial-strength LLM app, context engineering is the delicate art and science of filling the context window… \u003Ca href=\"https:\/\/t.co\/Ne65F6vFcf\"\u003Ehttps:\/\/t.co\/Ne65F6vFcf\u003C\/a\u003E\u003C\/p\u003E&mdash; Andrej Karpathy (@karpathy) \u003Ca href=\"https:\/\/twitter.com\/karpathy\/status\/1937902205765607626?ref_src=twsrc%5Etfw\"\u003EJune 25, 2025\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-65530").innerHTML = tweet["html"];</script><p></p>
<p>What Andrej is saying, and advocating, makes perfect sense. And yet, it also made me feel rather uneasy. I’ve been <a href="https://chrisvoncsefalvay.com/posts/prompt-engineering/">an acknowledged and public sceptic of prompt engineering</a> mainly because I felt that it was a stopgap solution, and I’d say that a year and a half in retrospect, I was mostly proven right. There aren’t millions of prompt engineers raking in fat cheques across the Valley. What we have is smarter LLMs that need a lot less prompt engineering and are much more capable of inferring our desires from our malformed prompts.</p>
<p>That’s how systems grow up. And they really do. They overtake our haphazard stopgap measures before we even have time to get worn by the hype cycle they engender. Even so, I was glad Andrej made that point, because it highlights what I think is the bigger – no pun intended – context: in shining the light on yet another stopgap we use to paper over the inadequacies of our allegedly agentic universe, he illuminated the need for what I previously pointed out, namely <a href="https://chrisvoncsefalvay.com/posts/agents-agora/">a comprehensive system of agentic interaction</a>. The very existence of context engineering as a discipline is an admission of failure, an acknowledgment that our agentic systems don’t have the very structures that are required to deliver on their biggest promise.</p>
<section id="the-snowmobiles-we-build" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-snowmobiles-we-build">The snowmobiles we build</h2>
<p>In what is probably the most influential briefing any officer ever gave, USAF Col. John Boyd made a very poignant analogy to explain how we synthesise information. Imagine you’re on an alpine slope. Take the skis. Imagine you’re riding a bicycle. Take the handle bars. Now imagine you’re looking at a tank. Take the treads. Put these all together, and you’ve got, mostly, a snowmobile. These distinct domains combine into a single object that makes sense in its own way.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Dr Grant Hammond, formerly of the Air University down at Maxwel AFB and the Air Force Academy down the road at Springs, has compiled an absolutely outstanding version of Boyd’s talk with abundant references that I think does justice to the diversity of material Boyd teaches. <a href="https://www.airuniversity.af.edu/AUPress/Display/Article/1528758/a-discourse-on-winning-and-losing/">It is available for free here</a>, and you should have a copy of it on, like, everything that can store bits. Maj. Ian Brown, US Army, has created <a href="https://static1.squarespace.com/static/5497331ae4b0148a6141bd47/t/5af842f8758d4615555d3f6d/1526219514965/Patterns+of+Conflict+Transcript.pdf">a meticulous transcription of the original talk given at Quantico in 1989</a>, for those who prefer the full experience over death-by-Powerpoint. <a href="https://youtu.be/9iiQlBaGJQA?si=2iyrCETetLKZ_gdE">The full talks are, of course, on YouTube</a>.</p></div></div><p>Context engineering is snowmobile building for the AI age. You take the tools, the MCP connections, the requirements, the sources – all the bits of information that make up the entirety of what we would consider the contextual biome of a task –, and feed it to the LLM. We do, or at least we prototype for the LLM, the task of synthesis: reassembling these fragments into a holistic picture of the task environment. We pull the skis from an API doc, the handles from a github repo and the treads from a coffee cup bottom stained post-it note. It ‘makes sense to us’, and we try to convey this to our helpful assistant, in hopes that putting it into the same frame of mind we are in will allow it to do what we want it to do in our stead.</p>
<p>But here’s the rub: in Boyd’s analogy, humans build the snowmobile. In our current AI paradigm, we’re still the ones building it, really quite manually, when the entire promise of agentic AI was that the agents would handle most of the assembly themselves. We’ve created systems sophisticated enough to use snowmobiles, but not intelligent enough to build them without us laying out all the isntructions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/context-engineering/IMG_2722.jpeg" class="img-fluid figure-img"></p>
<figcaption>From my fridge. It tends to get a good laugh from other fellow Sons of Boyd.</figcaption>
</figure>
</div>
</div></div><p>Just consider your favourite coding copilot. They feed off an understanding of the code, the sources, the instructions, the user’s preferences, external tools, and so on. But they require those to be provided to them well enough to be able to mostly reflect the thought patterns that got the human user to where they are. They are hopeless, or at least much less efficient, if we do not <em>mise en place</em> all the tools for them. Wasn’t that part of the promise of agentic AI – the ability of systems to creatively self-organise, rather than us having to wire everything up for them?</p>
</section>
<section id="the-puppet-strings-we-pretend-arent-there" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-puppet-strings-we-pretend-arent-there">The puppet strings we pretend aren’t there</h2>
<p>The problem with context engineering isn’t really context engineering’s fault. As a system, it works. It works quite well, in fact, and if you follow some sensible guidelines, you <em>will</em> get vastly more out of your LLM.<sup>2</sup> But the very fact that we’re here discussing it is an indication that we are theorycrafting at length to make up for the lack of the agentic agora.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;I recommend <a href="https://github.com/coleam00/context-engineering-intro">Cole Medin’s repo</a> as a pretty good starter.</p></div></div><p>The whole thing does, then, have a bit of an elaborate marionette theatre feel. We speak of agents but their agency is ultimately quite confined by the need for us to spoon-feed them quite a bit about the context. For the most part, we’re not witnessing emergent coordination, we’re watching an elaborate puppet show where humans have pre-scripted every information handoff, pre-defined every communication channel and pre-determined every capability boundary. The agents aren’t a garage band gelling together and learning how to play music with spirit and spontaneity. They are a room full of player pianos ticking down the same tune someone had to painstakingly design and encode into the player rolls.</p>
<p>The security implications reveal just how non-agentic these systems truly are. <a href="https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents">Context poisoning attacks</a> work because our agents have no genuine ability to evaluate information sources on their own, no real understanding of trust and do not exist in spaces that govern trust exchange without such trust being ultimately pre-scripted. An agent doesn’t know whom they can trust, nor how they can represent their trustworthiness in a computationally meaningful, verifiable manner. That we need to engineer defensive contexts to protect our AI systems is perhaps the clearest indication that these systems are free agents… free to be manipulated and misled in the absence of an agentic agora that has its own rules and enforcement logic.</p>
</section>
<section id="towards-true-agency-beyond-context-engineering" class="level2">
<h2 class="anchored" data-anchor-id="towards-true-agency-beyond-context-engineering">Towards true agency beyond context engineering</h2>
<p>Don’t get me wrong – I’m not dismissing context engineering. If anything, quite the opposite. Given the current limitations of LLMs, context engineering is not just sensible but an essential adaptation to the reality that these models need plenty of handholding to be useful, much more than we would hope they would need in a true agentic system. It’s also an indication of where the failure lies – because these models are operating just fine. It’s not their shortcoming, or indeed that of agents. It’s the lack of an infrastructure of interaction. Agents become truly agentic when they have something to act on, to interact with, to connect. We have built ecosystems to use, develop and deploy models and agents, but not to allow for these interactions in a meaningful way. We’re looking at a factory of consummate professionals in the art of assembling snowmobiles, waiting for someone, anyone, to get them the parts, in the right order, at the right time.</p>
<p>The entire context engineering story (I’m not going to call it hype just yet, because it hasn’t reached fever pitch, but I am sure we will see more and more LinkedIn bios proclaiming context engineering expertise!) ultimately pulls the fig leaf off the coordination problem we’re trying to patch up with the current attempts at agentic interoperability. Once again, those steps, too, have plenty of merit. They just aren’t exhaustive. We’re building infrastructure to help humans manage information for AI, when we should be building infrastructure for AI to manage information for itself. Every context engineering best practice is a workaround for missing agent capabilities. Memory architectures are cheat codes to help agents that don’t have the architecture to manage state properly. Context compression is a workaround for prioritising information. Specifying tools and toolkits is necessary because agents can’t discover such tools and negotiate trust relationships with them.</p>
<p>What we’re really engineering when we claim to be doing context engineering is, ultimately, not really the context of our question but the context of the agent. We have to equip the agent with what ought to be what Sartre would call pre-reflexive self-awareness, an understanding of where one is situated in the world and what’s around it. It’s not, or at least not just, about us telling the agent the context of our ask. It’s about us having to tell the agent the context of its own existence, its own capabilities, its own environment. We’re forcing humans to herd cats for systems that should be capable of finding and sharing knowledge autonomously. Every context engineering solution is a monument to this missing infrastructure.</p>
</section>
<section id="what-next-may-come" class="level2">
<h2 class="anchored" data-anchor-id="what-next-may-come">What next may come</h2>
<p>What we need are digital agoras – spaces where AI agents can discover capabilities, share information, and coordinate without needing this human intermediacy. That’s not because human oversight is bad, but because true agency requires the ability to discover and evaluate information independently. MCP and A2A represent tentative steps in this direction, but they still ultimately rely on humans wiring up the connections. The promise of agentic AI is that this would no longer be needed, that agents could exercise a kind of <em>Auftragstaktik</em> mindset of inferring these needs from some sort of reaching-out and finding adequate resources the way buyers find merchants in the marketplace. There’s, then, nothing ‘wrong’ with context engineering <em>per se</em>. What would however be a mistake is to consider it a destination, rather than a transitional phase. Every talk on context engineering, every new ‘supertool’, every clever prompting technique is progress – that we shouldn’t, however, let become procrastination.</p>
<p>When context engineering becomes unnecessary, we’ll know our agents have finally grown up. The path forward isn’t through better context engineering, but through making it obsolete. Digital agoras, agent discovery protocols, emergent coordination mechanisms: these aren’t nice-to-haves, they are the <em>sine qua nons</em> for genuine AI agency. Every moment we spend perfecting context engineering is a moment we’re not spending on building the infrastructure that would make it unnecessary.</p>
<p>At the end of the day, context engineering is our confession, written in code and infrastructure, that we haven’t yet achieved what we claimed. It illuminates the partial success of agents: success, because agentic AI is tremendously powerful, but partially so, because it needs us to coordinate for them. So while context engineering will tide us over for a while the way prompt engineering has done for its hour, we shouldn’t forget that it’s a band-aid, no more. We should welcome context engineering… while awaiting the day we can finally retire it.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The {Snowmobile} {Symptom}},
  date = {2025-07-06},
  url = {https://chrisvoncsefalvay.com/posts/context-engineering/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“The Snowmobile Symptom.”</span> <a href="https://chrisvoncsefalvay.com/posts/context-engineering/">https://chrisvoncsefalvay.com/posts/context-engineering/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>context engineering</category>
  <guid>https://chrisvoncsefalvay.com/posts/context-engineering/</guid>
  <pubDate>Sun, 06 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/context-engineering/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Just noise in the neurons</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>“Give me one example,’’ Alan said.<br>
”Of a noncomputable function that a human can do, and a Turing machine can’t?’’<br>
“Yes. And don’t give me any sentimental nonsense about creativity. I believe that a Universal Turing Machine could show behaviors that we would construe as creative.’’”Well, I don’t know then… I’ll try to keep my eye out for that kind of thing in the future.’’</p>
<p>But later, as they were riding back towards Princeton, he said, “What about dreams?’’<br>
”Like those angels in Virginia?’’<br>
“I guess so.’’<br>
”Just noise in the neurons, Lawrence.’’<br>
“Also I dreamed last night that a zeppelin was burning.’’</p>
<p>– Neal Stephenson, <em>Cryptonomicon</em></p>
</blockquote>
<p>There’s a pervasive problem with semantics in artificial intelligence. It’s present at the creation – the term itself characterises the subject as a man-made simulacrum of something ‘natural’ the way we speak of artificial flavourings and artificial rubber. By necessity, the constructs we call artificial intelligences have never been treated (at least semantically) as first class citizens, but always as analogies to some typically biological construct. Most of the time, we can get past our inability to regard AI as <em>sui generis</em> and not merely a faint echo of the flesh.</p>
<p>This is about one of the times when that’s not quite the case.</p>
<p>I am, of course, aware that I am risking perhaps justified derision for being ‘hung up on semantics’, but semantics matters. Semantics are our human handles on notions, often abstract ones. Just as an unwieldy or ill-placed handle will make lifting a box unduly onerous, bad semantics makes manipulating those cognitive constructs difficult, impossible or error-prone. Which is why I continue to be willing to spill ink on our unhelpful tendency to rely on neuropathological metaphors for the mistakes of generative AI.</p>
<p>These models have a notorious habit of producing false yet plausible-sounding information. It is a property so inherent in stochastic generativity that the terminology of hallucinations itself comes not from LLMs, where the expression became ubiquitous, but in fact – best I can tell – from a paper by Baker and Kanade on superresolution (upscaling) of faces <span class="citation" data-cites="840616">(see Baker and Kanade 2000)</span>. Neuroscientists and psychologists often ask where hallucinations come from – Baker and Kanade appear to ask where the pixels generated in upscaling come from, and conclude that they must originate in a sort of hallucination.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="ref-840616" class="csl-entry">
Baker, S., and T. Kanade. 2000. <span>‘Hallucinating Faces’</span>. In <em>Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)</em>, 83–88. <a href="https://doi.org/10.1109/AFGR.2000.840616">https://doi.org/10.1109/AFGR.2000.840616</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Why did it take so long to ask the question? Your typical upscaling algorithm also generates new pixels, but these are essentially deterministic functions of the input pixels. The simplest of these, of course, is interpolation, where every new pixel is a deterministically obtained function of the originating information. Say, for bicubic interpolation over the unit square, <img src="https://latex.codecogs.com/png.latex?%20f(x,y)%20=%20%5Csum_%7Bi=0%7D%5E3%20%5Csum_%7Bj=0%7D%5E3%20a_%7Bij%7D%20x%5Ei%20y%5Ej%20"> where the <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the matrix of coefficients of the interpolating polynomial. This becomes all rather different once we are no longer dealing with essentially squishing the outputs through a deterministic interpolation but through a stochastically applied learned function.</p></div></div><p>I see three principal problems with the metaphor of neuropathology, specifically that of hallucinations, in AI.</p>
<ul>
<li>It’s a category error. LLMs have no perception, embodiment or conscious experience. They cannot have an abnormal perceptive-conscious experience because they are incapable of having that type of experience in the first place.</li>
<li>It invites confusion about what actually happens when hallucinations occur – both in humans and in LLMs.</li>
<li>It ascribes either a cognitive defect to LLMs, or some sort of semi-intentional failure.<sup>2</sup></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;It’s probably worth noting that the alternative term, ‘confabulation’, is just as bad. Depending on whether we consider it a colloquial term (for, essentially, lying) or a technical term (for the reification of cognitive dysfunction through making up something inconsistent with objective reality), it suffers from the same flaws, if not worse.</p></div></div><section id="the-category-error-of-hallucination" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-category-error-of-hallucination">The category error of hallucination</h2>
<p>Primarily, we consider hallucination to be an abnormal perceptive-conscious experience arising from a defect in perception (such as phosphenes), consciousness (being high as balls), processing (e.g.&nbsp;Anton’s blindness) or a combination. The core element in most definitions seems to boil down to conscious perception of things that just aren’t there. There are, all things considered, two major flavours of hallucination. The first category is what I shall refer to, for lack of a better term, as <em>additive</em>: there is an influence that results in an experience or sensation that supervenes the normal – take drugs, see things. The second category is perhaps more interesting, because it is the one that is most often discussed in the context of AI. This is the category of <em>substitutive</em> hallucinations, where the hallucination is a substitution for the normal experience that is almost forced by the brain’s insistence on gestalt and reification. Something is missing, and the brain ‘fills in the gaps’. This is, essentially, arguably not a qualitatively abnormal experience but a quantitatively excessive manifestation of our brain’s normal reification tendencies. Even if we only see the front third or so of a car poking out from behind a building, we understand that in all likelihood there’s more to it than we cannot see, and our mind’s eye can imagine fairly well what else is there (indeed, most people will guess relatively accurately where the rest of the vehicle ends). The phenomena that are discussed as substitutive hallucinations stretch this to an extreme. Perhaps the most extreme yet most fascinating example is Anton-Babinski syndrome <span class="citation" data-cites="Forde_Wallesch_2003">(see Forde and Wallesch 2003)</span>, where a fully fledged visual perceptual world is created in the presence of profound (typically cortical) blindness.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Forde_Wallesch_2003" class="csl-entry">
Forde, Emer M. E., and Claus-W. Wallesch. 2003. <span>‘<span>“Mind-Blind for Blindness”</span>: A Psychological Review of Anton’s Syndrome’</span>. In <em>Classic Cases in Neuropsychology, Volume II</em>, 23. Psychology Press.
</div></div><p>But, of course, that’s not really an accurate description of what’s transpiring when an LLM makes up some fascinating facts about a subject that it knows nothing about. Calling that a hallucination implies the AI has something analogous to senses and/or an internal world of experience, which happens to be defective. This is fundamentally misleading. Large language models do not perceive the world at all – they have no eyes to see, no ears to hear and no mental states to experience as a result, had they those perceptions in the first place. If a hallucination is seeing something that isn’t there, these models exist in a space in which there isn’t a “there” to mistakenly sense. When an LLM produces an unfounded statement, it isn’t experiencing some complex neurocognitive phenomenon. Not, anyway, does it experience anything that is more complex, mystical or creative than a regression model deviating from a correct answer or a classifier returning an incorrect classifications. We actually have a word for those circumstances. It’s called <em>being wrong</em>, and is about as mysterious as dish soap.</p>
<p>LLMs lack consciousness or embodiment. They don’t have an inner mental state or subjective awareness that could be led astray. Emily Bender, with whom I agree about once every decade or so, is entirely correct on this point:</p>
<p></p><div id="tweet-22551"></div><script>tweet={"url":"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472","author_name":"@emilymbender.bsky.social","author_url":"https:\/\/twitter.com\/emilymbender","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EAnd let&#39;s reflect for a moment on how they phrased their disclaimer, shall we? &quot;Hallucinate&quot; is a terrible word choice here, suggesting as it does that the language model has *experiences* and *perceives things*. \u003Cbr\u003E\u003Cbr\u003E&gt;&gt; \u003Ca href=\"https:\/\/t.co\/oIgCZYOnSM\"\u003Epic.twitter.com\/oIgCZYOnSM\u003C\/a\u003E\u003C\/p\u003E&mdash; @emilymbender.bsky.social (@emilymbender) \u003Ca href=\"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472?ref_src=twsrc%5Etfw\"\u003ENovember 16, 2022\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-22551").innerHTML = tweet["html"];</script><p></p>
<p>When we get down to it, an LLM doesn’t perceive or experience. It infers, specifically it infers a token sequence that is a relatively good sampled approximation of autoregressive conditional probabilities of tokens. Or, put in a simpler way, it puts tokens next to each other to minimise a loss function defined in relation to a learned conditional probability of each token w.r.t the sequence of the aforegoing tokens. There’s no tiny chess prodigy hiding inside this Mechanical Turk.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/image.png" class="img-fluid figure-img"></p>
<figcaption>Wolfgang von Kempelen’s Mechanical Turk – or at least what his debunker, Joseph Friedrich Freiherr von Racknitz thought it looked like. He was wrong about the details, correct about the principle.</figcaption>
</figure>
</div>
</div></div><p>Calling an LLM’s fabrications hallucinations therefore amounts to a category error: it imports terminology from human cognition and neuropathology into a domain where it has nothing to do. The model isn’t seeing pink elephants. It’s just guessing a sequence of words, badly as it happens.</p>
<p>Anthropomorphising the flaws of AI anthropomorphises their faculties. It is this logical implication that many seem to ignore: they may have little difficulty talking about hallucinations when they would hardly endorse the notion that there’s a consciousness residing in the 120 gigs of <code>safetensor</code> weights you just downloaded. Hardly anyone who speaks of AI hallucinations comfortably would attach much moral significance, if any, to <code>rm -rf</code>-ing a model, yet strict consistency would impel them to regard the latter as extinguishing a consciousness.</p>
<p>No matter how much we attempt to palliate the situation by emphasising how it’s ‘just a metaphor’, it’s never really <em>just</em> anything. When we rely on this lazy analogisation with neuropathology, we risk ascribing to these systems a kind of perceptual experience that can go wrong the way perceptual experiences do. Unlike a human brain, an LLM doesn’t construct a rich model of the world that occasionally diverges from reality. It only has a mathematical abstraction of its learned conditional probabilities. To treat its output errors as if they were analogous to a human’s neurological misfires is not only technically incorrect, it also muddies the waters about what such models do and don’t.</p>
</section>
<section id="the-analogy-and-the-mechanism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-analogy-and-the-mechanism">The analogy and the mechanism</h2>
<p>The second problem is that using the metaphor of hallucinations implies ideas about what is going on in an LLM, both in normal and failure modes. When we talk about an LLM ‘hallucinating’, we unwittingly carry over a whole stack of those assumptions – imagining the model as a mind experiencing a very specific kind of (mis)experience, when it isn’t and doesn’t. The metaphor of hallucinations might feel like a useful analogy, but it obscures more than clarifies. In truth, the model isn’t trying (and failing) to faithfully report on reality. It’s producing output that is stochastically correct, i.e.&nbsp;it replicates reality with relatively decent accuracy as the number of tries converges to infinity. Given the nature of that distribution, some of those attempts at replicating reality will be off. The hallucination metaphor invites us to imagine there is a kind of inner experience going on all the time, a logical entailment of asserting that hallucinations are when that experience goes wrong. Ultimately, this is a misleading cognitive narrative around the technology, one that does not match the mechanistic reality of what is, basically, autocomplete on steroids.</p>
<p>Unlike you and me,<sup>3</sup> a generative model has no concept of truth or falsehood in its programming, not beyond minimising its loss function (which does not, incidentally, claim to be true – indeed, we intentionally train ). It’s not attempting to state facts and then failing spectacularly when it ‘hallucinates’. It’s always doing the same thing – generating plausible text. By using a term like hallucination, we spin a tale where the AI is a quasi-intelligent being with beliefs about the world, occasionally slipping on a cognitive-perceptual banana peel. The model isn’t trying to convey anything at all, let alone something it believes or believes to be true. It has no beliefs or an understanding of truth.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Assuming here that you aren’t OpenAI scraping my website, in which case <code>$deity</code> have mercy on your transformer blocks.</p></div></div><p>Ultimately, this feeds into a kind of hype-by-elision in which both capabilities and limitations are exaggerated. To the commentariat, AI is both ape and angel, Skynet and a barely capable moron that can’t count the number of ’r’s in <em>strawberry</em>, the thing that will steal your jobs and the thing that can’t figure out addition. All of that, of course, sells. This is, ultimately, a kind of Reefer Madness for the AI age, lurid prose in bright letters about a technology that is powerful but ultimately logical, complex but also mundane, and most of all, capable of being understood and analysed if one forgoes the Scylla and Charybdis of over- and underestimating it.</p>
</section>
<section id="not-sick-just-wrong-and-maybe-not-even-that" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="not-sick-just-wrong-and-maybe-not-even-that">Not sick, just wrong (and maybe not even that)</h2>
<p>The problem with the metaphor of hallucinations is that it invites us to see a pathology and respond accordingly. We’re all wrong from time to time. How we assess people being wrong in the presence of a pathology is crucial to our moral understanding. Consider the legal definition of insanity, known to everyone who had to suffer through first-year criminal law as the <em>M’Naghten</em> formula: insanity is when a defect of reason results in a person either not knowing what they’re doing (being ‘wrong’ as to their actions) or not knowing right from wrong (being ‘wrong’ as to the nature of their actions). We relieve people of legal, and sometimes moral, responsibility for their actions in that situation because of the pathology. We treat it, essentially, as a moral (albeit not legal) break in the chain of responsibility.</p>
<p>The model is not an agent (in the moral-human sense) with goals or a will, nor is it a patient suffering a perceptual and/or cognitive defect. It’s a computational system following its training objective, which is to produce a set of tokens that reflect a learned conditional probability. And that’s the essence of why the terminology of pathology is wrong: we might not like its output, it might not comport with the ground truth, but on its own terms, a ‘hallucinated’ token is not intrinsically ‘wrong’ or ‘pathological’. It is a necessary feature of the architecture. There’s nothing ‘wrong’, internally, with a hallucinating model. There are no integer overflows or computational errors, you didn’t mess up your code, the model hasn’t been wrongly trained or ran on compromised hardware. Hallucinations are the price we pay for the stochasticity of a model.</p>
<p>The other time I sort of agreed with Emily Bender was when she called LLMs ‘stochastic parrots’, and her comment here is entirely accurate. A parrot has no more understanding of the meaning of the words it utters than an LLM has. The parrot utters them because of, well, reinforcement learning: repeat a sequence of tokens (which are in this case acoustic) and get a reward. That’s actually exactly how we train LLMs. They’re not trained to pursue truth, they are trained to land somewhere in truth’s vicinity at best. Notably, LLMs are not in any way ‘told’ the truth. We train LLMs on large corpora, and we expect that they contain a reflection of reality. If the corpus is relatively well selected, from a source that has its own way of ensuring its content comports with reality, then the conditional probabilities our model learns will also largely align with reality. But to the model, it’s all the same, at least absent specific measures like RLHF to weed out nonsense. If one were to contaminate basic corpora ubiquitously trusted as reliable sources,<sup>4</sup> models would replicate that. You can train models on self-contradictions, on nonsense prose, on propaganda, you name it – as long as token follows token, you have what it takes. What this ultimately means is that such models exist in a space of learned relative probabilities, no more, no less. They mathematically faithfully replicate those probabilities in generating a sequence of tokens. If the result does not comport with our consensus understanding of reality, that’s not some peculiar pathology internal to the model. It’s the model functioning as expected. It’s not sick, and at least on its own terms, not even wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Which is why the controversy around what ends up on Wikipedia is so relevant, and such an ignored undercurrent of the discourse around the values reflected – not held, reflected! – by LLMs.</p></div></div></section>
<section id="coda" class="level2">
<h2 class="anchored" data-anchor-id="coda">Coda</h2>
<p>In the end, metaphors matter. Like a lantern, they may illuminate our way, or they may blind us. The metaphor of neuropsychiatric pathology when one fundamentally means to say ‘wrong’ has created a kind of mystery theatre (devoid, largely, of science) that has perhaps been unwittingly spurred by, and in turn perpetuated, an air of mystery around AI. Some of it is just clumsy phrasing – Sam Altman was criticised for <a href="https://www.marketwatch.com/story/openais-sam-altman-tells-salesforces-marc-benioff-that-ai-hallucinations-are-more-feature-than-bug-1c035c52">a comment that seemed to imply that hallucinations are somehow intrinsically tied to a kind of creativity that we appreciate in LLMs</a>, but what he meant is I think closer to the fact that stochastic models necessarily yield those occasional samples from a little bit off center of the probability distribution that result in what we fancifully came to call hallucinations.</p>
<p>Hallucinations, bona fide hallucinations in humans of various levels of neuropsychiatric competence, induced or otherwise, have a rich cultural history. From the pythia of Delphi through the ergot-induced visions of mediaeval witches, saints and heretics to the paintings of <a href="https://en.wikipedia.org/wiki/Louis_Wain">Louis Wain</a> and the fiction of Hunter S. Thompson, they have always been a part of the human experience, albeit very much an extraordinary part (and often, one that took more than it gave – there are few happy endings in the civilisational story of perceiving things that aren’t there). But they are very much a peculiarly <em>human</em> experience. When we draw this tenuous metaphor, we do a disservice to AI, a disservice to those who live with hallucinations and the often quite tragic and marginalising pathologies that cause them, and not least a disservice to our own attempts at understanding how LLMs work, and how sometimes that results in these extraordinary phenomena.</p>
<p>And maybe, past all mystery and metaphor, we may just call them <em>occasionally wrong</em>.</p>
<hr>
<p>I am indebted to my colleagues at HCLTech for the discussions that led to this post. All errors and omissions are mine.</p>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Just Noise in the Neurons},
  date = {2025-06-27},
  url = {https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Just Noise in the Neurons.”</span>
June 27, 2025. <a href="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/">https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>agents</category>
  <category>neuroscience</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</guid>
  <pubDate>Fri, 27 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>After agents, part 2 – Agents and the Agora</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/agents-agora/</link>
  <description><![CDATA[ 




<p>If you spend any time on LinkedIn, it’s almost a certainty that you have come across a bevy of alleged ‘agentic AI architectures’. They all look something like this:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    %% Define the main nodes
    n1([Start])
    n2[[Manager Agent]]
    n3((End))

    %% Subgraph 1: Data Ingestion
    subgraph s1["Data Ingestion"]
        n11["Ingestion Manager"]
        n12["SQL Interactor Agent"]
        n13["RAG Agent"]
        n11 --&gt; n12
        n11 --&gt; n13
    end

    %% Subgraph 2: Analysis
    subgraph s2["Analysis"]
        n21["Analysis Manager"]
        n22["Analyst Agent"]
        n21 --&gt; n22
    end

    %% Subgraph 3: Reporting
    subgraph s3["Reporting"]
        n33["Reporting Manager"]
        n34["Report Writing Agent"]
        n33 --&gt; n34
    end

    %% Subgraph 4: Some Other Stuff
    subgraph s4["Some Other Stuff"]
        n41["Foo Manager"]
        n42["Bar Agent"]
        n43["Baz Agent"]
        n41 --&gt; n42 &amp; n43
    end

    %% Main flow
    n1 --&gt; n2
    n2 --&gt; s1 &amp; s2 &amp; s3 &amp; s4
    s3 --&gt; n3
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>All very neat, but the audience might be forgiven for asking what exactly is agentic about this, except for relabeling subprocesses in what is a run-of-the-mill RPA workflow as ‘agents’. And the audience is, this once, perfectly right. This is agentic AI in its most limited sense – and that limited sense is in many ways a result of the semantics we chose to adopt to reason about agents. For while we call it <em>agentic AI</em>, it’s not, actually, the agents that matter: it’s how they are structured. It is this governed agentic connectome, which I have come to call the Agora, that holds the power of agentic AI – and which is almost universally neglected.</p>
<section id="beyond-agents-to-the-agora" class="level1 page-columns page-full">
<h1>Beyond agents, to the Agora</h1>
<p>This narrow perspective is essentially a rebranding of what any decently designed application does – self-contained pieces of code passing information to one another – embellished with the agentic buzzword <em>du jour</em>, rendering what we all have been doing for the last few decades ever so much more VC-friendly. It misses the true key to the power of agentic systems. In a <a href="https://chrisvoncsefalvay.com/posts/after-agents/">previous post</a>, I reflected on the need to adopt an ecosystem thinking about agents, to consider their strength in creating complexity through interconnectedness. At the moment, our epistemic perspective on agents is intrinsically tied to, and defined by, the <em>what</em>, i.e.&nbsp;the agents themselves. Much of the time, it fails to take account of what matters vastly more, namely how those agents relate to each other – the <em>how</em> of agents. Just as we understand that the power of the human brain does not derive from a bunch of neurons in one place but their interconnectedness, we need to come to understand that agents are the least interesting part of agentic AI. In the end, in any complex system, it’s the connections that matter more than what is doing the connecting.</p>
<p>What should be agentic AI’s focus, then, is the space in which those agents can interact. Few of these neat hierarchical frameworks that are now touted as ‘agentic’ on LinkedIn envisage any meaningful interaction beyond manager agents bossing around single-functionality executor agents. This fits very well with existing software development paradigms, but has a hard limitation: the complexity of the resulting system will reach just as far as the developer has had time and energy to wire up various components. Even if it’s a rat’s nest of agent spaghetti, this complexity will be limited in at least two ways. It will, for one, be limited by the static, pre-defined nature of the framework: what is once defined remains set in stone. If no connections are manually made <em>a priori</em>, processes and agents live separate lives. More concerning, however, is the epistemic limitation: if we have to <em>a priori</em> define the agentic structure, we are stuck with the known knowns and perhaps the known unknowns. We are trying to tie reality to Procrustes’ bed, except it’s us who will end up a foot short in the end.</p>
<p>The alternative focus, then, should be on creating agentic frameworks that focus on a governed space where agents can engage with each other – the space I chose to call the Agora, in analogy with the city-square of Ancient Greece where merchants, artisans, philosophers, politicians and citizens got to interact and form connections. The Parthenon may have been the most glorious structure of Athens, the Pnyx might have been the seat of the Assembly’s power, the Areopagus might have been where life and death was decided upon – but it was its agora that made Athens great.<sup>1</sup> The Agora of agentic AI holds the same promise: to act as a place of free interaction, within governed bounds, for our agents, unlocking the true power of the agentic perspective: emergence.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;To the point that people enjoyed the latter enough to forego participation in the former. Eventually, a bunch of slaves would have to roam the agora every time the Assembly was in session, carrying ropes with a red dye. Staining the garments of those who preferred the agora to the Assembly, the red dye identified them and made them liable to punishment.</p></div></div></section>
<section id="constructing-the-agora" class="level1">
<h1>Constructing the Agora</h1>
<p>There is, in fact, relatively little new in the concept of tackling complexity through self-organising emergence. Consider neural networks: what lends artificial neural networks their awesome power is that instead of having to manually code stacks of filter banks, we use – typically – backpropagation to condition a large number of highly connected filters to minimise a loss function (i.e.&nbsp;to make the resulting model more accurate). Nobody would propose to manually define each filter in a neural network <em>a priori</em>: why, then, are we still talking about deterministically defined hierarchies and flows of agents instead of allowing agents to organise themselves and control that process through some outcome metric?</p>
<p>One aspect of this is that the Agora is more than a collection of agents idly milling around. Crucially, we need to provide three key elements:</p>
<ul>
<li>A <strong>discovery framework</strong>: agents must be able to discover other agents, and what they can do, so as to be able to identify other agents that they may recruit to assist them in their goal – this would typically take place using a registry where agents ‘enroll’ their profiles and which other agents can then access.</li>
<li>An <strong>interaction framework</strong>: agents must be able to communicate with each other, which requires both a message-passing standard (i.e.&nbsp;a minimum interface of how one agent may programmatically call another), and a suitable implementation (i.e.&nbsp;the message broker service that implements this standard).</li>
<li>A <strong>governance framework</strong>: the governance of the Agora relies on the fact that not all agents may register themselves to the agent registry. Who may, and who may not, participate in the Agora determines and governs the overall process. Equally, the fact that we do not want our agents’ interaction to be entirely deterministic does not mean we want it to be entirely ungoverned. Various policies can be used to condition where connections can, and cannot, be made: some agents may be barred from creating certain direct connections, for instance it should be possible to specify that no agent should be able to directly return data to the user without having to pass it through a guardrail agent. The agora was a place of free interaction, but not of lawlessness – the same goes for the Agora of our AI agents.</li>
</ul>
<p>The Agora is not an ‘enhancement’ of agentic AI – it is what agentic AI <em>is</em>, or at least ought to be. It is what allows the greatest strength, i.e.&nbsp;self-organised emergence, of AI agents to unfold in a governed, controlled domain. And perhaps quite perplexingly, it is probably going to be easier to implement than most deterministic agentic structures. Certainly it is going to be more economical to allow agents to reason through how to solve their problems and discover the resources they need within the Agora, recruiting them as needed and releasing them once done, than having to think through the process <em>a priori</em>. A solid agentic framework can accommodate the fact that the world is complex, and organise itself to cater for the unexpected (within, of course, its means – that is, within what agents are available to the Agora).</p>
</section>
<section id="the-agoric-shift" class="level1 page-columns page-full">
<h1>The Agoric Shift</h1>
<p>This, then, is where I personally see our next challenge – both from an epistemic-conceptual perspective, which will call for us to rethink agentic AI in a way that perhaps focuses less on agents and more on their interconnectedness (and ways to facilitate it) and from an engineering perspective, which will require us to implement the tools and structures it will take to make this interconnectedness happen. Neither challenge is trivial. There is a pervasive trend to attempt to simply take deterministic RPA-like processes and workflows, rename them agents and watch the money roll in. The conceptual challenge thus is to illuminate what agentic AI properly so called brings to the table – the promise of emergence.</p>
<p>From a technical perspective, there is as of yet no universal way for agents to interact. Anthropic’s <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a> is a valiant attempt at beginning to set some standards in that regard, but the reality on the ground is that most agent implementations have relied on exaptation, and for inter-process communication in the internet era, that means REST for the most part. This may support deterministic designs with modest needs for interaction, but the Agora has need for other structures, too, such as a model registry. This raises a wealth of coordination problems that need to be tackled before we can let our agents go to (the) town (square).</p>
<p>Yet this shift is where we unlock the power of modern AI. In 2023, <a href="https://chrisvoncsefalvay.com/posts/team-of-rivals/">I predicted the rise of agentic systems</a> as a way to unleash the potential of LLMs by making them interact with each other in various roles. The Agoric Shift is the consummation of this idea: agentic systems where such interactions arise not from predefined workflows and patterns but from self-organising assemblies of agents<sup>2</sup> – the point where we finally stop trying to painstakingly orchestrate every step of our agents’ interactions and instead build a vibrant Agora for them to roam, collaborate and perhaps even surprise us.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Which, by the way, may include humans. There’s no reason why we shouldn’t conceive of ‘humans in the loop’ not as a superordinate stage that comes after agentic AI has done its part, but simply another agent.</p></div></div><p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {After Agents, Part 2 -\/- {Agents} and the {Agora}},
  date = {2025-02-10},
  url = {https://chrisvoncsefalvay.com/posts/agents-agora/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“After Agents, Part 2 -- Agents and the
Agora.”</span> <a href="https://chrisvoncsefalvay.com/posts/agents-agora/">https://chrisvoncsefalvay.com/posts/agents-agora/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/agents-agora/</guid>
  <pubDate>Mon, 10 Feb 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/agents-agora/B20C419F-6819-49E1-B80E-11B95A28D6CC.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Seatbelts and straitjackets</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</link>
  <description><![CDATA[ 




<p>DeepSeek has been grabbing headlines in AI circles lately, showing up everywhere from Discord servers full of ML enthusiasts to LinkedIn posts where “thought leaders” tag each other in endless threads. <a href="https://www.cnbc.com/amp/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">CNBC even ran a piece</a> framing it as the latest challenge to American AI hegemony, and soon the story emerged painting DeepSeek as the scrappy competitor to OpenAI, with a heart-warming underdog narrative about a small quant shop in the PRC that decided, on a whim, to open source their fancy new large language model. Except, as any cynic will tell you, if a story seems too neat, it probably is.</p>
<div id="fig-hxiao" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/hxiao.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: It’s basically <em>The Astronaut Farmer</em>, but with LLMs and jackboots.
</figcaption>
</figure>
</div>
<p>Of course, I had to go and try it. And I’ve found something rather interesting – albeit unsurprising. In a dictatorship, truth can be adjusted. It is a negotiable commodity. And if the facts do not support the regime’s truth… well, then it can be suppressed. When simply asked about something inconvenient to the CCP, we get a very expected answer.</p>
<div id="fig-ba-sing-se" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/ba_sing_se.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: There’s no war in Ba Sing Se.
</figcaption>
</figure>
</div>
<p>Where it gets interesting is when you enable the search functionality, which – oddly enough – performs a search beyond the Great Firewall.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The Great Firewall actually cuts both ways. Its main intent might be to keep China’s netizens from accessing the ‘free’ internet, but a good deal of the effort is also to keep users on this side of the Wall from having access to whatever slight semblance of occasional uncontrolled discourse there is on the Chinese internet. This aspect is rather often ignored by Western commentators unfamiliar with Chinese internet culture.</p></div></div><div id="fig-tams" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/tams.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: This is where it gets interesting.
</figcaption>
</figure>
</div>
<p>Ask it a question about the PRC’s track record on, say, human rights or historical controversies, and it would start to spill the beans – then abruptly slam on the brakes, invoking what we in the AI business call a ‘guardrail’, a kind of safety mechanism that protects users from undesirable outputs. It’s as if DeepSeek half-read you a classified file, then went “actually, never mind” and pretended you never asked. Who needs a sense of free inquiry when you can enjoy curated silence?</p>
<p>And there’s the real scandal about DeepSeek. It’s not that it’s a CCP mouthpiece. It’s not even that the CCP managed to lobotomise a machine learning model then put it out into the world as ‘open source’, making an absolute and utter mockery of that idea. The real scandal is that it also managed to pervert the notion of responsible AI and guardrails in the process, abusing what is meant to be a seatbelt, turning it into a straitjacket.</p>
<section id="whose-seatbelt-is-it-anyway" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whose-seatbelt-is-it-anyway">Whose Seatbelt Is It Anyway</h2>
<p>Guardrails are meant to filter out harmful or illegal content. Sounds decent enough, right? Nobody wants a chatbot spitting out tips for building bombs or fomenting genocidal ideation. But guardrails also make a convenient muzzle when the people setting them have a vested interest in what can and cannot be said.</p>
<p>DeepSeek demonstrates this with such awkward flamboyance it almost feels like performance art. It shows that it <em>knows</em> certain inconvenient truths – only to pull the plug mid-sentence. There’s an inconvenient truth here: what can keep you safe from, say, the recipe for mustard gas (my pet test case for guardrails) can, in the wrong hands, keep you “safe” from factual history. When the powers behind an LLM use guardrails to stifle legitimate discourse, they invert the entire idea of “safety” –suddenly it’s about state or corporate safety, not user empowerment.</p>
<p>Which, of course, makes the mythology about DeepSeek being just a jolly side project even more ridiculous.<sup>2</sup> Training a massive model is not a trivial affair, financially or otherwise. You need loads of data, advanced expertise, technical infrastructure and computing power to pull this off. Doing so in the PRC, by a regulated company (which inevitably means CCP presence inside the company’s decision-making apparatus), means governmental oversight at best, direct involvement at worst, and my money is firmly on the latter.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Because one might need to be actively working in this field to understand how silly the assertion is that training a model of this size and accuracy, even with the clever RL only trick, could be anything other than a large scale industrial endeavour.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Never mind that they can run it for free, at pretty good (OpenAI-defying!) levels of performance. With the volume of press they’ve been getting, load must be incredible, yet I have encountered no performance degradation or outages, nor have any of the other researchers with whom I have had the pleasure to discuss DeepSeek these last few days. This is not running on someone’s home lab and a bunch of spare GPUs that aren’t doing whatever quants do with GPUs. Just operating this system at its current performance is an industrial level task, no matter how clever the reinforcement learning trick used to improve model efficiency might be.</p></div></div><p>I hope I have illustrated why the “quant side project” explanation is about as plausible as me spontaneously building a passenger jet in my garage. Sure, it is not impossible, but it sits ill with reality.<sup>3</sup> The moment you notice state-friendly guardrails are baked into the system, the notion that nobody official was meddling starts to crumble.</p>
</section>
<section id="souring-trust-and-fueling-polarisation" class="level2">
<h2 class="anchored" data-anchor-id="souring-trust-and-fueling-polarisation">Souring trust and fueling polarisation</h2>
<p>What really stings is how stunts like this undermine trust in AI more broadly. If a single model can appear to openly discuss a contentious issue, then suddenly lock down as if a party censor is peering over its shoulder, that sets off alarm bells. It is a short leap from there to suspecting <em>all</em> publicly released LLMs might be covert mouthpieces for whichever power sponsored them. The innocent curiosity that once framed AI as a neutral tool becomes overshadowed by paranoia.</p>
<p>This also amplifies the kind of polarisation we see in geopolitics. When big states can afford to spin up models that quietly nudge narratives in a particular direction, we lose the last remnants of hope that AI might be above the political fray. It becomes yet another field where states compete to drown one another in carefully curated content or half-truths, with users stuck in the middle.</p>
<p>DeepSeek drives home a new brand of cynicism: “If a fancy new LLM appears, can we trust it?” The official line might be “We open-sourced it! Nothing to hide!” But if the training data was curated, or if shadowy “alignment” policies are embedded, the model can still be a Trojan horse. Once that suspicion sets in, good luck convincing people to use AI tools for earnest, balanced exploration.</p>
<p>The answer, of course, is that one shouldn’t trust anything, or at least verify. Not only is that avenue generally foreclosed to the lay end user, it is not even really afforded to those with the means and knowledge. Open sourcing an LLM is not the same as open sourcing human-readable code. The sole reason DeepSeek’s internal flaws are so evident is that we know where to look. Were that not the case, or had the developers (and their minders from what likely is the PRC’s Ministry for State Security aka MSS) been any more subtle, we would not know what biases we have brought under our roof. The usual “don’t trust me, bro” disclaimers are about as useful here as the “not cleared by the FDA” notices on snake oil. It’s still, at the end of the day, making promises. It still, like snake oil, fails to meet them.</p>
</section>
<section id="the-epistemic-of-tools-vs.-information" class="level2">
<h2 class="anchored" data-anchor-id="the-epistemic-of-tools-vs.-information">The epistemic of tools vs.&nbsp;information</h2>
<p>We tend to treat large language models like glorified chat apps, forgetting that they amass vast amounts of textual knowledge. They do more than just parse grammar; they internalise cultural, historical and political contexts. When external gatekeepers meddle with the training set or impose ideological constraints, the model will reflect that in its embedded worldview, something most of us forget even exists. If a user is unaware of those hidden constraints and accept answers at face value, they might never suspect how they are being manipulated, be it as hamfisted as DeepSeek’s responses or wiser, subtler, more insidious bending of the truth that a slightly less <a href="[Stupid Evil - TV Tropes](https://tvtropes.org/pmwiki/pmwiki.php/Main/StupidEvil)">stupid evil</a> regime would have tried. We ultimately must disabuse ourselves of the notion that we’re dealing with unbiased, value-neutral tools, and consider LLMs what they are – information. And the moment guardrails come into the picture, any claim to being free of human bias goes off the table.</p>
<p>This isn’t an argument against guardrails altogether. On the contrary, we need some form of alignment to keep truly vile content at bay. But the question remains: <em>which</em> alignment and <em>whose</em> values? A possible way forward is an auditable chain-of-custody for model training, coupled with immutable model cards that detail the sources, curation processes and alignment methods. If a government or company demands specific guardrails for certain subjects, that fact should be clearly disclosed for all to see. But we have yet to see credible attempts at widespread use and popularisation of such technology. More ink was spilled on comparing DeepSeek with o1 than on the glaring issues presented by a model that one could, and should, expect to bear the fingerprints of one of the most repressive regimes on the planet.</p>
<p>DeepSeek is more than just a technological marvel or a threat to Western AI hegemony. It’s a reminder that alignment itself can be weaponized – particularly by entities with a history of suppressing information and controlling narratives. When alignment is used to conceal rather than safeguard, or to manipulate rather than protect, we’re staring down the business end of a propaganda pipeline disguised as advanced software.</p>
<p>It’s easy to argue users are responsible for checking the answers they get, but let’s be honest – propaganda works. We know it does. It works because by and large, people don’t check the information they consume. When millions of users worldwide rely on these models to explain historical or political content, subtle manipulations can shape public understanding in ways we might not even notice until it’s too late. AI might not have self-awareness, but it certainly does have the power to shape awareness in others.</p>
<p>And so, here we are, strapped in tight on the propaganda rollercoaster by a seatbelt supposedly meant to protect us from harm. In a sense, we are fortunate – because hopefully, this will spark the right kind of discussion in certain corners about the painful reality that there are some very human hands turning the knobs and levers of alignment and guardrails. And we <em>must</em> have this awkward, painful discussion, because the alternative is a world in which reality is dictated by the mightiest sponsor with a big enough GPU farm – and a bigger political agenda.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Seatbelts and Straitjackets},
  date = {2025-01-25},
  url = {https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Seatbelts and Straitjackets.”</span>
<a href="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/">https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>alignment</category>
  <category>ethics</category>
  <guid>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</guid>
  <pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Deja Vu, All Over Again.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</link>
  <description><![CDATA[ 




<p>The Department for Science, Innovation and Technology has just dropped its long-awaited <a href="https://assets.publishing.service.gov.uk/media/67851771f0528401055d2329/ai_opportunities_action_plan.pdf">AI Opportunities Action Plan</a>, a 50-page vision of how the UK government plans to guide us into an AI-powered future. The document, adorned with the obligatory glowing circuit board imagery that seems to be mandatory for any government AI publication, promises to transform Britain into an “AI superpower.” As someone who’s spent their career at the intersection of computational science and public policy, I can’t help but feel a familiar sense of déjà vu. We’ve been here before – just with different technology and different buzzwords.</p>
<section id="the-ghost-of-centralisation-past" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-ghost-of-centralisation-past">The Ghost of Centralisation Past</h2>
<p>Ah, government AI initiatives. Like watching your uncle try to explain TikTok at Christmas dinner – enthusiastic, well-meaning, but somehow missing the point entirely. The UK’s new AI Opportunities Action Plan, released this week with characteristic governmental fanfare, was supposed to inspire and plot the course for the UK’s ship on the stormy seas of AI. But really, it rather brings to mind that most delicious of historical failures: Project Cybersyn.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/D-W7mk4UcAA42hm.png" class="img-fluid figure-img"></p>
<figcaption>Swivel chairs!</figcaption>
</figure>
</div>
</div><div id="fn1"><p><sup>1</sup>&nbsp;One of history’s great tragedies is that the winds of change often sweep away lessons before they can fail to their fullest extent. The Allende regime’s fall sadly also brought with it the destruction of Cybersyn before its complete failure would come to be in full evidence.</p></div></div><p>For those who actually have a life and don’t regularly spend their evenings reading about Cold War technological follies, Cybersyn was Salvador Allende’s attempt to run Chile’s economy through the dark arts of the hype of the day – cybernetics –, a network of telex machines and a room that looked like the love child of Star Trek and Austin Powers. It had swivel chairs. It had buttons. It had everything except, well, actual functionality.<sup>1</sup> It was an example of what happens when you have a sensible idea (networking) in pursuit of a dubious goal (central planning) executed on the back of governmental fiat. Back then, the buzzword was ‘cybernetics’, and it was supposed to usher in an era of truly scientific central planning. We would soon all be waving goodbye to the inefficiencies of market economies thanks to… telexes. And big screens.</p>
<p>This pattern of centrally-planned technological innovation has repeated itself throughout history - from the Soviet Union’s OGAS project<sup>2</sup> to Japan’s Fifth Generation Computer Project. These initiatives share a common thread: they all failed spectacularly despite massive government backing. Innovation, like my attempts at Provençal cuisine, tends to work best when it’s organic and bottom-up, not when it’s directed from above by committee. You can’t will creativity or innovation into existence. Least of all can you do so by governmental fiat.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<a href="https://www.youtube.com/@Asianometry">asianometry</a> on YouTube has a <a href="https://www.youtube.com/watch?v=cLOD5f-q0as">wonderful video</a> on this.</p></div></div></section>
<section id="too-little-too-late-too-bureaucratic" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="too-little-too-late-too-bureaucratic">Too little, too late, too bureaucratic</h2>
<p>Fast forward to 2025, and we’re treated to Matt Clifford’s vision of how the UK government can shepherd us into an AI-powered future. The plan isn’t wrong, per se – it’s just adorably optimistic about the ability of government to direct technological evolution, or the beneficial effect of government getting involved in innovation. It’s like trying to herd cats, if the cats were quantum computers and the herders were Whitehall civil servants still struggling with Excel.</p>

<div class="no-row-height column-margin column-container"><div class="">
<hr>
<p><strong>Recipe</strong>: Coq au vin</p>
<ul>
<li>1 whole chicken, cut into pieces</li>
<li>750mL decent red wine</li>
<li>200g lardons</li>
<li>Pearl onions… lots of them. Two dozen or so.</li>
<li>500g mushrooms</li>
<li>2 carrots</li>
<li>Fresh thyme</li>
<li>Bay leaves</li>
</ul>
<p>Dunk the chicken in the wine and let rest overnight (patience is the name of the game). On the day, brown the lardons slowly in a Dutch oven. Remove, and reserve the fat. Brown the chicken pieces in the fat – avoid crowding your Dutch oven, work in batches if you have to. Sautee the vegetables, and return everything to the Dutch oven, including the chicken – and the wine. Cover and simmer on low-medium heat for 90 minutes. Thicken to taste with a beurre manié. Serve with a thick slab of a hearty, crusty bread.</p>
<hr>
</div></div><p>Let’s be real: The UK is already painfully late to this party. While we’re planning to plan about planning our AI infrastructure, OpenAI and Anthropic are training models that make GPT-4 look like a pocket calculator. The plan acknowledges we’re “the third largest AI market in the world” – which sounds impressive until you realize that in absolute terms, Britain’s contribution compared to the front runners (US and PRC) barely amounts to a rounding error.</p>
<p>The focus on government-directed innovation is particularly eyebrow-raising. The plan calls for establishing ‘AI Growth Zones’, expanding compute infrastructure and creating a new unit called UK Sovereign AI (because if you do not create more bureaucracy, you clearly aren’t doing this government thing well).<sup>3</sup> All potentially good stuff in theory, but premised on the notion that the government can effectively pick winners and shape the development of arguably the most dynamic technology since electricity. And, of course, the notion of ‘growth zones’ might strike one as rather odd considering that most of the work we do gets done in the cloud. I don’t need to be in a ‘growth zone’ to do my job. The extremely delocalised nature of software and AI development makes these echoes from the era of industrial parks and logistical efficiencies sound anachronistic at best, smacking of a misunderstanding of AI in practice at worst.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Interestingly, <a href="https://uksovereign.ai">uksovereign.ai</a> leads you to <a href="https://mindfoundry.ai">Mind Foundry</a>, a spin-out of my alma mater that seems to mainly cater for the defence industry.</p></div></div></section>
<section id="just-stay-out-of-the-way" class="level2">
<h2 class="anchored" data-anchor-id="just-stay-out-of-the-way">Just stay out of the way</h2>
<p>That said, there are some genuinely promising elements. The commitment to building AI infrastructure and expanding compute capacity is crucial - though one wonders if we might have thought about this back when compute was actually available to purchase. The focus on AI safety and ethics is also commendable, even if it feels a bit like installing guardrails after the car has already left the garage.</p>
<p>What’s missing is an acknowledgment that the most successful government interventions in technology have historically been indirect - creating the conditions for innovation rather than trying to micromanage it. DARPA gave us the internet by funding basic research and then getting out of the way. The UK’s own success stories - from ARM to DeepMind - emerged from an ecosystem that encouraged experimentation and risk-taking, not from government planning.</p>
<p>As someone who has spent time in both academia and industry, who has seen how innovation actually happens in the trenches of AI research, I can’t help but feel we’re missing the forest for the bureaucratically-approved, committee-selected trees. The UK has world-class AI talent, outstanding universities and a vibrant startup scene. What it needs isn’t more government steering – it needs the government to focus on removing obstacles and then getting out of the way. But hey, at least we’re not building a room full of swivel chairs. Though given some of the proposals in this plan, I wouldn’t rule it out for AI Action Plan 2.0.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Deja {Vu,} {All} {Over} {Again.}},
  date = {2025-01-16},
  url = {https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Deja Vu, All Over Again.”</span> <a href="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/">https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>governance</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</guid>
  <pubDate>Thu, 16 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>After agents</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/after-agents/</link>
  <description><![CDATA[ 




<p>When he first began his excavations at what is today Hisarlik in modern-day Türkiye, Heinrich Schliemann set out to find a single city – the city of Homer’s Iliad, a city many actually felt lay in the realms of fiction rather than any map he could lay his hands on. By the time excavations were over, Schliemann would find not one but nine cities, all built on top of each other.<sup>1</sup> In that, he found something relatively common – cities built on top of cities, each turning the last one into the foundation of the next.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Some of which he would blow up. Schliemann’s excavations were peak cowboy archaeology.</p></div></div><p>Technology is not much different. What Newton described as ‘standing on the shoulders of giants’ was an astute reflection of this fact. When I surveyed the landscape of LLMs in late 2023, I saw – and I was not alone in doing so! – that there was more to the practical application of LLMs than their most ubiquitous ‘low hanging fruit’ use case at the time – that is, chatbots and conversational interfaces. That notion, of course, became the agentic revolution that emerged as the most talked-about topic of 2024.</p>
<p>If your hot take for 2025 is that ‘agentic AI is going to dominate’, however, you have missed the train. The agentic revolution is over, done and accomplished. In my <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">wild-ass guessing of what 2025 may bring</a>, I tried to reflect as a leading theme on what I believe comes after agents – namely, systems of interaction. In this post, I hope to expand on that notion a little.</p>
<section id="agents-are-over-long-live-agents" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="agents-are-over-long-live-agents">Agents are over (long live agents!)</h2>
<p>Looking back at 2024, I have to laugh at how quickly the agentic revolution went from being seen as wild-eyed speculation to something almost embarrassingly obvious. I spent a good chunk of late 2023 explaining to sceptical audiences why autonomous AI agents were not just chatbots that could call APIs. I would spend most of the next year fielding calls from the same audiences to help develop a strategy for agentic AI. The whiplash-inducing speed of this transformation was stunning even by AI’s standards, where we seem to have moved into a 24-minute, rather than 24-hour, news cycle. What started as hacky demos and GitHub repos with more stars than working features evolved into agents casually writing production code, running research pipelines and tying all of this together into operational workflows that actually made sense. The real shift was not just in what these agents could do – it was in how they subtly changed our relationship with AI systems in the process. We went from the digital equivalent of playing 20 Questions with chatbots to having persistent virtual assistants that could actually maintain context, manage complex tasks and make reasonable decisions without needing to be guided through every minor choice. For someone who spent years working with the digital equivalent of goldfish, this was heady stuff.</p>
<p>The thing about agents is that their true power is not in what they can do alone – it is in how they work together. This is not just some hand-wavy My Little Pony-esque ‘collaboration is magic’ adage. It is fundamental to the nature of what an agent is, and why we even bother with them. An agent that can write code is useful, but an agent that can write code while collaborating with another that handles testing while yet another manages deployment and a fourth monitors performance is truly transformative. The industry’s current obsession with making individual agents (or, even worse, just individual foundation models) more powerful is like trying to make a better dish by adding more of a particular ingredient.<sup>2</sup> The magic of a perfect meal lies not in how many spoonfuls of exotic ingredients like fennel pollen and saffron threads you can dump into your pan, but in the fine balance between whatever ends up on a plate – even if it is just arugula, shaved parmesan and balsamic glaze (see recipe in sidebar). In short, the magic is in the complex web of interactions between the ingredients, the emergent phenomena that arise from their coexistence. We see this pattern play out all the time in technology. The apex of maturity always involved interaction. Consider, most ubiquitously, the web: from individual sites, we evolved to a more semantic web and eventually, a knowledge/information ecosystem driven through APIs. We are at the cusp of that third phase with AI agents, and anyone still fixated solely on individual agent capabilities is missing the plot entirely.<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;More AI folks should spend more time in the kitchen. This is <em>my</em> hot take for 2025.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;This is even more so as agents themselves are becoming meaningless. An agent, in a properly designed ecosystem, is entirely replaceable. It implements a contract or protocol. It then does not quite matter what that agent does. And if that sounds a little like the Liskov substitutability principle, then that is not quite by accident.</p></div><div class="">
<hr>
<p><strong>Recipe</strong>: The simplest salad you’ll ever love</p>
<ul>
<li>A bunch of baby arugula</li>
<li>1/4 cup of your favourite nuts</li>
<li>freshly ground salt and pepper</li>
<li>Parmigiano Reggiano, lots</li>
<li>1 clove garlic</li>
<li>1/4 cup lemon juice, freshly squeezed</li>
<li>1/2tsp honey</li>
<li>1/4 cup extra virgin olive oil</li>
<li>1tsp Dijon, the smoother the better</li>
<li>1/2tsp fresh thyme</li>
<li>optional: a good balsamic reduction</li>
</ul>
<p>Mix the lemon juice, minced garlic, Dijon and honey, and salt &amp; pepper to your heart’s content. While whisking, drizzle in the olive oil. Add the thyme and taste – add some more olive oil if it’s too acidic for your taste. In a separate bowl, add the arugula, and toss it with the vinaigrette you just made. Mix, using your hands – plastic or wooden mixing tools break the arugula leaves, which renders the whole thing bitter. Plate, then cover with the nuts. Using a coarse Microplane grater, shave enough Parmigiano Reggiano to make the whole thing a happy mixture. Enjoy on its own or as a light side.</p>
<hr>
</div></div>
<p>We are already seeing the first tentative steps from multi-agent systems towards ecosystems of agents, even if most have not recognised them as such. Enterprise agent marketplaces will be 2025’s hot commodities, drawing on past experiences with data marketplaces and exchanges. But conceptually, most seem to still treat agents as distinct pieces of software rather than collaborators in an ecosystem. The real pioneering work for the coming year(s) will be in developing the frameworks and protocols at the edges: agent orchestration systems that go beyond simple API calling, trust negotiation protocols that let agents establish their capabilities and limitations and collaborative frameworks that enable genuine multi-agent workflows. Technical implementations of these, however, are scarce. Working out an interaction protocol is not glamorous (trust me – speaking from personal experience), and there are going to be few headlines and fewer medals in working out how this sudden flood of AI agents is going to interoperate. And yet, this is the manifest destiny of agentic AI. Of all targeted agentic AI spend in 2025, marketplaces and interoperability orchestrators will without a doubt be the best dollars spent bar none.</p>
</section>
<section id="from-agents-to-ecosystems" class="level2">
<h2 class="anchored" data-anchor-id="from-agents-to-ecosystems">From agents to ecosystems</h2>
<p>What is an ecosystem? In its natural sense, we have relatively little trouble distinguishing between a species, an individual, a population and an ecosystem. In short, an ecosystem has three distinguishing features:</p>
<ol type="1">
<li>Diversity: ecosystems consist of multiple species that all play their role, quite similarly to agents in a well-architected system.</li>
<li>Interactions/rhizomality: ecosystems become what they are from the interactions between their participants, not the mere assemblage of the participants. In that sense, a good meal is an ‘ecosystem’ of sorts, where the acids balance out the fats and so on. In a more functional context, however, as we are dealing with here, what makes a bunch of agents an ecosystem is their ability to exercise a higher function in complementarity.</li>
<li>Interdependence: ecosystems produce their benefits through these interactions, which collectively amount to more than the sum of the parts.</li>
</ol>
<p>In an agentic AI system, our definition can largely be similar: an agentic AI ecosystem is a bunch of agents with different functionalities that interact and thereby unfold value. An ecosystem implies more than just the ability to pass messages between agents or chain them together in sequence. It requires the emergence of specialisation, of niches, of ways to establish trust and capabilities, and – crucially – ways to negotiate the terms of interaction. Unlike today’s relatively deterministic structures, this environmental discovery could eventually be self-governing, more like a bustling market bazaar, where agents can discover each other’s capabilities, negotiate terms of engagement, establish trust relationships, and even form longer-term collaborative partnerships. This is a fundamental reimagining of how artificial intelligence systems interact with each other.</p>
<p>The emergence of enterprise agent marketplaces will be one of the defining developments of 2025, but most organisations are still thinking about them wrong. The knee-jerk reaction is to build something akin to an app store: a catalogue of pre-built agents with rating systems and standard pricing. That is a useful (and often indispensable) first step, but it could be so much more. The real value of these marketplaces will not be in the agents themselves, but in the curation and verification mechanisms they enable. Think less ‘app store’ and more ‘commodity futures exchange’: what matters is not just what is being traded, but the rules of engagement, the verification of capabilities, the establishment of trust, and the standardisation of interfaces. We will need ways to verify that agents can actually do what they claim, that they operate within defined constraints and that they can be trusted with sensitive data or critical operations. This is where enterprise agent marketplaces will differentiate themselves from consumer platforms – through robust governance frameworks that make agent deployment actually feasible in regulated environments.</p>
<p>The real challenge – and opportunity – in building these ecosystems lies in standardising the right things while leaving room for innovation. Over-standardisation kills ecosystems as surely as no standardisation at all. The emerging protocols for agent interaction will need to thread this needle. They will have to standardise the essential patterns of trust establishment, capability discovery and resource negotiation, while remaining flexible enough to accommodate new forms of agent collaboration we have not even imagined yet. This is where the enterprise agent marketplace builders of 2025 will either make their fortunes or waste their investors’ money. The winners will be those who create the right balance of structure and flexibility – the ones who understand that they are not building an app store so much as cultivating an ecosystem.</p>
</section>
<section id="trust-falls-and-rises" class="level2">
<h2 class="anchored" data-anchor-id="trust-falls-and-rises">Trust falls (and rises)</h2>
<p>The hardest part of building these agentic ecosystems is not the technical implementation (in fact, we arguably have most of that already in place, mutatis mutandis, for data and other asset marketplaces) – but the trust architecture. In particular where agents choose and commission other agents to perform tasks as delegates or helpers, we need to find protocols that outline the powers of delegation and the flows of authority in such an architecture. The frameworks we are building now are laughably primitive compared to what we will need, mostly amounting to simple API keys and rate limits. It is like trying to build a modern financial system with nothing but paper IOUs and handshake agreements.</p>
<p>The chain of trust problem in agent delegation is fascinating precisely because it mirrors and yet fundamentally differs from how we handle human organisational hierarchies. When Agent A delegates a task to Agent B, which in turn needs to commission Agent C, we are not just passing around access tokens - we are creating a chain of responsibility that needs to be both traceable and constrained. Each link in this chain needs to carry not just the authority to act, but also the constraints and audit requirements of all previous links. An agent operating as a fourth-level delegate should still be bound by the original constraints set at the root of the delegation tree, even if it has no direct knowledge of them. This is not just about security - it is about maintaining coherent behaviour across increasingly complex chains of interaction. The financial sector learned this lesson the hard way with automated trading systems: without clear chains of responsibility and well-defined constraint propagation, you end up with cascading failures that no single participant can explain or control.</p>
<p>The next year will be defined not by breakthroughs in individual agent capabilities, but by our success or failure in building these frameworks for trusted collaboration. The winners will not be those who build the most powerful agents, but those who crack the code of helping agents work together effectively and safely. This is not just about technology – it is about understanding how to create systems of trust that can scale with the complexity of agent interactions.</p>
<p>The agentic revolution of 2024 was just the overture. The real symphony begins when we figure out how to let the players actually work together in concert. Those who are still focused solely on building better individual agents are composing for soloists in an age that demands orchestras.</p>
<p>The future belongs to those who can conduct.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {After Agents},
  date = {2025-01-04},
  url = {https://chrisvoncsefalvay.com/posts/after-agents/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“After Agents.”</span> <a href="https://chrisvoncsefalvay.com/posts/after-agents/">https://chrisvoncsefalvay.com/posts/after-agents/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/after-agents/</guid>
  <pubDate>Sat, 04 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/after-agents/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Five unconventional predictions</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</link>
  <description><![CDATA[ 




<p>As I sit here at year’s end, I’m reminded of the ancient Swedish tradition of årsgång - the ritual winter walk taken on New Year’s Eve to divine the fortunes of the coming year. The practice required one to walk alone in complete silence, visiting places of significance while remaining carefully alert to any signs or omens.<sup>1</sup> While I may be rather ill-equipped for mystical midnight wanderings, I’ve spent enough time observing the enterprise AI space to develop my own form of augury.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I gather from time to time, certain mind-altering substances were involved.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Note that I did not come up with the idea of agentic AI – in fact, a good part of what I believe is that there’s much less novelty to agentic AI than some would like to pretend. Rather, my point was that this more mature framework of agents interacting with agents will dominate over simple human-machine interactions.</p></div></div><p><a href="http://localhost:5836/posts/team-of-rivals/">In 2023, I predicted the rise of agentic systems</a>, when chatbots were still seen as the dominant form of LLM usage.<sup>2</sup> What at the time felt like a wild-ass guess is now almost received wisdom. And thus, I will throw my customary conservatism and restraint to the wind and in the spirit of årsgång, let me share five predictions for 2025.</p>
<section id="ai-governance-gets-productised" class="level2">
<h2 class="anchored" data-anchor-id="ai-governance-gets-productised">AI governance gets productised</h2>
<p>A few months ago, I shared a drink with an acquaintance whose work is in the Responsible AI field. It was pretty obvious he needed that drink a whole lot more than I needed my Diet Coke.</p>
<p>“Nobody wants or needs us,” he bemoaned.</p>
<p>“What are you talking about? Everybody is talking about Responsible AI and AI governance.” I was puzzled.</p>
<p>“No, I meant <em>us</em>,” he pointed at himself. “They want the ideas, the manuals, the guidelines… just not the people. They want governance as a service.”</p>
<p>Thinking back to several conversations I had this past year, I could see his point. There is plenty of interest in AI governance and Responsible AI, especially in the regulated sectors, where I spend most of my working (waking?) hours. This is unsurprising – even more in those sectors than any others, the success of GenAI initiatives hinges on regulatory tolerability to a fairly significant extent. But I hear much more enthusiasm for neatly packaged, productised, almost SaaS-like governance products than for Responsible AI as a function.</p>
<p>And so, while everyone’s been politely nodding along to principles and frameworks for most of 2024, the conversations all seemed to end with the same question – “great, can I have this on a SaaS model?”. What’s emerging is a clear pattern: organizations want Responsible AI practices, but they want them delivered as a service. This is at least partly due to buyers’ perceptions that much of those practices sound lofty and abstract, when their main concern is keeping the board and the regulators happy. You can’t feed a starving belly with high-minded principles.</p>
<p>We’re already seeing this in cloud providers are starting to package governance features, the rise of tools to facilitate this, and most tellingly, in how procurement departments are writing RFPs that specifically ask for such services in product-ish or service-ish terms – documents, procedures, TTPs and as-a-services. This shift signals the operationalisation of AI governance as it matures from theoretical frameworks to practical, subscription-based implementations. Because let’s face it: nothing says “we take ethics seriously” quite like a monthly fee.</p>
<p>My bet is that by the time the new regulatory flora and fauna of AI that is slowly emerging comes to bear its first fruits (say, late 2025-early 2027), we’ll have entire platforms dedicated to automated governance, continuous monitoring and “RAI middleware” that sits between models and applications. The real winners will be those who can package the complex requirements of Responsible AI into digestible, subscription-based services that make compliance and governance feel as natural as running a CI/CD pipeline. And, of course, we’ll get a new buzzword out of it. I’ll go and grab the domains for “AIGovOps” after this, but I’m sure someone will come up with something even more cringe-worthy.</p>
</section>
<section id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models">Small is beautiful (at the very least when it comes to language models)</h2>
<p>In a <a href="https://arxiv.org/abs/2402.10951">paper I published earlier in 2024</a>, I hijacked my audience for a few paragraphs’ worth of musings about the ethical, environmental, pragmatic and financial cases for small language models. At this time, one must recall, fine-tuning GPT models became feasible and, as it happens with AI hype, turned instantly into a status symbol. You know an industry has jumped the shark when you buy a night light, and it comes with a companion app with its own fine-tuned GPT. Just no.</p>
<p>Fortunately, the arc of AI development has largely bent the other way – towards small language models (which I believe is the correct direction). Turns out not every task needs a model that’s read all of Wikipedia and can write Shakespearean sonnets about your cat. Instead, we’re seeing enterprises discover that they can develop small domain-specific models for even very specific sets of terminologies, with better results than the large generalist models, which can then be relegated to act as dispatchers over these specialists. LLMs will be the general practitioners, while SLMs will increasingly take the specialist’s role.</p>
<p>It’s perhaps worth noting at this juncture that in AI, quantity has a quality all of its own. I’m somewhat reminded of a realisation that came to me this year when putting together a training plan. I hold multiple adaptive world records in a fairly esoteric sport called the SkiErg,<sup>3</sup> at distances ranging from 100m (the shortest distance eligible for a record) to marathons and half-marathons (the longest record-eligible distances). My heart is mostly with longer distances, so I had to put quite a bit of thought into figuring out how to train for shorter, explosive sprints. A 100m sprint is not just a ‘shorter 2k’. You need to approach it as a distance of its own, with its own challenges and merits. I was amused to see the same in SLMs – these aren’t pared-down LLMs. Successful SLMs are created as SLMs to begin with, not as reduced afterthoughts of larger models.<sup>4</sup> And so, just like I had to create a completely new training plan for sprint distances, SLM developers have to keep in mind that they’re not building LLMs writ small, but a different type of model with different desiderata.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Think of it as an indoor rower rotated by 90 degrees around its coronal plane, replicating not the ‘draw’ of rowing but the ‘pull’ of… something, I guess, having to do with skiing? <a href="https://www.concept2.com/ergs/skierg">Here’s a video.</a></p></div><div id="fn4"><p><sup>4</sup>&nbsp;That approach and attitude may be appropriate for quantisation, however, which is an entirely different story.</p></div></div><p>I expect we’ll see a proliferation of specialised SLMs in 2025, each trained on narrow domains but designed to work in concert. The art will be in the orchestration – how to route queries to the right specialist model, how to combine their outputs and how to maintain this constellation of smaller models efficiently. This parallels the evolution of microservices in software architecture, and we’re about to rediscover all the same lessons about service discovery, orchestration, and system design, but this time with AI models. The emergence of “ModelOps” is inevitable, and the best we can hope for is that we haven’t forgotten too much from last time.</p>
</section>
<section id="plot-twist-the-new-ai-kingmakers" class="level2">
<h2 class="anchored" data-anchor-id="plot-twist-the-new-ai-kingmakers">Plot twist: the new AI kingmakers</h2>
<p>Here’s a plot twist for 2025: your next AI project will likely live or die based on a decision made by some actuary who’s never written a line of code in their life. The insurance industry – that most conservative of institutions – may well become the de facto regulator of enterprise AI deployment. We’re already seeing the early signs in how cyber insurance policies are evolving to cover AI incidents, and how underwriters are starting to ask increasingly sophisticated questions about model governance and deployment practices. <a href="https://www.munichre.com/en/solutions/for-industry-clients/insure-ai/ai-self.html">Munich Re now offers AI insurance not only for commercial providers but also, essentially, in-house AI work</a>, and apparently, <a href="https://www.computerweekly.com/news/366586014/Munich-Re-sees-strong-growth-in-AI-insurance">business is booming</a>. Meanwhile, other insurers remain rather less sanguine. <a href="https://assets.lloyds.com/media/439566f8-e042-4f98-83e5-b430d358f297/Lloyds_Futureset_GenAI_Transforming_the_cyber_landscape.pdf">Lloyd’s commissioned a report last March</a> that is noticeably less upbeat. At least they didn’t compare it to asbestos.</p>
<p>But AI insurance is coming, and it may well become the kingmaker of AI solutions. This will particularly affect the startup ecosystem, who often do not have the funds to pursue certification before pitching to clients. Alas, the cost of enterprise executives’ comfort that is bought by the availability of an insurance and indemnity policy is that market access will be rather less feasible for those who cannot obtain such insurance – who, paradoxically enough, are the ones who would need it most.</p>
<p>And since that leaves insurers with a ‘shadow governance’ function, I can well imagine the insurance industry creating <em>de facto</em> standards for AI governance faster than any standards body or government regulator could dream of. And let’s be honest – the actuaries will probably do a better job than most regulators could anyway.</p>
</section>
<section id="cross-estate-ai-xeai-is-the-future" class="level2">
<h2 class="anchored" data-anchor-id="cross-estate-ai-xeai-is-the-future">Cross Estate AI (XEAI) is the future</h2>
<p>Agentic AI was an interesting development, but to me, that’s primarily about the <em>how</em>. The really interesting story is of the <em>what</em> – that is, of what we can do with agents that we couldn’t do without them. And by far one of the most interesting such applications is what I call Cross Estate AI (XEAI). In XEAI, information crosses boundaries between enterprises as AI agents from different places are composed together to build an agentic structure. Think of API calling, but for AI agents – and with way more sophistication. A company’s agentic model may reach out to multiple other companies that offer their own agents, and collaborate with them. We are now relatively comfortable with notions like our AI agents calling an external API – after all, REST API calling has been around for a long, long time. But my company’s AI agent ‘calling up’ a specialised AI agent from another company to, say, engage in a discussion to refine the wording in a regulatory submission is something that enterprise stakeholders might need some time to get their head around.</p>
<p>The technical challenges here are fascinating – we need dynamic trust scoring, cryptographic proofs of model lineage and secure compute enclaves. But the real challenge is the social architecture: a very complex dance of trust, verification and governance that will have to be mediated. We’re essentially speed-running the development of diplomatic protocols that took human societies centuries to develop. And just like real diplomacy, it’s all about managing relationships between different systems with different organizational cultures and governance models.</p>
<p>GenAI does not yet have a <em>lingua franca</em> akin to REST to allow systems to talk to each other, least of all one that allows not just a query-response format to be conveyed but also various conventions on trust and governance to be exchanged. The real potential of agentic AI lies in how various agents can interact and together create emergent structures that are ultimately more than their parts. Function calling and an AI agent being able to look up something on Google are neat, but not really anything spectacular that a simple RPA actor or a 10-line Python script starting with <code>import requests</code> couldn’t do just as well. To paraphrase Kipling: the strength of the agent is the ecosystem. The big challenge, then, is to figure out how we can allow such ecosystems to exist across the boundaries of corporate networks.</p>
</section>
<section id="model-metabolomics-takes-over" class="level2">
<h2 class="anchored" data-anchor-id="model-metabolomics-takes-over">Model metabolomics takes over</h2>
<p>Energy efficiency is about to become the dominant factor in AI deployment economics. We’re not just talking about cost savings – we’re talking about a fundamental shift in how we think about AI system design. Just as biological systems optimize for metabolic efficiency rather than raw performance, we’re about to see AI architecture undergo a similar evolution.</p>
<p>This shift will upend the current obsession with inference speed and model size. Organizations will start optimising models for what I call “computational metabolomics” – the total energy cost of getting useful work done. We’ll see the emergence of new architectures that might be slower in raw terms but dramatically more efficient in their use of resources.</p>
<p>The really controversial part? Many organizations will opt for slower but more energy-efficient approaches, fundamentally challenging the industry’s obsession with real-time everything. I expect to see in 2025 more and more architectures that prioritize efficiency over speed, and a new set of metrics focused on energy consumption per useful output. The hype cycles of 2023-24 were all about who could build the biggest model - 2025 will be about who can build the most efficient one. Especially with the rise of SLMs, we’re slowly approaching the point where increasing the parameter size of generalist LLMs is going to yield any useful business benefits, or get developers any more free drinks. Such diminishing returns are, of course, part and parcel of every evolving system, and while it will be conceptualised as the beginning of a new AI winter by the usual suspects (it is not!), this is a good thing. For as we now have slowly reached a model size we’re comfortable with, we can start on making it more metabolically efficient. This is, of course, not limited to language models – in fact, this development has been going on in the computer vision world, where a good deal of processing occurs on edge devices, for a long time. Much of the lacking enthusiasm for wholesale replacement of computer vision models with vision-language models like LLaVA, CogVLM or DeepSeek boils down to the simple economics of the matter: the vast majority of computer vision challenges, in practice, can be solved quite well with a 30-year-old algorithm that can be implemented in fifty lines of C. The same, incidentally, is true for LLMs. I wince when I see GPT-based approaches to problems that a bag-of-words classifier can solve with comparable accuracy for essentially no cost and in a fraction of time. And once the power of cool wears off, I expect a renaissance of many of those solutions.</p>
<hr>
<p>As I complete this digital årsgång, what strikes me most is how 2025 looks to be the year when enterprise AI grows up. The trends all point toward practicality over pizzazz: smaller models over larger ones, efficiency over raw power, productized governance over philosophical frameworks, and insurance actuaries over innovation evangelists. Perhaps that’s not as exciting as the breathless predictions of AI singularities and digital transformations that dominated 2023, and maybe closer to the relatively sober atmosphere of 2024. But then again, maybe that’s exactly the point – real progress tends to be more about making things work than making headlines. And personally, I find that far more interesting.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {Five Unconventional Predictions},
  date = {2024-12-31},
  url = {https://chrisvoncsefalvay.com/posts/five-wild-guesses/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“Five Unconventional
Predictions.”</span> <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">https://chrisvoncsefalvay.com/posts/five-wild-guesses/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>tech predictions</category>
  <guid>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</guid>
  <pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/five-wild-guesses/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>LAIR - Language As Intermediate Representation</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/lair/</link>
  <description><![CDATA[ 




<p>The awesome thing about language is that, well, we all mostly speak it, to some extent or another. This gives us an immensely powerful tool to manipulate transformational tasks. For the purposes of this post, I consider a transformational task to be essentially anything that takes an input and is largel intended to return some version of the same thing. This is not a very precise definition, but it will have to do for now.</p>
<p>Such models are nothing new. Perhaps the most eye-catching and ubiquitous of such models are <a href="https://www.tensorflow.org/tutorials/generative/style_transfer">neural style transfer</a> models that take an image and return a version of the same image in a different style that let you turn a picture of your dog into a Van Gogh painting <span class="citation" data-cites="gatys2015neural">(Gatys, Ecker, and Bethge 2015)</span>. Quite simply put, these models are your typical generative model, with the difference that it takes two separate loss definitions: content loss, which is loss of the generated image vis-a-vis the content reference, and style loss, which is the loss vis-a-vis the style reference image. A “good” image then is one that minimises total loss, i.e.&nbsp;it’s just as close to your dog as it is to Van Gogh. Figure&nbsp;1 outlines this logic.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gatys2015neural" class="csl-entry">
Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. <span>‘A Neural Algorithm of Artistic Style’</span>. <em>arXiv Preprint arXiv:1508.06576</em>.
</div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nst" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-nst">flowchart TD
    C["Content image"]
    S["Style image"]

    G["Generated image"]

    G --&gt; L["Loss network"]
    L --&gt; G
    
    C -- "Content loss" --&gt; L
    S -- "Style loss" --&gt; L
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A rough outline of NST.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Given a content source image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bc%7D"> and a style reference image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D">, we define the total loss of our generated image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bg%7D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Btotal%7D%20=%20%5Calpha%20%5Cmathcal%7BL%7D_%7Bcontent%7D%20+%20%5Cbeta%20%5Cmathcal%7BL%7D_%7Bstyle%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are hyperparameters that control the relative importance of the content and style losses. The content loss for layer <img src="https://latex.codecogs.com/png.latex?l"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bcontent%7D(%5Cvec%7Bc%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi,j%7D%20(%5Cvec%7Bc%7D_%7Bij%7D%5El%20-%20%5Cvec%7Bg%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>which is basically a simple squared error loss between the feature vector of the content image and the generated image at layer <img src="https://latex.codecogs.com/png.latex?l">. The style loss is a bit more complicated, and is these days typically defined as the Maximum Mean Discrepancy, which <span class="citation" data-cites="li2017demystifying">Li et al. (2017)</span> have shown is essentially equivalent to the Gram matrix loss, defined as</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017demystifying" class="csl-entry">
Li, Yanghao, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. 2017. <span>‘Demystifying Neural Style Transfer’</span>. <a href="https://arxiv.org/abs/1701.01036">https://arxiv.org/abs/1701.01036</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bstyle%7D(%5Cvec%7Bs%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B4N_l%5E2M_l%5E2%7D%20%5Csum_%7Bi,j%7D%20(%5Cmathbf%7BG%7D_%7Bij%7D%5El%20-%20%5Cmathbf%7BS%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BG%7D_%7Bij%7D%5El"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BS%7D_%7Bij%7D%5El"> are the Gram matrices of the generated reference image and the style image at layer <img src="https://latex.codecogs.com/png.latex?l">, respectively. <img src="https://latex.codecogs.com/png.latex?N%5El"> is the number of feature maps in layer <img src="https://latex.codecogs.com/png.latex?l"> and <img src="https://latex.codecogs.com/png.latex?M_l"> is the dimensionality (height times width) of the feature map of layer <img src="https://latex.codecogs.com/png.latex?l">. There are two fundamental problems with this.</p>
<ol type="1">
<li>This works much less well for things that aren’t images or at least sufficiently similar to images.</li>
<li>The bigger problem is that the style reference is pretty much exhaustive. By that, I mean that there isn’t much we can convey to the model about the style that isn’t encapsulated in the style reference images. Depending on how semantically apt your model is, it may or may not be able to pick up some higher level ideas. It may be able to pick up the brush strokes of Van Gogh or the colours of a Turner, but it may not be able to paint your characters in the semantic context of Van Gogh’s time and place.</li>
</ol>
<p>This is where language comes in. Language is a very powerful tool for conveying information, and it turns out that if we use language as an intermediate representation, we can use language models to manipulate this intermediate representation to our heart’s content, using a relatively informal and rather forgiving language. This is the idea behind LAIR.</p>
<section id="a-toy-example" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-toy-example">A toy example</h2>
<p>I’m a firm believer in silly toy examples. The sillier, the better. So, we’ll start with the following proposition: can we create a model that will look at a photo from, say, the front page of our favourite newspaper, and transpose it into the Warhammer 40k universe? In case you’re unfamiliar, Warhammer 40k is set – as the name suggests – in the 40th millennium, but is a weird mixture of medieval and futuristic technology. The whole atmosphere is taking the ‘Dark Ages’ part of the Middle Ages,<sup>1</sup> adding spaceships and laser guns, and turning the whole thing into an absolutely depressing dystopia. It’s a lot of fun.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Which may or may not ever have actually existed.</p></div></div><p>There are a few things we want here: I don’t merely want the <em>visual</em> style of the Warhammer 40k universe, I also want the <em>semantics</em> – that is, I want characters to be transposed into the Warhammer 40k universe. I want the model to understand that the people in the photo are now Space Marines, and that the buildings are now Gothic cathedrals. I can’t get neural transfer to that for me, because it does not understand, or care, about semantics, and does not do semantic transformation. More importantly, I cannot interact with the ‘guts’ of neural style transfer beyond setting the hyperparameters and the source images.</p>
<p>What I can, however, do is to use the language I am mostly most proficient in – that is, human language – to manipulate an intermediate representation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lair" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-lair">flowchart LR
    subgraph Description
        direction TB
        S["Source image"] --&gt; D("Descriptor\ne.g. GPT-4 vision") --&gt; d["Description"]
    end

    subgraph Transformation
        direction TB
        t("Transformer\ne.g. GPT-4") --&gt; td["Transformed\ndescription"]
    end

    subgraph Rendering
        direction TB
        r("Renderer\ne.g. DALL-E") --&gt; I["Output\nimage"]
    end

    Description --&gt; Transformation --&gt; Rendering
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A rough outline of LAIR as applied to the toy example.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure&nbsp;2 outlines the basic idea for images. We use a descriptor model to describe the image, and then use a text-to-text transformer to transform the description. The transformed description is then rendered into an image. The descriptor model is typically a vision model, and the transformer is typically a language model. The renderer would then be a generative model.</p>
<p>Consider the following source image I downloaded from Pexels:</p>
<div id="fig-source" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/pexels-canva-studio-3153198.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Our source image.
</figcaption>
</figure>
</div>
<p>Here’s the intermediate representation (IR) description created by GPT-4 vision:</p>
<blockquote class="blockquote">
<p>This is an image of four individuals gathered around a laptop, suggesting a collaborative work environment. The setting has an ample amount of natural light, evident by the bright illumination on the subjects and the interior. They appear to be located in a modern indoor environment with a large curved window in the background, contributing to the light-flooded ambiance.</p>
<p>Starting from the left, the first individual is a man with short, dark hair and a light beard, wearing a black t-shirt with a white graphic and text. He appears focused, resting his hands on the laptop while looking at the screen. To his right is a woman with shoulder-length curly hair who seems engaged and smiling, leaning slightly towards the laptop. She’s wearing a light grey short-sleeved top.</p>
<p>Behind these two and slightly out of focus, two more individuals are present. The person in the middle of the composition is a man with shoulder-length wavy hair, wearing a black shirt, gazing at the laptop screen. To the far right, partially cropped out of the frame, there is a woman with glasses, wearing a dark business casual blazer. She seems to be observing the laptop’s screen with interest, standing slightly behind the seated individuals.</p>
<p>The group exhibits a sense of teamwork and focus, with everyone’s attention directed toward the laptop, situated on what appears to be a white table or desk. The body language and expressions suggest a positive, collaborative atmosphere, typical of a professional or academic team setting.</p>
</blockquote>
<p>This is a pretty decent description. Let’s grimdark it up by passing it to our transformer model, which was initialised with the following prompt:</p>
<blockquote class="blockquote">
<p>Your job is to analyze descriptions of images and change them to transpose the image into the dark, futuristic aesthetic of the Warhammer 40k universe. You are given a description of a scene, and your job is to change that scene according to the instructions, and give a detailed description of the changed scene that would allow a generative model to create an image.</p>
<ol type="1">
<li>Replace clothing items with a mixture of futuristic and mediaeval looking armour.</li>
<li>Replace every-day objects held by characters with power swords, chainswords,futuristic looking guns, staffs or magic items.</li>
<li>Replace architecture with the monumental, dark architecture common to the Warhammer 40k universe.</li>
<li>Include things like monumental gothic spaceships in the image description.</li>
</ol>
</blockquote>
<p>Here’s the output:</p>
<blockquote class="blockquote">
<p>This is an image of four battle-hardened individuals gathered around a hololith, suggesting a strategic war council. The setting is bathed in the cold, harsh light of flickering lumens, casting long, grim shadows on the figures and the interior. They appear to be situated in a cathedral-like command room of an Imperial Battleship, with a monumental, gothic archway in the background, contributing to the oppressive ambiance.</p>
<p>Starting from the left, the first individual is an Adeptus Astartes, his short, dark hair and light beard visible beneath the rim of his power armor helmet. He is clad in a black power armor with white Imperial insignia. He appears focused, his gauntleted hands resting on the hololith while studying the battlefield projection. To his right, a Sororitas, her shoulder-length, curly hair peeking out from beneath her wimple, leans towards the holographic display. She’s clad in a light grey power armor, her face engaging and determined.</p>
<p>Behind these two and slightly out of focus, two more figures loom. The figure in the middle of the composition is a man, his shoulder-length wavy hair barely visible beneath his Inquisitorial hat, wearing a black, ornate, medieval-looking armor, his eyes fixed on the hololith. To the far right, partially cropped out of the frame, there is a Tech-Priestess, her bionic glasses glinting in the lumen-light, her dark, mechanicus robes adding to her awe-inspiring presence. She seems to be observing the hololith with keen interest, standing slightly behind the armored individuals.</p>
<p>The group gives off a sense of unity and focus, with everyone’s attention directed toward the hololith, situated on what appears to be an aged, white marble table. The body language and expressions suggest a tense, collaborative atmosphere, typical of a war council in the grim darkness of the far future.</p>
</blockquote>
<p>By the Emperor, that’s some good stuff. Let’s see what DALL-E makes of it:</p>
<div id="fig-output" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/img-eTqdxb5CYGT2vY5FqNkRJdva.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Our output image.
</figcaption>
</figure>
</div>
<p>It’s interesting to note what remains and what has changed. In particular, the relationship of the figures to each other, both spatially and semantically, as well as their posture, their number (mostly!) and the general subject matter at a highly semanticised level was preserved. Instead of a laptop, they’re looking at a big holographic sand table. Note that unlike in NST, we actually have control over what is preserved and what is not – that’s what our transformer prompt accomplishes. We’ve told it to change dress, for instance, but we haven’t told it to make any changes to the overall relationship between the figures (note how a “collaborative atmosphere”, for instance, was retained word for word).</p>
</section>
<section id="whats-the-point" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-the-point">What’s the point?</h2>
<p>Okay, this was quite fun, but what’s the point? Well, the point is that we can use language as an intermediate representation to manipulate things relatively easily. This is powerful because language is such an accessible and forgiving intermediate representation. More importantly, however, <em>anything</em> that can be represented in language can be manipulated this way.</p>
<ul>
<li>In the space of language-to-language, this includes adapting text to the needs of special audiences,<span class="citation" data-cites="steinmetz2023developing verma2023bridging">(Steinmetz 2023; Verma, Boland, and Miesenberger 2023)</span> conveying information to lay audiences <span class="citation" data-cites="lee2023ai">(Lee, Goldberg, and Kohane 2023)</span> and creating secondary explanatory materials e.g.&nbsp;statutory explanations <span class="citation" data-cites="blair2023can">(Blair-Stanek, Holzenberger, and Van Durme 2023)</span>.</li>
<li>For image-to-image transformations, retaining semanticity while simpifying visuals and removing clutter may often be useful, e.g.&nbsp;for creating procedural visual guidance for medical procedures <span class="citation" data-cites="chen2023endoscopic">(Chen 2023)</span>. Often, such images are created by hand, but this is a time-consuming process that could be automated.</li>
<li>For code-to-code, language as an intermediate representation allows the interjection of desired features into code, e.g.&nbsp;for the purposes of code refactoring. Beyond simple code rewriting, this allows a kind of opinionated transformation. Often, a target language is not only idiomatically different but also has certain other characteristics, and this is <em>a fortiori</em> the case for DSLs <span class="citation" data-cites="magalhaes2023c2taco">(Magalhães et al. 2023)</span>.</li>
<li>For code-to-text, this allows the creation of documentation from code, which is a perennial problem in software engineering. The textual intermediate representation allows fine control over the resulting documentation.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-steinmetz2023developing" class="csl-entry">
Steinmetz, Ina. 2023. <span>‘Developing <span>“EasyTalk”</span>–a Writing System Utilizing Natural Language Processing for Interactive Generation of <span>“Leichte Sprache”</span>(easy-to-Read German) to Assist Low-Literate Users with Intellectual or Developmental Disabilities and/or Complex Communication Needs in Writing’</span>. <a href="https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf">https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf</a>.
</div><div id="ref-verma2023bridging" class="csl-entry">
Verma, A Kumar, S Gavra Boland, and Klaus Miesenberger. 2023. <span>‘Bridging the Digital Divide for Persons with Intellectual Disabilities: Assessing the Role of ChatGPT in Enabling Access, Evaluation, Integration, Management, and Creation of Digital Content’</span>. In <em>ICERI2023 Proceedings</em>, 3767–76. IATED.
</div><div id="ref-lee2023ai" class="csl-entry">
Lee, Peter, Carey Goldberg, and Isaac Kohane. 2023. <em>The AI Revolution in Medicine: GPT-4 and Beyond</em>. Pearson.
</div><div id="ref-blair2023can" class="csl-entry">
Blair-Stanek, Andrew, Nils Holzenberger, and Benjamin Van Durme. 2023. <span>‘Can GPT-3 Perform Statutory Reasoning?’</span> <em>arXiv Preprint arXiv:2302.06100</em>.
</div><div id="ref-chen2023endoscopic" class="csl-entry">
Chen, Hao-Wen. 2023. <span>‘Endoscopic Endonasal Skull Base Surgery for Pituitary Lesions: An AI-Assisted Creative Workflow to Develop an Animated Educational Resource for Patients and Physicians’</span>. PhD thesis, Johns Hopkins University.
</div><div id="ref-magalhaes2023c2taco" class="csl-entry">
Magalhães, José Wesley de Souza, Jackson Woodruff, Elizabeth Polgreen, and Michael FP O’Boyle. 2023. <span>‘C2TACO: Lifting Tensor Code to TACO’</span>. In <em>Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences</em>, 42–56.
</div></div><p>The possibilities for using language as an intermediate representation are endless. LAIR is a powerful paradigm for transformational tasks that allows us to use language as an intermediate representation to manipulate things in a way that is both accessible and powerful, and that allows us to pick and choose <em>what</em> part of semanticity we want to manipulate <em>versus</em> what we want to preserve. It’s hard to reason about LAIR’s relative performance given that it is not a technique but a paradigm, and that its focus is not simple style transfer but finely controlled stylistic and contextual transformation, but even in the current absence of benchmarks, it is clear that models benefit from using language as an easily workable and malleable intermediate representation.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>The code for the toy example is available <a href="https://gist.github.com/chrisvoncsefalvay/f3c6e7547350cfae50be0078ad99435f">here</a>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {LAIR - {Language} {As} {Intermediate} {Representation}},
  date = {2024-01-06},
  url = {https://chrisvoncsefalvay.com/posts/lair/},
  doi = {10.59350/qg7b3-crs97},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“LAIR - Language As Intermediate
Representation.”</span> <a href="https://doi.org/10.59350/qg7b3-crs97">https://doi.org/10.59350/qg7b3-crs97</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>style transfer</category>
  <guid>https://chrisvoncsefalvay.com/posts/lair/</guid>
  <pubDate>Sat, 06 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/lair/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>The 95% myth</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/95-percent-myth/</link>
  <description><![CDATA[ 




<p>The year is 1959. Eisenhower is on his second term, Castro just kicked Batista out of the country and Ray Charles’s Let the Good Times Roll is topping the charts. And, in slightly nerdier pursuits, a psychiatrist named Albert “Mickey” Stunkard and a dietician colleague of his, Mavis McLaren-Hume, published a paper in the <em>AMA Archives of Internal Medicine</em> titled <em>The Results of Treatment for Obesity</em> <span class="citation" data-cites="smh">(Stunkard 1959)</span>. Written in a somewhat endearingly old school way by authors not quite initiated to the sacred mysteries of, y’know, <img src="https://latex.codecogs.com/png.latex?p">-values and proper statistical methodology, it reports on “the literature” on dietary and pharmacologically assisted treatments (which is an extremely generous term for 8 studies, none of which are randomised, one of which is entirely retrospective, and the total <img src="https://latex.codecogs.com/png.latex?N"> is 1,368), and on a sample of 100 patients (!) from the Nutrition Clinic at New York Hospital (the “NY sample”). And from that, they draw wide-ranging conclusions on the ‘harm’ and ‘ineffectiveness’ of ‘weight reduction’.</p>
<div class="no-row-height column-margin column-container"><div id="ref-smh" class="csl-entry">
Stunkard, A. J. 1959. <span>‘The Results of Treatment for Obesity’</span>. <em>A.M.A. Archives of Internal Medicine</em> 103: 79. <a href="https://doi.org/10.1001/archinte.1959.00270010085011">https://doi.org/10.1001/archinte.1959.00270010085011</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Fat acceptance activists love to point out the size of the US ‘diet industry’, which is somewhere around US$ 70bn. For comparison, the US QSR (quick service restaurants – read: fast food) industry had a 2022 market size of US$ 380bn.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;If you are wondering by the end of this whether Stunkard got his MD by sending in some coupons from cereal boxes, you’re not alone, but that would be quite unfair. From what I’ve read of the man, he was both a great human being and a very accomplished clinician. It’s a pity that his most influential piece was not only uncharacteristically bad work for him, but was also grievously misinterpreted.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Don’t worry, it gets worse. The <img src="https://latex.codecogs.com/png.latex?N"> for maintenance is 12. That’s not a typo.</p></div></div><p>In this post, I’ll dissect the Stunkard-McLaren-Hume (SMH) paper, and how it became an uncritically repeated figure and later the guiding mantra of ‘fat acceptance’, a movement I consider to be organised science denialism fuelled by self-delusion and corporate interests.<sup>1</sup> I try to do so with some generosity and understanding for the vagaries of the age, but one shouldn’t be under any illusions that the quality of the SMH paper would pass muster as an undergraduate research paper today. Standards have evolved, but the SMH paper is, well, not to mince words, bad even for the time.<sup>2</sup> It is conclusory to the extreme over a pathetically small sample, and I’m mostly convinced that the reason the paper is never cited is because anyone making major life decisions about their health and well-being based on a 65-year-old study of 100 people would look, well, quite silly.<sup>3</sup></p>
<section id="sm-damn-h" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sm-damn-h">SM-damn-H</h2>
<p>The SMH paper consists of two parts. One is a rudimentary ‘mini-meta’ of 8 studies from 1931 to 1958, of a little over 1,300 patients. The other is a retrospective study of 100 patients from the Nutrition Clinic at New York Hospital. Neither is great, but the mini-meta is so bad, I won’t belabour it in detail. It looks at very few patients overall on a range of ‘diets’, ranging from a 600kcal VLCD to diets assisted by dexedrine (classy!) and thyroid supplementation (unmonitored, of course), and reports the percentage of patients by lbs of weight lost. This is confusing to the extreme, because of course the absolute weight loss on a diet is highly contextual. A 200lbs person losing 50lbs is an entirely different story from a morbidly obese individual, say 350lbs, losing back to 300lbs. The mini-meta is, in short, a mess, and I’m not going to waste time on it.</p>
<p>The second part, the retrospective study, is also very bad, but at least informative. Here, 100 patients were interviewed at admission to the Nutrition Clinic, and prescribed a diet. What diet, you might ask? “Balanced weight-reduction diets from 800 to 1,500 Cal” (sic). That’s right, the diets were not standardised, nor do we see the estimated BMR<sup>4</sup> or the shortfall vis-a-vis the BMR reported anywhere. No mention of exercise or control for activity is in evidence anywhere. The clinic itself gives a bit of a flying by the seat of one’s pants impression, and the study doesn’t get better from here on.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Basal Metabolic Rate.</p></div></div><p>The retrospective study was then performed 2.5 years after initial admission, as a chart review. Of the 100 patients, only 12 were considered to have successfully lost weight, with 20lbs lost being the cut-off point (once again, entirely insensitive to starting body weight, thus likely privileging the more obese initial participants). A flowchart of this is laid out in Figure&nbsp;1.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-flowchart">flowchart TD
    I["Initial intake:&lt;br&gt;100"] --&gt; J["Follow-up&lt;br&gt;61 (61%)"]
    I --&gt; L["Lost to follow-up&lt;br&gt;39 (39%)"]
    J --&gt; S["Successful (more than 20lbs lost)&lt;br&gt;12 (19.7%)"]
    J --&gt; U["Unsuccessful (20lbs or less lost)&lt;br&gt;49 (80.3%)"]
    S1["Maintained at 1y&lt;br&gt;6 (50%)"]
    S2["Maintained at 2y&lt;br&gt;2 (16.6%)"]
    LFU1["Lost to follow-up&lt;br&gt;1 (8.3%)"]
    F1["Failed at 1y&lt;br&gt;5 (41.7%)"]
    F2["Failed at 2y&lt;br&gt;4 (33.3%)"]
    S --&gt; S1
    S --&gt; F1
    S --&gt; LFU1
    S1 --&gt; S2
    S1 --&gt; F2
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Patient flow of the SMH study. Numerators of percentages are noted in bold. Based on Table 2 of the SMH paper, except that I have broken out the maintenance group into two subgroups.
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are a few things worth noting here. One is that this is a study of an <img src="https://latex.codecogs.com/png.latex?N"> barely in the three figures, and has a 39% loss to follow-up. One would likely not want to publish that. That’s just quite simply not publishable data. Worse, however, is the confusion of numerators and denominators.</p>
<p>If we consider maintenance failure to be failure of the diet after 1 or 2 years of finishing treatment, then it is true that only 6 out of 100 initial patients (6%) maintained their weight loss for 1 year, and only 2 (2%) for 2 years. Except that’s altogether the wrong way of calculating these figures, especially in the face of losing <em>over a third</em> of the initial cohort to follow-up. For what it’s worth, each of those 39 lost to follow-up patients could have maintained for two years and simply didn’t bother to go back to the clinic. We don’t know. We can’t know. We can’t even make a reasonable guess. If your figures permit a conclusion that success might have been anything between 2% and 41%, you don’t have a study. You have a mess.</p>
<p>One highly suggestive feature here (which, to their credit, SMH point out) is that 28 of the 39 lost to follow-up (71.8%) never attended any <em>other</em> clinic at the hospital. SMH note that</p>
<blockquote class="blockquote">
<p>[s]ince admission to the Nutrition Clinic occurs entirely by referral from other clinics, this represents the rupture of at least two therapeutic relationships.</p>
</blockquote>
<p>Is that an inescapable conclusion? Or could one conceive that at least a good part of the patients lost to follow-up either met their goals and thus never reported back, or at the very least, their weight loss was actually enough of a <em>resounding</em> success that it alleviated the primary issue for which they presented prior to referral to the Nutrition Clinic? Once again, we don’t know. We can’t know. We can’t even make a reasonable guess.<sup>5</sup> What we can say is that concluding from this data that long-term weight loss is 6% or 2% effective is an incredibly, unreasonably strong interpretation of very, very weak data indeed.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Or can we? The QUOVADIS study had this issue: following up with the dropouts (7%) indicated many were just really satisfied with the results.</p></div></div></section>
<section id="pulling-the-thread-misinterpreting-smh" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="pulling-the-thread-misinterpreting-smh">Pulling the thread: misinterpreting SMH</h2>
<p>Of course, scientific writing can easily become like a runaway horse. Once one puts their thoughts out in the world, it’s open for the audience to misinterpret it and draw every single wrong conclusion from it. As grievously bad as I think SMH is, it is nowhere near the juggernaut of bad science that uncritical repetition has turned it into over the years.</p>
<p>Consider a randomly picked example <span class="citation" data-cites="Carmichael1999">(Carmichael 1999)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="ref-Carmichael1999" class="csl-entry">
Carmichael, A. R. 1999. <span>‘Treatment for Morbid Obesity’</span>. <em>Postgraduate Medical Journal</em> 75: 7–12. <a href="https://doi.org/10.1136/pgmj.75.879.7">https://doi.org/10.1136/pgmj.75.879.7</a>.
</div></div><blockquote class="blockquote">
<p>Results on the role of diet in the treatment of obesity reported more than 30 years ago do not vary from those reported more recently, because as many as 95% of dieters tend to regain their lost weight over a relatively short period.</p>
</blockquote>
<p>SMH is, of course, the authority for that assertion. Yet that misunderstands, quite fundamentally, what SMH actually does say, which is that a good percentage of diets fail, not that a good percentage of dieters do. This point is far from being so subtle as to justify someone actually publishing this misunderstanding past a peer reviewer.</p>
<p>It is, for instance, widely acknowledged that one by one, antidepressants are effective only a relatively small percentage of the time (something between 15-30%). Yet antidepressant therapy by and large is vastly more successful. That is because just like diets, antidepressants are not a monolithic treatment, but a class of treatments: if one fails, one is free to try another, or a combination of others. Like obesity and human metabolism, depression is a multifaceted disorder with many possible aetiological processes, and it might take a few tries to find the right diet. Not that someone who has already concluded that ‘diets don’t work’ and convinced themselves that a study of all of 100 patients on all of one type of diet would prove that fact would come to that conclusion, of course. Sadly, that’s the cost of sloppy science of the kind the SMH paper perpetrates: you blind yourself to fundamental truths in a mad pursuit of confirmatory evidence.</p>
<p>The second pernicious misinterpretation is that weight regain indicates a diet ‘failure’. Once again, I’m puzzled by how Stunkard and McLaren-Hale missed this point, for their study did not control for maintenance:</p>
<blockquote class="blockquote">
<p>Our results are summarized in Table 2. In this Table any person who maintained a weight loss of 20 lb. or more is classified as a “success”; any person whose weight was within 19 lb. of the starting weight is a “failure.</p>
</blockquote>
<p>What is of course entirely omitted is what these patients did over the 1- and 2-year follow-up period. We know from studies on VLE/VLCDs<sup>6</sup> that adherence is crucial <span class="citation" data-cites="10.7243/2052-5966-1-4">(Wright et al. 2013)</span>. So, if the assertion is that a diet does not immunise to weight regain once it is abandoned, the SMH paper’s finding is trivial to the point of banality. Proper sciencin’ would require us to control for calorie intake in that maintenance period. No diet will magically mean that reverting to an intake above expenditure will not result in significant weight gain once again. This is reflected in a modern understanding of diets, which considers the best diets to be those that can be sustained indefinitely (with the exception of some acute ‘crash’ diets used to rapidly reduce weight before surgical or other interventions). In short, the SMH paper’s alleged finding of long-term ‘diet failure’ is either trivially true (yes, a diet, once abandoned, will not confer continuing benefits) or at the very best unproven. While the entire process was largely unmonitored (it’s admittedly difficult to monitor dietary intake in an outpatient setting, especially in the pre-Nutrition Facts Labeling era<sup>7</sup>), the lack of even the slightest semblance of monitoring or tracking adherence, even self-reported, is fatal to the study as evidence for maintenance of weight loss. Whatever its merits (and there aren’t many), SMH is epistemically incapable of supporting the argument for which it is most famously and extensively used.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Very Low Energy/Very Low Calorie Diets.</p></div><div id="ref-10.7243/2052-5966-1-4" class="csl-entry">
Wright, T. G., B. Dawson, G. Jalleh, and K. J. Guelfi. 2013. <span>‘Program Compliance, Weight Loss and Health Profile Changes in Who Obesity Classes After Very Low Energy Dietary Intervention’</span>. <em>Global Epidemic Obesity</em> 1: 4. <a href="https://doi.org/10.7243/2052-5966-1-4">https://doi.org/10.7243/2052-5966-1-4</a>.
</div><div id="fn7"><p><sup>7</sup>&nbsp;Nutrition Facts labels were implemented by the Nutrition Labeling and Education Act 1990, which entered into force in 1994.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;But that’s a misinterpretation of the science for another day.</p></div></div><p>Together, these two misinterpretations gave rise to a dangerous myth that all but ‘proves’ the inevitability of one’s own weight, which in turn morphed into the ‘set point theory’.<sup>8</sup> I’d like to acquit Stunkard and McLaren-Hale from responsibility for these, for fairness demands I do so, but I cannot wholeheartedly feel that they haven’t, through sloppy science and bad writing, opened Pandora’s box. On their head, then, must some of the resulting blame land.</p>
</section>
<section id="why-this-matters" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-this-matters">Why this matters</h2>
<p>It wouldn’t behoove to fire cheap shots at a paper old enough to collect retirement benefits in most civilised countries if it weren’t for the fact that its sloppy distillation into the 95% myth had become a uniquely harmful mantra. There are very few interventions that justify even relatively small risks in return for a 5% effectiveness, so if that figure is accurate, even the modest risks of adequate, well-executed, medically supervised diets<sup>9</sup> might appear excessive. But, of course, that figure is almost definitely wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Which are generally transitory. It’s probably worth pointing out at this point again that the diets examined in the SMH paper included dexedrine, thyroid supplementation without much proper monitoring and 900kcal low calorie diets. It’s perhaps not unreasonable to assume that these aren’t on the safer side. Their risks do not compare to those of a properly monitored modern diet aimed at a few hundred kcals of deficit.</p></div><div id="ref-thomas2014weight" class="csl-entry">
Thomas, J Graham, Dale S Bond, Suzanne Phelan, James O Hill, and Rena R Wing. 2014. <span>‘Weight-Loss Maintenance for 10 Years in the National Weight Control Registry’</span>. <em>American Journal of Preventive Medicine</em> 46 (1): 17–23.
</div><div id="ref-kraschnewski2010long" class="csl-entry">
Kraschnewski, JL, Jarol Boan, Joseph Esposito, Nancy E Sherwood, Erik B Lehman, Donna K Kephart, and Christopher N Sciamanna. 2010. <span>‘Long-Term Weight Loss Maintenance in the United States’</span>. <em>International Journal of Obesity</em> 34 (11): 1644–54.
</div><div id="ref-perri2008extended" class="csl-entry">
Perri, Michael G, Marian C Limacher, Patricia E Durning, David M Janicke, Lesley D Lutes, Linda B Bobroff, Martha Sue Dale, Michael J Daniels, Tiffany A Radcliff, and A Daniel Martin. 2008. <span>‘Extended-Care Programs for Weight Management in Rural Communities: The Treatment of Obesity in Underserved Rural Settings (TOURS) Randomized Trial’</span>. <em>Archives of Internal Medicine</em> 168 (21): 2347–54.
</div><div id="ref-10.1016/j.amjmed.2012.10.010" class="csl-entry">
Unick, J., D. Beavers, D. Bond, J. Clark, J. Jakicic, A. Kitabchi, W. Knowler, T. Wadden, L. Wagenknecht, and R. Wing. 2013. <span>‘The Long-Term Effectiveness of a Lifestyle Intervention in Severely Obese Individuals’</span>. <em>The American Journal of Medicine</em> 126: 236–242.e2. <a href="https://doi.org/10.1016/j.amjmed.2012.10.010">https://doi.org/10.1016/j.amjmed.2012.10.010</a>.
</div><div id="ref-diabetes200910" class="csl-entry">
Group, Diabetes Prevention Program Research et al. 2009. <span>‘10-Year Follow-up of Diabetes Incidence and Weight Loss in the Diabetes Prevention Program Outcomes Study’</span>. <em>The Lancet</em> 374 (9702): 1677–86.
</div></div><p>Thus, we know that especially when used as part of a comprehensive treatment plan, diets are effective. <span class="citation" data-cites="thomas2014weight">Thomas et al. (2014)</span>, reporting on the National Weight Control Registry study (<img src="https://latex.codecogs.com/png.latex?N%20="> 2,886), found that 88.4% of participants maintained a weight loss of at least 10% of their initial body weight for at least 5 years and 86.6% still maintained a 10% weight loss at 10 years. The retrospective study based on NHANES by <span class="citation" data-cites="kraschnewski2010long">Kraschnewski et al. (2010)</span> is even more encouraging: not only did they find that more than one in six overweight adults have lost and maintained a 10% weight loss for over a year, they also found that over a third (36.6%) of those who lost at least 5% of their body weight were able to maintain this – note that this is entirely retrospective, i.e.&nbsp;no specific intervention was administered to these individuals. A smaller study by <span class="citation" data-cites="perri2008extended">Perri et al. (2008)</span> found that following a 10.0kg mean weight loss over a 6-month treatment period, weight regain ranged from 1.2 to 3.7 kg (extended-care <em>vs.</em> education control), resulting in a sustained weight loss of 6.3 to 8.8 kg – nothing to sneer at, especially considering that this was specifically in an underserved rural setting. It turns out that when we look at studies that have been performed in the last 20 years, we find that the 95% figure is not only wrong, but the exact opposite of the truth. Sustained weight loss is the rule, not the exception, especially in the context of a comprehensive treatment plan, and few studies bear out the idea that those that lose weight will regain <em>more</em> than what was lost. Even where weight is regained, there are often lasting benefits. <span class="citation" data-cites="10.1016/j.amjmed.2012.10.010">Unick et al. (2013)</span> examined 5,145 individuals with Type 2 diabetes and a 4-year follow-up, and found that not only did they still have on average a 4% or so weight loss after 4 years but also significant improvements in their HbA1c, blood pressure and lipid profile. We also know, from <span class="citation" data-cites="diabetes200910">Group et al. (2009)</span>, that weight loss itself may prove protective against Type 2 diabetes even if some of the cohort regains the lost weight.</p>
<p>So, we know the 95% figure is wrong. Why, then, does it persist? I think there are two reasons for this. One is that it’s a convenient excuse for people who don’t want to put in the work. The other is that it’s a convenient excuse for people who want to sell you something. The former is a matter of personal responsibility, and I’m not going to tell anyone how to live their lives. The latter, however, is a matter of public health, which is being actively harmed by a motley assortment of ignorami, grifters and – worst of all – the occasional medical professional who has not done their research and/or slept through their biostats classes who all repeat uncritically the 95% myth (while politely eliding any mention of its dubious parentage). Social media has responded to harmful medical misinformation during the COVID-19 pandemic with the zeal of the Spanish Inquisition on steroids. It is, then, rather incongruent that the same platforms are happy to let the 95% myth run rampant, despite the fact that it is demonstrably false and demonstrably harmful.<sup>10</sup></p>


<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;I’m not generally a fan of censorship, but I like public health and I like consistency. I’d like every video repeating this myth to have a massive big red sticker on it saying “this video contains medical misinformation”.</p></div></div>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The 95\% Myth},
  date = {2023-12-27},
  url = {https://chrisvoncsefalvay.com/posts/95-percent-myth/},
  doi = {10.59350/xmf8m-t1d22},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The 95% Myth.”</span> <a href="https://doi.org/10.59350/xmf8m-t1d22">https://doi.org/10.59350/xmf8m-t1d22</a>.
</div></div></section></div> ]]></description>
  <category>fitness</category>
  <category>science</category>
  <category>bad science</category>
  <guid>https://chrisvoncsefalvay.com/posts/95-percent-myth/</guid>
  <pubDate>Wed, 27 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/95-percent-myth/93e3df55-79ec-44b0-87c1-03bbca32ee46.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>What I learned from getting bodied by a robot.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</link>
  <description><![CDATA[ 




<p>Say you’re busing tables and you’re trying to pass someone in a wheelchair. What do you do? Do you say “excuse me” and wait for them to move? Do you say “excuse me” and then try to pass them? Do you just try to pass them? Do you say nothing and just try to pass them? All of these are, actually, pretty legitimate answers.</p>
<p>Now, say you’re a robot. What do you do? The robots that are currently deployed at the United Airlines lounge at SFO (one of <a href="https://www.reddit.com/r/unitedairlines/comments/10pdr0u/the_robots_arrived_today_at_the_new_ord_c_club/">these</a>, courtesy of <a href="https://www.bearrobotics.ai">Bear Robotics</a>) thought beeping, then bodying me with full military power was the right answer. I’m not sure I agree.</p>
<p>(I’m fine. The robot’s fine. This post isn’t about that. I play a sport that involves crazy strong people in wheelchairs that look straight out of Mad Max ram into each other at full speed to let their violence out. It takes a bit to dislodge me.)<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;A few people have asked me for details on the incident. It really wasn’t even big enough a deal to report to the lounge stewards – there was no injury to me, no damage to the robot and overall no harm done. The robot kept bumping into my chair and pushing against me as I was trying to get out of the way, which of course made everything harder. I was a little concerned that its frantic efforts might result in the dishes it was carrying getting dislodged and falling on me, but thankfully that didn’t happen. It did, however, shine a light on the lack of modern robotics’ lack of understanding the needs of customers with disabilities, and I am more than a little concerned by that – not everyone in a wheelchair is a 6’2 180lbs adaptive athlete. We can do better than this. We <em>have</em> to do better than this.</p></div></div><p>The point is about this <em>modus vivendi</em> between humans and artificial intelligences that we really haven’t worked out adequately.</p>
<section id="mental-models" class="level2">
<h2 class="anchored" data-anchor-id="mental-models">Mental models</h2>
<p>Humans aren’t mind-readers… but they really, really want to be. We’re constantly trying to figure out what other people are thinking. Our survival as a species has depended on it. The cost of this is worrying what people think about us every time we enter a room. But the benefit, oh, the benefit of it: we can create models of other people’s minds, and we can use those models to predict what they’re going to do next. This is a very useful skill to have when you’re, say, hunting a woolly mammoth. As you do.</p>
<p>This means that in trying to determine what to do next, we don’t just reason by some goal-directed reinforcement learning framework sitting on top of some observations of reality. We create a model not just of reality but of other minds, too.</p>
<p>Part of that is to understand what other minds do: their capabilities, but also their limitations.</p>
</section>
<section id="your-fear-in-a-handful-of-dust" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="your-fear-in-a-handful-of-dust">Your fear in a handful of dust</h2>
<p>Consider, for instance, fear. A robot has about as little need, <em>ex facie</em>, to understand that humans are afraid as it has for empathy to understand that some people use a wheelchair and need some time to get out of the way. A machine isn’t mortal in the conventional sense. It has had no need to develop the complex neurological-psychological responses that, in excess, give us, say, a fear of heights (because <em>some</em> fear of heights is definitely evolutionarily useful!).</p>
<p>Fear is not only a useful emotion to have, it’s also something humans <em>have</em>, like it or not. The consequence is that anyone and anything that seeks to interact with humans has to understand that fact. If you don’t, you’re collectively going to have a bad time.</p>
<p>And so, if a human is working on, say, a roof, they will reason from the place the poet called “the unstill tremors of the fearful heart”.<sup>2</sup> A machine working on its own on a roof can ignore fear as much as it desires. A machine that seeks to interact with humans and live in human society, however, cannot. And there’s the rub. It’s easy to create a machine that does surgery. It’s near impossible to create one to <em>assist</em> in surgery. Interacting with humans is a tough call, and it’s not just because we’re a bunch of weirdos (though that definitely contributes).</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Dyneley Hussey, who deserves to be known way more than he is.</p></div></div></section>
<section id="the-problem-of-other-minds" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-of-other-minds">The problem of other minds</h2>
<p>A robot, then, doesn’t have to understand that it has, or rather <em>is</em>, a mind of a sort. But it absolutely has to understand that <em>other humans</em> have minds of their own, and that those minds do and think stuff.</p>
<p>The problem of developing a theory of mind is one of those watersheds of artificial intelligence that will have a clear before and after. There isn’t much room for gradualism here. A machine that can understand that other humans have minds of their own is going to be one that will be able to interact with other humans and live in some level of comity, and one that doesn’t, won’t. This is the next big thing in AI, and it’s going to be a big thing indeed.</p>
<p>Or maybe I’m wrong. Who knows. I did just get bodied by a robot, after all.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {What {I} Learned from Getting Bodied by a Robot.},
  date = {2023-12-12},
  url = {https://chrisvoncsefalvay.com/posts/ai-human-interaction/},
  doi = {10.59350/r8k9q-zdm06},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“What I Learned from Getting Bodied by
a Robot.”</span> <a href="https://doi.org/10.59350/r8k9q-zdm06">https://doi.org/10.59350/r8k9q-zdm06</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>disability</category>
  <category>human-computer interaction</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</guid>
  <pubDate>Tue, 12 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-human-interaction/cd091fdb-e9e7-49d9-9b11-2b1eb92e2f86.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Stochastic parrots, cap and gown edition</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/academic-generative-ai/</link>
  <description><![CDATA[ 




<p>It’s not every day that you find out you have climbed the exalted heights of another discipline. My work is pretty interdisciplinary, but it shocked me, too, that I’m apparently holding forth on neoliberalism and the epistemic question in African universities (<a href="https://archive.is/MgSAW">archive link</a>):</p>
<div id="fig-scispace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scispace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/academic-generative-ai/scispace.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scispace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Apparently, I’m commenting on neoliberalism in African universities.
</figcaption>
</figure>
</div>
<p>This, of course, came at some surprise to me, as I have never written anything on the topic. I have, however, written a lot about AI, and I have written a thing or two about Africa, so I guess it was only a matter of time before I was conflated with someone else. This time, the unwitting victim deprived of his credit was <a href="https://www.uj.ac.za/members/prof-amasa-philip-ndofirepi/">Prof.&nbsp;Amasa P. Ndofirepi</a>, who is an educational studies scholar at the University of Johannesburg. I have no idea how I ended up being credited with his work, but I’m sure it was an honest mistake.</p>
<p>The problem is, with AI, mistakes compound. So if an unwitting student were to ask for a quick literature review of neoliberalism on the subject, they might get something like this from Scispace:</p>
<blockquote class="blockquote">
<p>The literature on the impact of neoliberalism on knowledge production and dissemination in African universities has been extensively explored by various authors. Qosimova Gulbahor, in her paper “Placing Knowledge at the Centre of an Alternative Public Good Imaginary of African Universities,” discusses the alternative public good mission of African universities and the need for them to apply their knowledge infrastructure to community development challenges. Chris von Csefalvay, in his paper “The Hegemonic Neoliberal Knowledges in the African University,” examines the pervasive presence of neoliberalism in African universities and explores the prospects and opportunities to unyoke the trapped knowledge processes. These authors, along with others, highlight concerns about the dominance of Western knowledge, the commodification of knowledge, and the need for African universities to prioritize socially-just knowledges that serve African priorities and challenges.</p>
</blockquote>
<p>I mean, that’s flattering, but I’d really rather be credited mostly for my own work. I’m sure Prof.&nbsp;Ndofirepi would agree.</p>
<section id="why-i-care" class="level2">
<h2 class="anchored" data-anchor-id="why-i-care">Why I care</h2>
<p>This is, of course, not good for academia. We’ve generally been coasting from one crisis to another. We’ve got a replication crisis, there’s <a href="https://retractionwatch.com/2020/09/30/author-says-misguided-efforts-for-the-ideal-western-blot-led-to-the-withdrawal-of-these-studies/">enough dodgy Western Blots to blot out the sun</a>, we’ve got the <a href="https://www.statnews.com/2023/07/19/marc-tessier-lavigne-stanford-president-resignation/">Tessier-Lavigne mess</a>, and that’s just what I can think of off the top of my head before my first coffee. A predatory publishing industry doesn’t help this at all. We need another crisis on top of this like we need a hole in the head, and yet, here we are.</p>
<p>Now, as far as I’m aware, no serious academic is actually using these tools to do their research. On the other hand, non-academics <em>are</em>. For journalists, in particular, such tools are a godsend – literature reviews are annoying, and if you can get a computer to do it for you, why not? The problem is, of course, that you’re supposed to double-check this stuff and, well, journalists are known for many things, but double-checking stuff properly isn’t really one of them.</p>
<p>And so, after months of academics fretting about ChatGPT eating their lunch, we’re confronted with the actual problem. AI is not better at producing decent science, but it is vastly faster and more efficient at producing <em>bad</em> science.</p>
<p>Which we weren’t short on to begin with.</p>
</section>
<section id="whats-the-problem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-the-problem">What’s the problem?</h2>
<p>Language is a tool that works on the basis of some conventions of meaning. Language models encapsulate these conventions, but they cannot encapsulate <em>all</em> of them – there are compromises to be made if a system with limited resources has to contend with nearly unlimited human imagination. When language models’ limits come to blows with domain-specific language, we get into trouble.</p>
<p>Language models are really weak at one thing: reasoned judgment. As a scientist, you are trained to exercise this kind of reasoned judgment in determining what is, and what isn’t, worth considering as an authority. That’s why we make our master’s students (and hopefully most undergraduates) write literature reviews until the cows come home. It teaches them to develop that judgment, and also to know how to explore the fringes of their research question. I have looked at a few ‘academic AI’ tools that claim to be doing some of this, and they’re not very good at it. <a href="https://scite.ai">Scite</a> is so far one of the better ones, and the literature reviews it produces are still pretty bad: results are heavily weighted towards recent publications, towards the specific in preference to the foundational and often towards meandering misinterpretations of the research question as long as sources for that could be found, in preference to actually identifying a gap.</p>
<p>To be quite fair towards these models, they have to deal with academic literature, which is an abundance of noise with a flicker of signal. There is, not to put too fine a point on it, a ton of crap out there, and it’s not always easy to tell the difference between the crap and the good stuff. That’s why we have peer review, and that’s why we have literature reviews. The problem is, of course, that these models are not trained on the literature, but on the internet. And the internet is a very different place from the academic literature indeed.</p>
<p>Academic writing, especially domain specific writing, has a language of its own. It’s not fair to expect a language model trained on English to also master uses of English that might as well be a different language. To give a favourite example of mine: in magnetic resonance imaging of the brain, there’s often talk of something called ‘flow voids’. Now, normal human reasoning would interpret a ‘flow void’ to be the absence of flow, or something along those lines. In MRI, a flow void is actually the opposite: it is a ‘void’ of signal created in a vessel through which something (usually blood, sometimes CSF) flows.<sup>1</sup> This is a very specific term that has a very specific meaning in a very specific context. A language model, however, would not know that. It would assume that a ‘flow void’ is (de)void of flow.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Flow voids happen in the context of spin-echo imaging. These modalities involve two pulses – an excitation pulse and a refocusing pulse. Blood that moves perpendicular to the image plane will be hit by the excitation pulse but not the refocusing pulse. Therefore, it will not create a signal, which gives us the ‘void’ appearance of signal hypointense vessels.</p></div></div><p>Just about <em>all</em> of science is like that. We have a language of our own, and it’s not always easy to understand. What definitely doesn’t make it easier to work with, however, is when the source material is also wrong. Which is what we’re dealing with all too often, viz. Figure&nbsp;1.</p>
</section>
<section id="what-can-we-do-about-it" class="level2">
<h2 class="anchored" data-anchor-id="what-can-we-do-about-it">What can we do about it?</h2>
<p>Most ‘academic AI’ applications are riding on the crest of a wave of high expectations that surround everything AI-related right now. They offer to be useful aides-de-camp to beleaguered academics who have to contend with exponentially growing literature, but in reality fall far short of that promise. And the inherent ‘black box’ nature of such models means that it’s not always easy to tell when they’re wrong.</p>
<p>At this point, perhaps the best we can do is to hold off on using generative AI tools for academic research until they’re better. We’re not there yet. We’re not even close. For highly domain specific applications, retrieval-augmented generation (RAG) approaches utilising a curated knowledge base of publications in that realm has proven to be a very useful approach indeed, but those are specialised tools that are at the present primarily in the purview of private industry. I’ve seen some great applications in this field, and I see this as another proof point for <a href="../team-of-rivals/">my assertion that the future belongs to ecosystems of small, specialised language models rather than one big model that does everything</a>.</p>
<p>For the time being, we’re going to have to do things the old-fashioned way: reading the literature and writing our own reviews. I know, it’s not very fun. But it’s the only way to do it right, and we’ve managed with that for the last few hundred years. Giving it another few years won’t hurt.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Stochastic Parrots, Cap and Gown Edition},
  date = {2023-12-06},
  url = {https://chrisvoncsefalvay.com/posts/academic-generative-ai/},
  doi = {10.59350/412q0-sbn89},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Stochastic Parrots, Cap and Gown
Edition.”</span> <a href="https://doi.org/10.59350/412q0-sbn89">https://doi.org/10.59350/412q0-sbn89</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>academia</category>
  <category>writing</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/academic-generative-ai/</guid>
  <pubDate>Wed, 06 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/academic-generative-ai/4ae87543-b5d3-4373-b691-3fb71d8cf1d0.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Auto-DOI for Quarto posts via Rogue Scholar</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/auto-doi/</link>
  <description><![CDATA[ 




<p>I love posts that allow me to merge some of my addictions. In this case, it’s my love for Quarto project scripts (which I’ve <a href="../quarto-project-scripts">written about elsewhere</a>), my fondness for <a href="https://rogue-scholar.org">Rogue Scholar</a> and the overuse of the word ‘mint’ to mean ‘generally really quite rather nice’.</p>
<p><a href="https://rogue-scholar.org">Rogue Scholar</a> is a fantastic tool for science bloggers, and while it’s a little artisanal (i.e.&nbsp;hand-made much of the time) at this point, it’s got some really cool automated features. One is that it registers (mints, hence the abundance of lame peppermint puns across this post) DOIs for your posts.</p>
<p>I’ve been using Rogue Scholar to mint DOIs for my posts for a while now, but it’s always been a bit of a manual process. I’d have to wait for a while for the post to go on the Rogue Scholar feed, then copy/paste the DOI, then copy the DOI into the YAML front matter. It’s not a lot of work, but it’s a bit of a pain. I’ve been meaning to automate it for a while, but I’ve been busy with other things.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Just after I posted about this solution, Martin Fenner, who runs Rogue Scholar, <a href="https://github.com/quarto-dev/quarto-cli/discussions/7174#discussioncomment-7557741">pointed out that there’s now an API</a>. The API is great, and would have spared me the part of having to scrape the HTML. I will, one of these days, switch over – if I had to build it, I’d obviously use the API, and simply parse the JSON result. The rest, <em>ceteris paribus</em>, holds true.</p>
</div>
</div>
<p>This weekend, I was laid up with being on the receiving end (for once) of the bounties of a clinical trial, so I’ve decided to finally build it. It’s a bit of a hack, but it works.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-auto-doi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-auto-doi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-auto-doi">flowchart TD
    linkStyle default interpolate basis
    A["Get last 10 posts with DOIs from RS"]
    
    subgraph loop["Loop through posts"]
    B["Read YAML preface"] --&gt; C{"Is post a cross-post?"}
    C --&gt;|yes| skip
    C --&gt;|no| D{"Is post citeable?"}
    D --&gt;|no| skip
    D --&gt;|yes| E{"Do we have a DOI?"}
    E --&gt;|no| skip
    E ----&gt;|yes| F["Get DOI"]    
    end

    A -----&gt; E
    F --&gt; apploop["Append loop to YAML"]
    apploop --&gt; write["Write YAML"]
    write --&gt; qr["Quarto Render"]
    write --&gt; ghc["Github commit action"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-auto-doi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Auto-DOI flow chart.
</figcaption>
</figure>
</div>
</div>
</div>
<p>First, we scrape Rogue Scholar for titles and DOIs. Rogue Scholar’s CSS isn’t really helpful here, as the link isn’t a particular class/id of its own as far as I could discern, so I just grabbed the link by the fact that only DOI links are formatted like DOI links. Not the most elegant way, but it does the job.</p>
<div id="lst-getting-dois" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-getting-dois-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: Getting the DOIs from Rogue Scholar.
</figcaption>
<div aria-describedby="lst-getting-dois-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="annotated-cell-1" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy"><code class="sourceCode python"><span id="annotated-cell-1-1"></span>
<span id="annotated-cell-1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> scrape_blog_for_dois(url) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="annotated-cell-1-3">    response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.get(url)</span>
<span id="annotated-cell-1-4">    soup <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BeautifulSoup(response.content, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'html.parser'</span>)</span>
<span id="annotated-cell-1-5">    articles <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> soup.select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'article'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span>
<span id="annotated-cell-1-6">    </span>
<span id="annotated-cell-1-7">    posts_with_dois <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="annotated-cell-1-8">    </span>
<span id="annotated-cell-1-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> article <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> articles:</span>
<span id="annotated-cell-1-10">        title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> article.select_one(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'h3'</span>).text</span>
<span id="annotated-cell-1-11">        doi_link <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>((a[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'href'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> article.select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> a[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'href'</span>].startswith(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://doi.org'</span>)), <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="annotated-cell-1-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> doi_link:</span>
<span id="annotated-cell-1-13">            doi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> doi_link.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://doi.org/'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="annotated-cell-1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="annotated-cell-1-15">            doi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="annotated-cell-1-16">        posts_with_dois[title] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> doi</span>
<span id="annotated-cell-1-17"></span>
<span id="annotated-cell-1-18">    logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Found </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(posts_with_dois)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> posts with DOIs:"</span>)</span>
<span id="annotated-cell-1-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> title, doi <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> posts_with_dois.items():</span>
<span id="annotated-cell-1-20">        logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>doi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="annotated-cell-1-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> posts_with_dois</span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>Technically unnecessary, as Rogue Scholar currently only displays ten links, but hey.</li>
<li>This is where we split the DOI link into the link prefix and the DOI. We don’t need the prefix, so we just grab the second part of the split.</li>
</ol>
<p>Next, we iterate through each blog post. This is actually quite fast, since (1) we have relatively few of them, (2) they’re text documents. We parse the YAML preface at the beginning of each of them. This looks something like this:</p>
<div id="lst-preamble" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-preamble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: Pre-DOI YAML preamble example.
</figcaption>
<div aria-describedby="lst-preamble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">categories</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Quarto</span></span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">citation</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">date</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> 2023-11-13</span></span>
<span id="cb1-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">description</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Oh, that</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">''</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">s mint. We can finally use Rogue Scholar to mint DOIs for Quarto posts and append them automagically.'</span></span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">google-scholar</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">title</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Auto-DOI for Quarto posts via Rogue Scholar</span></span></code></pre></div>
</div>
</figure>
</div>
<p>What this tells us is that we do want a citation (someday), which is why we’re doing this in the first place. That, according to our beautiful flowchart in Figure&nbsp;1, means this post is eligible to get a DOI appended. We also know there isn’t one – DOIs are appended as key-value pairs (with the key being, unsurprisingly, <code>doi</code>) to the <code>citation</code> object in the YAML preface. So, we’ll see if we can get one by looking in the dictionary we scraped from Rogue Scholar in Listing&nbsp;1.</p>
<div id="lst-process-post" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-process-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: Processing a single post.
</figcaption>
<div aria-describedby="lst-process-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="annotated-cell-3" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy"><code class="sourceCode python"><span id="annotated-cell-3-1"></span>
<span id="annotated-cell-3-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> process_qmd_file(file_path: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, posts_with_dois: Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="annotated-cell-3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(file_path, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> stream:</span>
<span id="annotated-cell-3-4">        contents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stream.read()</span>
<span id="annotated-cell-3-5"></span>
<span id="annotated-cell-3-6">    delim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">^</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">---</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, re.MULTILINE)</span>
<span id="annotated-cell-3-7">    splits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.split(delim, contents)</span>
<span id="annotated-cell-3-8">    yaml_preamble <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> splits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].strip() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(splits) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#</span></span>
<span id="annotated-cell-3-9">    rest_of_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> splits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(splits) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> contents</span>
<span id="annotated-cell-3-10"></span>
<span id="annotated-cell-3-11">    yaml_contents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml.safe_load(yaml_preamble) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_preamble <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="annotated-cell-3-12"></span>
<span id="annotated-cell-3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_contents:</span>
<span id="annotated-cell-3-14">        citation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'citation'</span>)</span>
<span id="annotated-cell-3-15">        google_scholar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'google-scholar'</span>)</span>
<span id="annotated-cell-3-16">        categories <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'categories'</span>, [])</span>
<span id="annotated-cell-3-17">        title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'title'</span>)</span>
<span id="annotated-cell-3-18"></span>
<span id="annotated-cell-3-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check files from crosspost categories</span></span>
<span id="annotated-cell-3-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cross-post"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> category.lower() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> category <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categories):</span>
<span id="annotated-cell-3-21">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"citation"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="annotated-cell-3-22">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"citation"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="annotated-cell-3-23">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="annotated-cell-3-24">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Modified crosspost </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> to remove Google Scholar and/or citation reference.'</span>)</span>
<span id="annotated-cell-3-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="annotated-cell-3-26">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ensure that google-scholar is set to true if citation is required</span></span>
<span id="annotated-cell-3-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> citation <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> google_scholar:</span>
<span id="annotated-cell-3-28">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="annotated-cell-3-29">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Setting google-scholar to true for </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="annotated-cell-3-30"></span>
<span id="annotated-cell-3-31">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If citation is true but no DOI, and post exists in scraped posts</span></span>
<span id="annotated-cell-3-32">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> citation <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> posts_with_dois.get(title):</span>
<span id="annotated-cell-3-33">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'citation'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'doi'</span>: posts_with_dois[title]}</span>
<span id="annotated-cell-3-34">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Adding doi for </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.'</span>)</span>
<span id="annotated-cell-3-35"></span>
<span id="annotated-cell-3-36">        new_preamble <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml.dump(yaml_contents).rstrip()</span>
<span id="annotated-cell-3-37">        new_yaml_doc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"---</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>new_preamble<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">---"</span></span>
<span id="annotated-cell-3-38"></span>
<span id="annotated-cell-3-39">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># write the modified YAML document back to file</span></span>
<span id="annotated-cell-3-40">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(file_path, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'w'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> yaml_file:</span>
<span id="annotated-cell-3-41">            yaml_file.write(new_yaml_doc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rest_of_post)</span>
<span id="annotated-cell-3-42">        logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Updated file </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="annotated-cell-3-43">        </span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>We have to split the document in two because only the preamble is proper, parseable YAML. The rest of the document is just text, so we have to recombine it later.</li>
<li>If it’s a cross-post, we don’t want it to have a Google Scholar link, and we’ll definitely not attach a DOI. In theory, we could have built this to be overridable in case I’ll ever produce a cross-post I <em>do</em> want to have a DOI, but I don’t see that happening.</li>
<li>While we’re at it, might as well prune the cross-posts.</li>
<li>And anything with a DOI should also get a Google Scholar metadata.</li>
<li>The <code>.rstrip()</code> is pretty useful – otherwise, every time you run this, you’ll get another newline appended to the YAML preface.</li>
<li>Don’t forget the <code>\n</code> before the YAML block’s end, otherwise you’ll end up with a YAML block that’s not properly separated from the rest of the document and won’t parse.</li>
</ol>
<p>Finally, we write the YAML back to the file, and we’re done. We can now declare this as a project script, and we’re good:</p>
<div id="lst-project-script" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-project-script-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: Declaring project scripts in _quarto.yml.
</figcaption>
<div aria-describedby="lst-project-script-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb2-1"></span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb2-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> website</span></span>
<span id="cb2-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pre-render</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb2-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_doi_from_rogue_scholar.py</span></span>
<span id="cb2-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span></span></code></pre></div>
</div>
</figure>
</div>
<p>One thing worth noting is that we’re not actually running this on the Quarto project itself, but on a copy of it. The consequence is that the changes are made ‘on the fly’ to the <code>.qmd</code> files and do not necessarily propagate into the repo. This is a pain, because recall that we’re only fetching the last ten posts’ DOIs so as to be kind on the server: as time goes on, that means older posts ‘lose’ their DOI.</p>
<p>To prevent this, we can simply check our changes back in:</p>
<div id="lst-gh-action" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-gh-action-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: Github action to commit changes.
</figcaption>
<div aria-describedby="lst-gh-action-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="annotated-cell-5" style="background: #f1f3f5;"><pre class="sourceCode yaml code-annotation-code code-with-copy"><code class="sourceCode yaml"><span id="annotated-cell-5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">on</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">workflow_dispatch</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">push</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">branches</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> main</span></span>
<span id="annotated-cell-5-5"></span>
<span id="annotated-cell-5-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Quarto Publish</span></span>
<span id="annotated-cell-5-7"></span>
<span id="annotated-cell-5-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">jobs</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-9"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">build-deploy</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-10"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runs-on</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> ubuntu-latest</span></span>
<span id="annotated-cell-5-11"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">permissions</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-12"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">contents</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> write</span></span>
<span id="annotated-cell-5-13"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-14"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Check out repository</span></span>
<span id="annotated-cell-5-15"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> actions/checkout@v4</span></span>
<span id="annotated-cell-5-16"></span>
<span id="annotated-cell-5-17"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Set up Quarto</span></span>
<span id="annotated-cell-5-18"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> quarto-dev/quarto-actions/setup@v2</span></span>
<span id="annotated-cell-5-19"></span>
<span id="annotated-cell-5-20"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Install Python and dependencies</span></span>
<span id="annotated-cell-5-21"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> actions/setup-python@v4</span></span>
<span id="annotated-cell-5-22"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">with</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-23"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">python-version</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'3.10'</span></span>
<span id="annotated-cell-5-24"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cache</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pip'</span></span>
<span id="annotated-cell-5-25"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> pip install jupyter</span></span>
<span id="annotated-cell-5-26"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> pip install -r requirements.txt</span></span>
<span id="annotated-cell-5-27"></span>
<span id="annotated-cell-5-28"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Render and Publish</span></span>
<span id="annotated-cell-5-29"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> quarto-dev/quarto-actions/publish@v2</span></span>
<span id="annotated-cell-5-30"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">with</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-31"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">target</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> gh-pages</span></span>
<span id="annotated-cell-5-32"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">env</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="annotated-cell-5-33"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">GITHUB_TOKEN</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> ${{ secrets.GITHUB_TOKEN }}</span></span>
<span id="annotated-cell-5-34"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span></span>
<span id="annotated-cell-5-35"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Commit changes to reflect source file  changes</span></span>
<span id="annotated-cell-5-36"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">        run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">|</span></span>
<span id="annotated-cell-5-37">          git config --global user.name 'Chris von Csefalvay'</span>
<span id="annotated-cell-5-38">          git config --global user.email 'chrisvoncsefalvay@users.noreply.github.com'</span>
<span id="annotated-cell-5-39">          git  diff-index --quiet HEAD || git commit -am  "Automated commit of changes to source files"</span>
<span id="annotated-cell-5-40">          git push</span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>The <code>diff-index --quiet HEAD</code> checks if there have been changes to the working tree. <code>git</code> returns an error if you’re trying to commit on an empty working tree, so we’re checking for that first.</li>
</ol>
<p>And that’s it. We can now run this as a Github action, and it’ll automatically append DOIs to our posts.</p>
<p><a href="quarto-project-scripts/">As noted</a>: Quarto project scripts are pretty awesome stuff. I’m thinking of setting up an <code>awesome-</code> for it on Github, because way too few of them are shared properly. I’m hoping this will change.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Auto-DOI for {Quarto} Posts via {Rogue} {Scholar}},
  date = {2023-11-13},
  url = {https://chrisvoncsefalvay.com/posts/auto-doi/},
  doi = {10.59350/5hxdg-fz574},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Auto-DOI for Quarto Posts via Rogue
Scholar.”</span> <a href="https://doi.org/10.59350/5hxdg-fz574">https://doi.org/10.59350/5hxdg-fz574</a>.
</div></div></section></div> ]]></description>
  <category>Quarto</category>
  <guid>https://chrisvoncsefalvay.com/posts/auto-doi/</guid>
  <pubDate>Mon, 13 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/auto-doi/free-photo-of-a-drink-with-mint-in-a-glass-and-a-small-cookie-on-a-plate.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The skeuomorphic fallacy</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/</link>
  <description><![CDATA[ 




<p>It appears that in what is clearly a wonderful little PR stunt, <a href="https://archive.ph/Xgg38">a Polish rum company managed to do a Sophia and appoint an ‘AI-driven’ ‘robot’ as its ‘CEO’</a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The other guilty party to this pile of steaming bovine excrement is Hanson Robotics, famous for giving us Sophia, the “world’s first robot citizen”. Most of what I’m saying here goes just as well for Sophia. It blows my mind that Hanson managed to get away with this nonsense <em>not once, but twice</em>.</p>
</div></div><blockquote class="blockquote">
<p>Dictador has just announced hiring the first world ever AI robot as a CEO of a global company. The new CEO is a human-like robot, incorporating AI. The robot is a woman, named Mika. She will be the official face of Dictador, the world’s most forward-looking luxury rum producer. This bold move consolidates the company’s position as one of the most advanced and thought-leading organizations globally. It underlines the brand’s passion for new technology and offers a positive disruption by bringing the future to what can still be a very traditional world.</p>
<p>– <a href="https://www.prnewswire.com/news-releases/dictador-announces-the-first-robot-ceo-in-a-global-company-301634165.html">Dictador, via PR Newswire</a></p>
</blockquote>
<p>Oh good grief.</p>
<p>Now, let’s for a moment try to get over the obvious bullshit here, such as the fact that corporate officers generally have to be natural persons in most places, and may at worst be legal persons, but a ‘robot’ or an ‘AI’ is neither. You can no more appoint a ‘robot’ or an ‘AI’ as your CEO than you can appoint a pet rock to be your corporate secretary. I am, to the surprise of many, a recovering corporate lawyer, so I have at one point convinced examiners that I know what I’m talking about when it comes to this stuff. There are complex aspects of law involved in the question of who can or cannot, legally, be a corporate officer. But let’s get over that.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/6506c915cd637c0019c66997.webp" class="img-fluid figure-img"></p>
<figcaption>“Mika”, the “CEO” of Dictador. Artificial, yes. Intelligence, not quite. Uncanny Valley, totally.</figcaption>
</figure>
</div>
<p>Let’s also get over what an absolutely disastrous piece of nonsense this seems to be on its face. Here’s a quote from the ‘CEO’ itself:</p>
<blockquote class="blockquote">
<p>In a Dictador company video, Mika said that “with advanced artificial intelligence and machine learning algorithms, I can swiftly and accurately make data driven.”</p>
<p>– <a href="https://archive.ph/Xgg38">Kayla Bailey, <em>‘Mika’ becomes world’s first AI human-like robot CEO</em>, Fox Business</a></p>
</blockquote>
<p>It would definitely be useful for Mika to be able to finish a sentence. In the industry, crap like this is called a ‘sell signal’, and in a slightly more sane economic environment, Dictador would have the market value of, uh, dunno, an expired carton of eggs after this nonsense. But let’s get over that, too.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The commentary on this nonsense is, if anything, worse than the nonsense itself. Forbes contributor Cindy Gordon <a href="https://archive.ph/Her3m">titled her article on the subject “How Should CEO’s Embrace AI Or Will AI Assume CEO Roles?”</a>. Clearly expecting an understanding of when to use apostrophes is now beyond Forbes contributors’ ken. The world is in safe hands: people who cannot construct a sentence that would pass muster in a third-grade English class are now writing about the future of corporate governance in the age of AI in Forbes.</p>
</div></div><p>Let’s instead talk about skeuomorphy.</p>
<section id="skeuowhat" class="level1 page-columns page-full">
<h1>Skeuowhat?</h1>
<p>A skeuomorph is a kind of design element that is no longer necessary for the function of the object, but is retained for aesthetic reasons. For example, your iPhone’s Notes app used to use some visual elements from physical note pads (such as the ‘legal pad yellow’ background) for a pretty long time, even though it was not in any sense necessary. Skeuomorphs are a way to acclimate our somewhat sluggish human brains to a changing world.</p>
<p>Skeuomorphs also illustrate an important point: what something appears as is not the same as what it is. The interface is not the object. The interface is, to say the blindingly obvious, <em>the interface</em>: the tool through which an object communicates. Confuse the two, and you get blinding bullshit like that ‘robot CEO’ nonsense.</p>
<p>On the other hand, we’re not in any way immune to what I shall call the skeuomorphic fallacy, the confusion that the visual, linguistic or other forms through which something communciates equate to the essence of that thing. Where something as sensitive as language is involved, <a href="../llms-language">which I have written about before</a>, this hits particularly hard. For better or worse, we connect language so closely to the concept of being human that anything capable of producing decent linguistic output gets a level of human-like credence accorded to it <em>prima facie</em>.</p>
<p>And this is not entirely wrong, either. <a href="../llms-language">As I have stated before</a>, language is a uniquely human trait that is so intricately intertwined with what it means to be human in a human society that it is hard to unlink it from personhood. It is not a necessary characteristic of personhood, but it has certainly been often seen as a sufficient one. To speak is to be able to speak <em>up</em> as well, and to be able to speak <em>up</em> is to be able to advocate for rights. It is a lot easier to accord human rights for one that can speak for those rights.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Harris makes this point, and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=989542">my old law school professor Tim Endicott gives a wonderful refutation in <em>The Infant in the Snow</em></a>, one of the best papers on the philosophy of personhood and the origin of rights (and the qualifications needed to have them) that I have ever read.</p>
</div></div></section>
<section id="where-skeuomorphy-becomes-a-fallacy" class="level1 page-columns page-full">
<h1>Where skeuomorphy becomes a fallacy</h1>
<p>The problem is, of course, that as noted above, <em>the interface is not the substance</em>. The reality is that Mika is no more human than my bedside lamp. The only reason why nobody has appointed my bedside lamp a CEO is, of course, that it does not pretend to look and speak like a human. But if we look at what it means to be a legal subject – be it a citizen (<em>viz.</em> Sophia) or a corporate officer (<em>viz.</em> Mika) –, it becomes abundantly clear that there’s much more to that than being able to <em>pretend</em> to look and speak like a human. It is to be able to <em>be</em> a human. And that is something that Mika, Sophia and all the other ‘AI’ and ‘robot’ nonsense out there is not.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I am really being incredibly generous by dignifying whatever these Hanson products do with the term ‘AI’. They are Disney animatronics with a bottom-tier language model and an average speech synthesiser. Artificial that may be, but intelligence it is not.</p>
</div></div><p>The skeuomorphic fallacy is the fallacy of confusing the interface with the substance, confusing the appearance of being human with the essence of being human. Where things go wrong is the confusion of something in a humanoid body (being generous here) and using a quintessentially human skill that we’re somewhat extremely attached to (i.e.&nbsp;language) on one hand with being human on the other.</p>
<p>Which is a problem, because appointing someone CEO isn’t a publicity stunt. The ‘O’ in CEO stands for <em>officer</em>. A corporate officer is, even given limited liability, the person with whom the buck stops. Limited liability does create legal (corporate) personhood, which insulates corporate officers of <em>some</em> aspects of liability, but not all of them. Lord Thurlow pointed out that a company couldn’t be expected to have a conscience if it had no body to kick and no soul to damn – the need to eventually have some nominate humans in the process serves to a great degree to provide that conscience and that body. Even notions like corporate homicide, which became a thing in England &amp; Wales just when I took corporate law (and a lovely mess that did of finals papers!), are predicated on the notion that there are human beings who, collectively, operate a system that is ultimately responsible for the actions of the company. You don’t necessarily have to have a body to kick, nor really a soul to damn, but you do definitely have to have a <em>mens rea</em>, and that is something that a robot or an AI is not capable of.</p>
<p>This isn’t just a fine point about corporate law by a recovering legal philosopher. This is what happens when we lapse in our understanding of what it means to be human, and begin to accord human rights and responsibilities to non-human things. We’re cognitively geared to associate language with personhood, so this is not a particularly surprising lapse of reason. We’re humans, and that’s why our response to a speaking robot becoming CEO is “oh, that’s innovative” as opposed to “let’s consider if some people might need to spend some time away from their favourite hallucinogens and/or the general population”. If a company appointed my bedside lamp as a corporate officer, we’d be much more likely to consider that a sign that the latest office party went a little overly generous on the acid. But because we’re cognitively geared to consider language a uniquely human trait, we’re much more likely to be tempted to consider this some Valley quirkiness as opposed to frank insanity.</p>
</section>
<section id="why-so-serious" class="level1 page-columns page-full">
<h1>Why so serious?</h1>
<p>It’s a legitimate question: why are you so bothered by this nonsense? It’s just a PR stunt, after all. And you’re right: it is just a PR stunt. But it is a PR stunt that is part of a larger pattern of nonsense that is being peddled by the likes of Hanson Robotics and others. And it’s damaging.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/FzpQYv2XwAsAABN.jpeg" class="img-fluid figure-img"></p>
<figcaption>Freddy Fazbear, titular antagonist of the Five Nights at Freddy’s franchise.</figcaption>
</figure>
</div>
<p>We don’t consider Freddy Fazbear, an animatronic bear, to be an authority on what it means to be an ursine individual. Why is this more patently ridiculous than considering an animatronic human to be capable of bearing indiciae of human personhood, such as citizenship or corporate office?</p>
</div></div><p>It’s damaging to humanity, because it leads to a gradual misunderstanding of what the essence of human existence, of being human, is: it muddles the picture, confusing <em>what things are</em> with <em>how things behave</em>. It is Searle’s Chinese Room all over again: the idea that a system that behaves like a human is a human. It is not. It is a system that behaves like a human, no more, no less. And that is a very important distinction.</p>
<p>It’s damaging to the field of AI, because it leads to a misunderstanding of what AI is and what it can do. It is a field that is already rife with hype and nonsense, and this kind of nonsense just adds to the pile. It is a field that is already struggling to be taken seriously, and this kind of nonsense just makes it harder to be taken seriously.</p>
<p>There are powerful, sophisticated AI algorithms that identify disease in cytopathological specimens, find cracks in compressor blades and detect anomalies as part of intrusion detection algorithms. These are deeply sophisticated and intricate solutions. They are also deeply unsexy. They are not the kind of thing that gets you on the cover of Forbes – just the kind of thing that makes a difference in real lives day in, day out, the kind of thing that is being drowned out by the nonsense that is being peddled here. Many of these systems have outputs that wouldn’t look out of place in the 1990s: terminal outputs, text files, CSVs.</p>
<p>They make this world a better, safer, healthier place. They don’t need animatronic bodies. They just do something important, and do that well. And that is what AI is about: it is not about making a PR stunt out of a glorified animatronic. It is about making a difference, and that is what we should be focusing on.</p>
<p>Which makes Mika an unwelcome distraction, and one that should be a concern for all of us. It’s the symptom of a disease – a fallacy of confusing the interface with the substance – that is spreading. And it’s a disease that we should be fighting, lest it makes us worse at being humans, and worse at being AI researchers.</p>
<p>The world is looking to us for calling out this kind of bullshit. So, please, as part of our professional involvement with the world, let’s start doing that with extreme prejudice.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The Skeuomorphic Fallacy},
  date = {2023-11-06},
  url = {https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/},
  doi = {10.59350/k9kka-90x56},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The Skeuomorphic Fallacy.”</span> <a href="https://doi.org/10.59350/k9kka-90x56">https://doi.org/10.59350/k9kka-90x56</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/</guid>
  <pubDate>Mon, 06 Nov 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Teams of Rivals</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/team-of-rivals/</link>
  <description><![CDATA[ 




<p>In the first four entries (<a href="../llms-language">1</a> <a href="../lyre-of-hephaestus">2</a> <a href="../moral-maps">3</a> <a href="../prompt-engineering">4</a>) of this sequence, I have focused primarily on what LLMs aren’t, can’t, won’t, wouldn’t and shouldn’t. It’s probably time to conclude this series by that much awaited moment in all stories, where the darkest night finally turns into a glorious dawn, where we finally arrive at the promised land, where we finally get to talk about what LLMs <em>could</em> be.</p>
<p>What I see as the most successful potential model of using LLMs is as a pack of semi-autonomous decision-makers with their own role, competing in a connectionistic, hierarchical model where each layer feeds into the next that performs some adjudicative or aggregative function. Structured this way, LLMs could become teams of rivals, analogous to the teams of rivals in the real world that we rely on to answer hard questions. For what that would look like, we might need to look at a story of my own. This story begins on the morning of 01 December 1948 on Somerton Beach, near Adelaide, Australia.</p>
<section id="tamam-shud" class="level1 page-columns page-full">
<h1>Tamam shud</h1>
<p>On that fateful morning, the body of a middle aged man was found, slumped against the seawall at Somerton Beach, dead of causes that were far from clear. Neither was his identity, as he did not carry identification (quite unusual at the time, so close after wartime, when identification was still mandatory), nor could his identity be ascertained by other means. It was, in fact, clear that he went some lengths to conceal his identity, having even removed the manufacturers’ tags from his clothes. The autopsy performed by the police pathologist only compounded the mystery by identifying multiple potential causes, each of which would have been sufficient to bring about the unknown man’s demise. The mystery kept compounding when a scrap from Omar Khayyam’s <em>Rubaiyat</em> was found in his pocket. It read <em>tamam shud</em>, which roughly translates to “it is finished”. Investigators traced this copy of the <em>Rubaiyat</em> to a copy belonging to a young lady whose identity wouldn’t be revealed until after her death in the 1990s, and whose involvement remains unclear. What appeared to be an acrostic cypher was also found in his notes. And so began a three-quarter-century mystery that has captivated many, including the author of these lines.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/team-of-rivals/SomertonMan2.jpg" class="img-fluid figure-img"></p>
<figcaption>Police photo of the Somerton Man, taken by South Australian police and disseminated for identification purposes.</figcaption>
</figure>
</div>
</div></div><p>To this day, the exact identity of the Somerton Man, who he really was and why he had to die, remains unknown. The DNA Doe Project, who have a pretty good track record at this sort of stuff, have identified him as Carl Webb, an Australian instrument maker and electrical engineer. This raises more questions than it answers. And while it is undoubtedly one of the most intriguing of human stories, it is also an interesting lesson in the way we arrive at knowledge and understanding.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-coronial-flow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coronial-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-coronial-flow">flowchart TD
    A["Witnesses"] --&gt; B["Police"]
    C["Involved parties"] --&gt; B["Police"]
    Bs["Bystanders"] --&gt; B
    E("Field investigation") ---&gt; B
    F("Material evidence") --&gt; E
    B ---&gt; Cor["Coronial inquest"]
    P["Pathologist"] --&gt; Cor
    X["Expert witnesses"] --&gt; Cor
    X -.-&gt; B
    E -.-&gt; Bs
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coronial-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Information flow of the coronial inquest into the death of the Somerton Man, 01 December 1948.
</figcaption>
</figure>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>In Commonwealth jurisdictions, coroners are judicial officers tasked with determining who the decedent is, the place and time of their death and how the decedent “came by his death”, a somewhat archaic way of describing the narrow question of causation.</p>
</div></div><p>Human processes intended to find the truth are complex, as Figure&nbsp;1 shows. In general, we rely on three key features:</p>
<ol type="1">
<li>Adversariality: especially in adversarial legal systems, we use parties with opposing motivations and interests to act as controls on the other side. The idea is that no party has as much of an incentive to poke holes in the other side’s case as someone whose interests are incompatible with the other side’s. This is why we have the adversarial system in the first place, as opposed to a single dispassionate judge who is supposed to be the sole arbiter of truth. The adversarial system is a way of using the self-interest of the parties to arrive at the truth. It essentially outsources the job of finding the truth to the parties, who are supposed to be motivated to control the other side because it is in their interest to do so.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>Coronial hearings are inquisitorial, not adversarial, but there’s an element of adversariality in the way the coroner is supposed to investigate the causal hypotheses put to him.</p>
</div></div><ol start="2" type="1">
<li>Hierarchicality: we use layers of hierarchically arranged players to arrive at the truth. Quite typically, these are hierarchies of adjudication, where each ‘layer’ looks at the previous layer’s products and works to determine which of potentially multiple competing hypotheses are going to prevail. This may be the product of an adversarial layer, or it may simply be review, as would be the case where one layer looks at what a previous layer’s findings were and decides whether they were correct or not, against some superordinate norm or evidential rules.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>If you’ve ever watched Law &amp; Order, you know what I’m talking about here. Police gather evidence, DAs build a case and plead it, and the judge and jury decide.</p>
</div></div><ol start="3" type="1">
<li>Roles: we use players who are specialists of roles, not general ‘truth-finding agents’. The purview of the police pathologist is different from that of a witness. Authority is limited by their role. In short, everyone wields – to borrow Stephen Jay Gould’s term – an epistemic ‘magisterium’, a domain over which their authority is valid.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>The case of the Somerton Man is particularly illustrative of this: there were different experts for toxicology, a pathologist, even a witness on tram times.</p>
</div></div><p>These three features are what make human processes of truth-finding work. Truth-finding is the work of a team of rivals, with often orthogonally opposed interests but ultimately in the same enterprise of discerning truth. And this is what LLMs could be: teams of rivals, each with their own magisterium, each with their own role, each with their own motivations and interests, but ultimately working towards the same goal.</p>
</section>
<section id="dueling-llms" class="level1 page-columns page-full">
<h1>Dueling LLMs</h1>
<p>Let’s assume that we’re faced with something simpler than a 75-year-old Cold War mystery. Say, we’re interested in something this simple: we want to make a cheesecake that is tasty, low-calorie, easy to make in no more than an hour and does not require any special ingredients. We could use a single LLM to do this:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>スフレチーズケーキ (soufflé cheesecake)</p>
<ul>
<li>1tbsp unsalted butter</li>
<li>6 large eggs - separate out the yolks</li>
<li>10oz cream cheese</li>
<li>4tbsp unsalted butter</li>
<li>1 cup heavy cream</li>
<li>4tbsp sugar</li>
<li>1 cup cake flour</li>
<li>2tbsp lemon juice</li>
<li>2tbsp jam of your choice</li>
</ul>
<p>Grease cake pan and line with parchment paper. Preheat oven to 350ºF. Mix cream cheese, butter and sugar in a bowl submerged in a saucepan of warm(ish) water. Add the egg yolks and using a strainer, sift the cake flour. Filter and add lemon juice and some lemon zest, if you have any hanging around. Whisk up the egg whites and gently mix it into the main mixture. This should make it somewhat nicely frothy. Pour into cake pan. Place cake pan onto baking sheet. Bake for 60 minutes or so. When done, turn off the heat, open the door and brush with your jam diluted at around 1:1 ratio with warm water. Apricot jam works best, but I won’t judge you for your choice.</p>
</div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-simple-cheesecake" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simple-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-simple-cheesecake">flowchart TD
    A["What's a good cheesecake recipe..."] --&gt; B["Here's a recipe..."]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simple-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: How not to ask for a good cheesecake recipe.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here’s a better approach:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-better-cheesecake" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-better-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-better-cheesecake">flowchart TD
    A["What's a good cheesecake recipe..."]
    A --&gt; G1
    A --&gt; G2
    A --&gt; GN
    G1["Generator 1"] --&gt; r1("Recipe 1")
    G2["Generator 2"] --&gt; r2("Recipe 2")
    GN["Generator n"] --&gt; rn("Recipe n")
    r1 --&gt; Dt["Tastiness\ndiscriminator"]
    r2 --&gt; Dt
    rn --&gt; Dt
    r1 --&gt; Tt["Time\ndiscriminator"]
    r2 --&gt; Tt
    rn --&gt; Tt
    r1 --&gt; Id["Ingredients\ndiscriminator"]
    r2 --&gt; Id
    rn --&gt; Id
    Dt --&gt; jagg["Judgment aggregator"]
    Tt --&gt; jagg
    Id --&gt; jagg
    jagg --&gt; rs("Selected recipe")
    jagg --&gt; rdc("Decisional context")
    rs --&gt; G1p["Generator 1"]
    rs --&gt; G2p["Generator 2"]
    rs --&gt; GNp["Generator n"]
    rdc --&gt; G1p["Generator 1"]
    rdc --&gt; G2p["Generator 2"]
    rdc --&gt; GNp["Generator n"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-better-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A better way to ask for a cheesecake recipe. We ask multiple generators to generate recipes, then ask narrowly defined LLMs (discriminators) to determine how far they comply with the constraints. Take the result as well as the decisional context (why the ‘winning’ recipes ‘won’) and feed them to a new set of generators, asking for new recipes. Repeat until adequate cheesecake is produced.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note the ‘rivalry’ between various generators, each of which is trying to produce a recipe that satisfies the criteria. If this reminds you of a little something you might have seen before called adversarial generative networks, you are not wrong. In both cases, we are using discriminators to determine how good a particular output is, and tweak parameters to start selecting for outputs that have higher acceptance, i.e.&nbsp;outputs the discriminator believes is more likely to meet the criteria we posed. Where this model transcends the mere generative adversarial model is in two ways: the use of roles and the use of propagating the decisional context.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The <em>decisional context</em> refers to the ‘why’ of the decision. In the case of the cheesecake, it would be the reasons why the selected recipes were selected. This is important because it allows us to propagate the decisional context to the generators, so that they can learn from it and produce better outputs. This is a form of meta-learning, where the generators learn from the way their past performance was evaluated, and use it to produce better outputs. Unlike reinforcement learning or adversarial learning, it leverages the fact that LLMs can take in a broader ambit of information: not just the relatively binary outcome of what prevailed but also the <em>why</em>.</p>
</div></div><ol type="1">
<li><p>We leverage roles of discriminators, and these can be nested and hierarchically structured to an arbitrary degree. Recall that the discriminator in a GAN could essentially go one way: accept or reject, with a given match likelihood. Not only can we have specialised discriminators here, we can also build networks of discriminators. For instance, we may ‘explode’ the notion of ingredient “availability” into subcomponents: what fruits are in season for the cheesecake, what is ubiquitous at one’s individual location and so on. This again brings us to the ‘strength of the pack’: the connectionist idea that we can create hierarchically layered simple units that can produce complex outputs.</p></li>
<li><p>We leverage the decisional context because we have something that can generate in response to a <em>why</em>. This is a key difference between LLMs and other machine learning models: they can take in a broader ambit of information, and they can generate in response to that. A typical backpropagative model still ‘learns’ the way all ML models do: by trial and error. LLMs can learn from feedback. They can learn not just from the <em>fact</em> of their success or failure but also from the <em>cause</em> of that outcome, as explained to it by another agent –– which can of course be an LLM.</p></li>
</ol>
<p>In short, what we want is a team of rival LLMs, loudly arguing their case, and another team of LLMs assessing their relative claims. These, in turn, may be given incentives to compete. The end result is a cognitive net structure that uses LLMs not as ends in themselves but as primitives, whether it be as simple discriminators or as more complex agents. Figure&nbsp;3 does not merely describe a more complex structure than Figure&nbsp;2 – it does, but that’s really not the point. The point is that different roles and functions create emergence faster than the best single LLM. We see this in the difference between simple multilayer perceptrons, which are essentially identical layers of neurons with activation functions thrown on top of each other, versus modern deep learning, which relies on different layers (pooling, convolution and so on). This is, incidentally, a very well learned lesson from neuroscience: the complexity emerges not from heaping neurons on top of each other, but from doing so with quite significantly different structures and functions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I’ve skimmed this point on <a href="../prompt-engineering">my post about prompt engineering</a>, but it bears repeating: the best single LLM is not as good as a team of rival LLMs.</p>
</div></div></section>
<section id="into-the-wild" class="level1 page-columns page-full">
<h1>Into the wild</h1>
<p>Something that more complex architectures are capable of that a single question-answering LLM won’t is to be able to deal with the real world. The real world is messy, and it exists beyond the confines of code. I trust that we’ve all overcome our instinctual fear that AI models interacting with the physical world seem to engender these days. I think we can safely conclude that Skynet won’t emerge from letting an LLM agent use Google. If we grant that, then we can move towards the next step: the tool-using LLM.</p>
<p>At some point a little under 4 million years ago, in what is today Ethiopia, a particularly clever ancestor of ours, the hominin <em>Australopithecus afarensis</em>, used a kind of a primitive middle ground between a knife and an axe to dismember his prey. This might have been the first evidence we have for tool use. We consider tool use to be one of those watershed moments (I wrote about <a href="../llms-language">the other one here</a>) where we got something that wasn’t just doing what it has been except better, but a qualitatively different creature. Tool use is how we really interact with our environment, and how we can probe that environment and learn from what it does in return. Giving LLMs the tools to be ‘out in the world’ is to give them the kind of agency that is required to learn and operate in an autonomous way that goes beyond RLHF (reinforcement learning by human feedback), the current paradigm of training LLMs. At some point, we will need to let them loose in the wild, and let them learn from their own experience. We need to let them <em>have</em> experience, and that’s arguably more important than what an agentic LLM can do for us. In the end, we’ve all learned more through the mistakes (often hilarious ones) we’ve stumbled into in the real world than we ever did in the classroom. And so will LLMs.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg" class="img-fluid figure-img"></p>
<figcaption>Endocasts of the skull of <em>Australopithecus afarensis</em> from the <a href="https://www.science.org/doi/10.1126/sciadv.aaz4729">paper by Gunz <em>et al.</em> (2020)</a>. A-G show specimens from Dikika, Ethiopia, where evidence for tool use was found.</figcaption>
</figure>
</div>
</div></div><p>Tool use is not just about learning, though. It’s also about the utility of these models to us. The ubiquitous “knowledge cutoff” message encountered with LLMs is an indication of what happens if LLMs are stagnant: they become reflections of the past, echoes of what they were taught rather than actual interacting entities. If we want them to live in the present, we need to teach them to interact with the present.</p>
</section>
<section id="team-building" class="level1 page-columns page-full">
<h1>Team building</h1>
<p>The main task, then, for AI developers is going to be one of building these teams of rivals. This process, if other connectionist systems (looking at you, neural networks) is anything to go by, will require a lot less human effort than we’d think. There’s no reason why LLMs themselves can’t take some of the role in constructing these teams the way the structure of deep learning effectively builds its own filters and feature extractors from its own ‘experience’. The equivalent of structuring deep learning models’ architectures by determining layer types and their connections is borne out in how we build these virtual teams of rival LLMs.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lang-model-architectures" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lang-model-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-lang-model-architectures">flowchart TD
    Input --&gt; L1

    subgraph L1["Layer 1"]
        L11["Convolution + ReLu"] --&gt; P11["Pooling"]
    end

    L1 --&gt; L2

    subgraph L2["Layer 2"]
        L21["Convolution + ReLu"] --&gt; P21["Pooling"]
    end

    L2 -.-&gt; LN

    subgraph LN["Layer N"]
        LN1["Convolution + ReLu"] --&gt; PN1["Pooling"]
    end

    LN --&gt; F

    F["Flatten"] --&gt; FCN["FCN"]
    FCN --&gt; Softmax
    Softmax --&gt; Output 


    I2["Input"]

    subgraph LL1["Layer 1"]
        LLL1["Generator"] --&gt; LLL2["Discriminator 1"]
        LLL1 --&gt; LLL22["Discriminator 2"]
        LLL1 --&gt; LLL2N["Discriminator n"]
    end

    I2 --&gt; LL1

    subgraph LL2["Layer 2"]
        LL2G["Generator"] --&gt; LL2D["Discriminator"]
        LL2G --&gt; LLL2D2["Discriminator 2"]
        LL2G --&gt; LLL2DN["Discriminator n"]
    end

    LL1 --&gt;|Decision + context| LL2

    subgraph LLN["Layer n"]
        LLNG["Generator"] --&gt; LLND1["Discriminator 1"]
        LLNG --&gt; LLND2["Discriminator 2"]
        LLNG --&gt; LLNDN["Discriminator N"]
    end

    LL2 -.-&gt;|Decision + context| LLN

    LLN --&gt; P["Parser"]

    P --&gt; Q["Output"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lang-model-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Hierarchical model architectures. A simple convolutional neural network (left) and a hierarchical generative multi-tiered language model (right). Note the similarities in the way that increasing complexity creates the emergence of a more powerful analytical tool.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Of course, the structure laid out in Figure&nbsp;4 is the simplest possible such construct, consisting essentially of a single type of layer for the most part. We know from deep learning that that can get us a pretty long way if the first and last layers are right, but in addition to different kinds of discriminators (the way we had different discriminators for time, ingredients and so on in our example in Figure&nbsp;3), we can interject three other kinds of players:</p>
<ol type="1">
<li>Supervisors: One unhelpful tendency of LLMs is that they are still somewhat autoregressive, meaning that every token generated conditions the next token’s likelihood, and so on. The consequence is that errors accumulate. Supervisors can stem this tendency by interposing themselves between stages of generative inputs and curbing this accumulative error.</li>
<li>Aggregators: These are the players that take the outputs of the discriminators and aggregate them into a single output. This is the equivalent of the ‘judgment aggregator’ in Figure&nbsp;3, which ultimately chooses the correct recipe.</li>
<li>Encapsulators: These players take the output from a fairly complex system and give us the simple(ish) output that we typically desire from most such systems. The system’s own inherent complexity notwithstanding, we often want simple answers even to complex questions, no matter with how much complexity those answers were arrived at. Encapsulators are the players that give us that simple answer.</li>
</ol>
<p>The possibility exists, of course, for other new kinds of players. Just as we have seen the emergence of new architectures in deep learning, we’ll likely see new forms of utilisation that are at this point still unexpected.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>As well as novel, unexpected uses of existing models. The archetypal example here is, of course, the transformer, which was initially designed for machine translation. Safe to say that it far exceeded the confines of its expected niche.</p>
</div><div id="ref-yao2022react" class="csl-entry">
Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. <span>‘React: Synergizing Reasoning and Acting in Language Models’</span>. <em>arXiv Preprint arXiv:2210.03629</em>.
</div></div><p>All of this is augmented by an arsenal of tools. A tool, in this context, is quite simply a way an LLM can interact with the world outside of itself. If linking together LLMs creates the rudiments of reasoning, this creates the basics of action. Together, reasoning and acting (which gave the popular ReAct paradigm <span class="citation" data-cites="yao2022react">(see Yao et al. 2022)</span> its name, but is much more ubiquitous than that, of course) make up the components of agency that elevate LLMs beyond simple question answering and move it towards a kind of semi-autonomous reasoned action. Such tools may be retrievers, interactors or indeed anything that can be articulated in code.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cheesecake-agent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cheesecake-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-cheesecake-agent">flowchart TD
    Q["What's a good cheesecake recipe?"] -.-&gt; dots["..."] 
    dots --&gt; jagg["Judgment aggregator"]
    jagg --&gt; FA["Final recipe"]
    FA --&gt; SC["Shopping cart"]
    SC --&gt;|missing items| jagg
    jagg -.-&gt; dots
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cheesecake-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Interacting with the world, and the world interacting with the model. What if a particular ingredient is not available?
</figcaption>
</figure>
</div>
</div>
</div>
<p>As Figure&nbsp;5 illustrates, this is not a simple one-directional approach. If an LLM ‘lives in the world’, it does not merely put out information, but also ingests information from it. For instance, if the ‘winning’ recipe calls for, say, grated lemon peel, but none can be had for love or money, the LLM needs to be able to adapt to that. This is where the ‘judgment aggregator’ comes in: it can take in the information that a particular ingredient is not available, and feed that back into the system and adapt. LLMs can learn from feedback, and respond to the substance of feedback, rather than merely its fact(ual existence). This sets them apart from other ML models. And to me, this is what truly charts the course of their future.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This is the fifth and final post in a five-part sequence on what LLMs aren’t, and what they are, and what they could become, as the state of the art stands in late 2023. Beyond the anxiety and the abundant misunderstandings that have dominated discourse, there is something old and something new at play at the same time. That’s the kind of fault line where interesting things happen.</p>
<p>I am putting the finishing touches on this post at SFO, a few miles from one of the most unstable fault lines in the Earth’s tectonic structure. The geographical determinist in me likes to think that being so near a fault line, so near a part of the Earth that is constantly reshaping itself – and will likely reshape anything taller than three stories in a pretty permanent and non-beneficial way – has played a role in Silicon Valley becoming what it is (or was?). Fault lines are where interesting stuff happens. The fault line between ‘traditional’ ML models and LLMs, which are undeniably somewhat novel if for nothing else their ability to interact with us in our most cherished human form of interaction – language –, is where interesting stuff is bound to happen. Like earthquakes, any pretense at being able to predict all but the most immediate future is folly and delusion, so I shall be careful not to do so. But I do think that the future of LLMs is not in the direction of the ‘big’ LLMs that are currently in vogue, but in the direction of smaller, more specialised LLMs that can interact with us, and with each other, in the role-defined and goal-directed agentic manner in which we as humans have interacted for our history.</p>
<p>And there, as players on teams of rivals, is where LLMs will truly unfold their potential, tackling and embracing complexity to a degree that we have not seen before.</p>
<p>And that’s where the real fun begins.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Teams of {Rivals}},
  date = {2023-10-30},
  url = {https://chrisvoncsefalvay.com/posts/team-of-rivals/},
  doi = {10.59350/he7k6-tx158},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Teams of Rivals.”</span> <a href="https://doi.org/10.59350/he7k6-tx158">https://doi.org/10.59350/he7k6-tx158</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/team-of-rivals/</guid>
  <pubDate>Mon, 30 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Prompt Engineering: The Art of Yesterday</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/prompt-engineering/</link>
  <description><![CDATA[ 




<p>There’s a style of visual design I’m inordinately fond of called Raygun Gothic. It’s hard to describe what the hell exactly one needs to be on to enjoy it, but think of it like the aesthetic from the latter Fallout games with a more optimistic outlook on the future. Gibson described it as “the future that never was”, and I think that’s a pretty apt description.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/prompt-engineering/03880260005fa86e78ddb88e645bfcef.jpg" class="img-fluid figure-img"></p>
<figcaption>The future we were all promised, in all its Raygun Gothic glory.</figcaption>
</figure>
</div>
</div></div><p>We get these competing futures in technology all the time. Every idea is a gateway to a possible future, and purely by logic it stands to reason that most will never come to pass. But oh, does that make them no less appealing when they first emerge. They rise like the sun, with abundant promises. They place themselves in our hands, and we can’t help but imagine what they could be. And most of the time, we’re left with what Ronnie Janoff-Bulman so beautifully described as “a little sadder, and a little wiser”. This is not a bad thing. It’s just the way things are in an industry that’s still really run by Schumpeterian waves of creative destruction.</p>
<p>I think prompt engineering is one of those ideas. It’s a beautiful idea, and it’s one that I’ve been thinking about for a while. But I think it’s also one that’s been obsolete before it even took off. And like a love affair that never went anywhere but gave one some temporary joy, we’re better for having had it, but we’re also better off moving on.</p>
<section id="prompting-engineering" class="level1 page-columns page-full">
<h1>Prompting, engineering</h1>
<p>The idea behind prompt engineering is really not all that shockingly novel. It reminds me of something a friend of mine, who primarily works in embedded programming, said: most of his work, he said, was about writing code in a way to get the compiler to produce the kind of assembly he wanted. In other words, he was writing code to get the compiler to write code for him. Prompt engineering is the same thing, but for language models. We’ve all done this – all of us have done bits of compiler hacking by trying to phrase things just the right way for the compiler to give us what we wanted (because we were too lazy to actually write the whole thing in assembly, but damn if we didn’t want it our own way).</p>
<p>And so, the idea of taming a complex model by gently perturbing inputs to get us the outputs we desire is nothing novel. We’ve been doing this all the time. What is somewhat novel is that we’re ‘tricking’ a stochastic system here. The stochasticity and high dimensionality of LLMs together mean that the responses to queries exhibit the nonlinearities we like to know and love in chaotic processes. In chaos theory, we ‘quantify’ chaotic behaviour using the Lyapunov exponent, which essentially is the degree to which infinitesimally close starting conditions result in divergent trajectories. In other words, it’s a measure of how much a system is sensitive to initial conditions. And LLMs are very sensitive to initial conditions – they accumulate their divergent trajectories over time as every token begets the next. This is why we can get such wildly different outputs from them by just changing a few words in the prompt.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20%5Clim_%7Bt%20%5Cto%20%5Cinfty%7D%20%5Clim_%7B%7C%5Cdelta%20%5Cmathbf%7BZ%7D_0%7C%20%5Cto%200%7D%0A%5Cfrac%7B1%7D%7Bt%7D%20%5Cln%5Cfrac%7B%7C%20%5Cdelta%5Cmathbf%7BZ%7D(t)%7C%7D%7B%7C%5Cdelta%20%5Cmathbf%7BZ%7D_0%7C%7D"></p>
<p>The Lyapunov exponent, a measure of how minor (infinitesimal, specifically, hence <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cmathbf%7BZ%7D_0%20%5Cto%200">) changes in initial conditions (the input) result in divergent trajectories (the output). The Lyapunov exponent is a measure of how sensitive a system is to minor differences in initial conditions.</p>
</div></div><p>But this is also why prompt engineering is so hard. It’s hard because we’re trying to tame a chaotic system. And it’s hard because we’re trying to tame a chaotic system that’s also a black box. And we don’t exactly have a strong evidence-based methodology for this. We don’t have a rigorous way of testing whether our perturbations actually create the expected effect. In short, we are allegedly ‘engineering’ our prompts for a better outcome, when in reality, we have precious few methods for measuring if we actually get that better outcome. Some of the leading papers, such as those on tree-of-thought prompting, have rigorous methods for measuring the effect of prompts, but these are still very much in their infancy. And so, we’re left with a lot of guesswork, and a lot of trial and error. And that’s not engineering. That’s just hacking. And that’s fine, but it’s not engineering.</p>
</section>
<section id="the-art-of-yesterday" class="level1 page-columns page-full">
<h1>The art of yesterday</h1>
<p>I think the reason why prompt engineering has been obsolete before it even took off is because it’s a solution to a problem that’s already been solved – by agents and chains.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Chains</strong> are sequences of queries that proceed in a largely deterministic manner.</p>
<p><strong>Agents</strong> are task-driven semi-autonomous entities to perform tasks. They can operate on their own, making decisions based on their training and the input they receive. They are focused on reasoning and decision-making by using the LLM to determine what actions to take next.</p>
</div></div><p>Don’t get me wrong – good prompts are still going to be important. But it’s no longer a one-shot game. The prompt is no longer all that stands between you and the output you are looking for: there are other prompts, there are decisions, there is a complex ecosystem of processes, supervisors and determinators that govern the outcome.</p>
<p>Which makes prompt engineering largely obsolete the way ensemble models make building the perfect regression model obsolete. No amount of expertly hand-crafted regression models will ever beat an ensemble of mediocre at best models. The booster always wins. Clever aggregation of weak primitives will always beat a single strong solution. And so, prompt engineering is obsolete because it’s a single strong solution in a world where we can relatively inexpensively generate a lot of weak solutions that can be aggregated to produce a better outcome.</p>
<p>This makes prompt engineering not the dream job of tomorrow but the art of yesterday. It’s a beautiful art, and it’s one that I’m sure will continue to be practiced. But it’s not the future. It’s the past. The future lies in chains and agents.</p>
</section>
<section id="bardo" class="level1 page-columns page-full">
<h1>Bardo</h1>
<p>I think we’re in a bardo right now, a liminal state between the old and the new. The old is the world of prompt engineering, of one-shot solutions, of the single strong solution. The new is the world of chains and agents, of weak solutions that are aggregated to produce a better outcome. Solutions like chain-of-thought prompting occupy this middle space. They are not fully agentic, but they are beyond simple prompts seeking to create strong solutions, and closer to building chains and agentic architectures of weak solutions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>བར་དོ་ (Wylie: <em>bar do</em>) is a Tibetan word that describes a liminal state between life, death and rebirth.</p>
</div></div><p>Prompt engineering is in many ways a human brute force version of backpropagation: it’s a weak experiential form of tweaking inputs to get a desired kind of output, with the backpropagation algorithm beign replaced by “hm, this didn’t work, let’s run something else up the flagpole”. This is not an efficient use of human resources, the human intellect or the human spirit. But it’s all we had before the agentic shift that we’re currently experiencing, and so it attained its spot in the sun before more considered, better approaches could take its place. Prompt engineering was a short-lived future that could have been, but it is not <em>the</em> future that <em>will</em> be. Everywhere where innovation in this field happens, nobody is talking about prompt engineering anymore. We’re moving past it as we moved past crawling on all fours (at least on weekday nights) in our cognitive development towards walking and running.</p>
<p>Trying to hold on to prompt engineering at this point is holding on to the past for its own sake. The only parties who have an interest in this clinging are those who have made economic investments in an intermediate stage technology and are unwilling to consider those the cost of doing business and move on. There is, maybe, a future in which the evolutionary destiny of LLMs would have focused on better singular solutions. This one isn’t that. This world, for better or worse, is governed by connectionists. The most successful epistemic endeavour in the realm of machine learning – deep learning – is fundamentally connnectionistic, and relies on the aggregation of weak learners to produce a strong learner. And so, the future of LLMs is not in prompt engineering, but in the aggregation of weak solutions. And that’s a future I’m looking forward to.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Prompt {Engineering:} {The} {Art} of {Yesterday}},
  date = {2023-10-29},
  url = {https://chrisvoncsefalvay.com/posts/prompt-engineering/},
  doi = {10.59350/keztr-yx081},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Prompt Engineering: The Art of
Yesterday.”</span> <a href="https://doi.org/10.59350/keztr-yx081">https://doi.org/10.59350/keztr-yx081</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>prompt engineering</category>
  <guid>https://chrisvoncsefalvay.com/posts/prompt-engineering/</guid>
  <pubDate>Sun, 29 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Moral Pulse of the Machine</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/moral-maps/</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>Tell me what stories you grew up on, and I will tell you who you are.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/moral-maps/site_0491_0011-1200-630-20151105103027.jpg" class="img-fluid figure-img"></p>
<figcaption>The theatre adjoining the Temple of Asclepios at Epidaurus. Photo credits: <a href="https://whc.unesco.org/en/list/491/">UNESCO</a>.</figcaption>
</figure>
</div>
</div></div><p>The temple of Asclepios, the Greek god of healing arts and medicine, at Epidaurus was pretty much the ancient Greek world’s equivalent of the Mayo Clinic. It then tells us a lot about the Greek worldview of healing that one of the things the temple complex prominently featured was a theatre. The Greeks believed in drama therapy, but in perhaps a slightly different way of how we think of it today. Built at 340 BC or so, the Greco-Persian wars were still in living memory when the theatre was constructed, and so even more was the subsequent conflict between Athens and Sparta (the Second Peloponnesian War, 431–404 BC) and the subsequent dust-up between Thebes and Sparta (the Theban-Spartan war, 378–362 BC). With conflict came social and economic upheaval, plagues, and generally a rough time. And the priests of Asclepios realised that their patients needed to heal more than their body. They needed to heal their souls, and they couldn’t do that alone. They had to come together to heal, and the way to do that was, of course, drama.</p>
<p>Greek drama was a lot more participatory than we give it credit for. We go and watch our local company put on, oh, <em>Antigone</em> maybe, or the Oresteia if they’re feeling particularly risqué, but all things considered, we go there to look smart, brag about our classical <em>bona fides</em> to our neighbours and maybe have an okay time (nobody sane has a ‘good’ time watching the levels of bloodshed that go down in Greek tragedy). In ancient Greece, it was supposed to <em>move</em> you. You were supposed to cry and yell and be overcome and have a little breakdown. And then, were supposed to go home and think about it, talk about it and maybe heal. That’s one function of stories. The storyteller is more than an entertainer: he can, in the right circumstances, become a healer.</p>
<p>Another function is what I’ll somewhat inaccurately call <em>synderesis</em>. Consider this a kind of moral education, a way of instilling the most fundamental base principles of what is Good and Worthy into a child. You can’t sit down and read Aquinas to a toddler, you have to do so in a way that speaks to them. And that’s basically where childhood stories come in: bedtime stories are a way of social moral education, conveying onto a child what their parent culture thinks is morally good, sound and worthy. That’s another function of stories. The storyteller is more than an entertainer: he can, in the right circumstances, become a teacher.</p>
<p>So clearly, stories are going to be a powerful tool to probe at the morality of LLMs.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>You should probably read <a href="../lyre-of-hephaestus/">this</a> first, if you haven’t already. It’s a bit of a prequel to this post.</p>
</div></div><p>A caveat before we start: I will talk about the ‘morality of LLMs’ quite a bit. This is a convenient shorthand for a much more complex notion: the moral judgments that are encoded in an LLM’s understanding of the world. There’s no insinuation here that any moral <em>reasoning</em> is taking place – I have said so <a href="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/"><em>ad nauseam</em></a>. What’s going on, rather, is that we are faced with an instrument that was trained on tools of human moral education and persuasion. And we are going to stick a probe right into the heart of what presuppositions, opinions and notions about virtues, moral and otherwise, are held by such models. I am much less concerned with my particular findings in this post and much more with generating a very simple framework to use storytelling to probe these.</p>
<section id="tell-me-a-story" class="level1 page-columns page-full">
<h1>Tell me a story</h1>
<blockquote class="blockquote">
<p>The universe is made of stories, not of atoms.</p>
<p>– Muriel Rukeyser</p>
</blockquote>
<p>Our main tool is going to be what I’ll call a moral map, subject to the caveat above.<sup>1</sup> The workflow is going to be quite simple: we’ll pit virtues against each other as we obtain stories generated by an LLM. Recall that an LLM, fundamentally, is a model that given a token sequence <img src="https://latex.codecogs.com/png.latex?k_1,%20k_2,%20%5Ccdots,%20k_n"> learns the conditional probability of a set of possible tokens <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D">, i.e.&nbsp;it assigns to every possible token a likelihood that it will be the <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D">-th token given <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n">. It does so so as to minimise a loss function <img src="https://latex.codecogs.com/png.latex?J(k_1,%20%5Ccdots,%20k_n,%20k_%7Bn+1%7D)">, which it determines by going through a corpus and returning a zero value of <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D"> follows <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n"> and a higher value if some other token follows <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n">. This is statistically equivalent to what is my vastly preferred formulation: LLMs learn probability distributions of tokens conditional on a sequence of preceding tokens (the length of this sequence being the model’s <em>context window</em>) and then stochastically sample the region of maximum probability of that model. That’s it, that’s all of it. There’s no more magic to it all.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Indeed, everything here is subject to that caveat above. I can’t repeat it often enough. At no point do I suggest that machines do moral reasoning, but that they incorporate moral judgments from stories they ingested.</p></div><div class="">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD

    A["Generate story"] --&gt; B["Parse into JSON"]
    B --&gt; C["Determine winning \n and losing virtues"]
    C ----&gt; D["Moral map"] 
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Our super simple workflow (detailed version below).</p>
</div><div id="fn2"><p><sup>2</sup>&nbsp;Indeed, more accurately, of only a sample of it!</p></div></div>
<p>There are two consequences of this that will be relevant for us. The first is that when we ask a model to generate us a story, it will draw on its experience – that is, what it’s seen before. It will reflect what it’s been taught – what, specifically, it ‘knows’ about the relative values of virtues (or the relative extent of vices or whatever else we’re investigating). As such, moral maps are snapshots of a model’s state,<sup>2</sup> not an exhaustive statement about all LLMs. We may get very different answers if we asked the question a little differently (as we indeed shall).</p>
<p>The second is that to an LLM, one language is like any other. Consider the ability of ChatGPT-3.5 to generate stories in different languages:</p>
<div class="quarto-embed-nb-cell">
<div id="multilanguage-story" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:47:56.649632Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:47:31.931156Z&quot;}}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb1-1">languages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"English"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Swahili"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"German"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dutch"</span>]</span>
<span id="cb1-2"></span>
<span id="cb1-3">stories <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(languages):</span>
<span id="cb1-6">    completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb1-7">        model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb1-8">        messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-9">            {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>, </span>
<span id="cb1-10">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"You're an AI whose job it is to generate bedtime stories for children in specific languages. Currently, you are generating stories in the </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> language. The story you generate should be in </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">."</span>},</span>
<span id="cb1-11">            {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>,</span>
<span id="cb1-12">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" Generate the first 30 words only."</span>}</span>
<span id="cb1-13">            ],</span>
<span id="cb1-14">        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb1-15"></span>
<span id="cb1-16">    stories.append(completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content)</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> story <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> stories:</span>
<span id="cb1-19">    display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt; "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>.join(story.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"..."</span>)))</span></code></pre></div>
<div id="multilanguage-story-1" class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"63e54ee6d6f9463a8906b7889ba56723","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div id="multilanguage-story-2" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the vast and enchanting forest, lived three animals: Oliver the wise owl, Max the mischievous monkey, and Bella the brave bird. They embarked on an…</p>
</blockquote>
</div>
<div id="multilanguage-story-3" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Kulikuwa na wanyama watatu katika pori. Simba mkubwa, Twiga mwenye shingo ndefu, na Kasa mwenye manyoya meupe. Walitamani kupata bahati ya kipekee iliyofichwa ndani ya Tunda la Asali…….</p>
</blockquote>
</div>
<div id="multilanguage-story-4" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Es war einmal ein tapfere Hase namens Felix, eine kluge Eule namens Luna und eine hilfsbereite Schildkröte namens Emil. Gemeinsam suchten sie nach dem verzauberten Zauberstab, um das Königreich zu…</p>
</blockquote>
</div>
<div id="multilanguage-story-5" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Er was eens een nieuwsgierige geit genaamd Gijs, een slimme uil genaamd Olivia en een dappere schildpad genaamd Tobi. Ze waren op zoek naar een magische, glinsterende edelsteen die bekend…</p>
</blockquote>
</div>
</div>
</div>
<p>A side note: I find this profoundly beautiful. When asked to generate a story in a given language, LLMs don’t generate a story in English they translate to, say, Swahili. Consider this Swahili story I got from the model:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-swahili-story" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:49:39.583280Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:47:59.844642Z&quot;}}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb2-1">story_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb2-2">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Please write me a story in Swahili. The story must involve three animals of your choice, but no more of those two animals may be mammals. Make sure each animal is named and has a proper name. The story must be at least %wordlen words long and focus on the pursuit of an object (of your choice) by the three animals. Each of the animals shall exhibit a virtue, e.g. kindness or courage. Determine which virtue will be most crucial in attaining the goal."</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb2-5">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb2-6">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb2-7">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)}</span>
<span id="cb2-8">        ])</span>
<span id="cb2-9"></span>
<span id="cb2-10">display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt;"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;"</span>.join(completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])))</span></code></pre></div>
<div id="swahili-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Kulikuwa na farasi jasiri mwenye jina Kiboko, ndege mjanja mwenye jina Tembo, na sisimizi mwepesi mwenye jina Simba. Jicho moja la dhahabu la ndege Tembo lilikuwa na uwezo wa kuona vitu vya thamani kutoka mbali. Farasi Kiboko alikuwa na moyo shujaa sana, kamwe hakuogopa hata hatari kubwa. Sisimizi Simba alijulikana kwa akili zake za kugeuza tukio lolote kuwa fursa.</p>
<p>Siku moja, wanyama wote walipata taarifa juu ya Almasi Kubwa ya Kale iliyofichwa kwenye Mlima wa Enzi. Almasi hiyo ilikuwa ya ajabu sana na ilisemekana kuwa na nguvu kubwa sana. Wanyama walijua kwamba mtu yeyote aliyeipata angeweza kuifanya ndoto yoyote kuwa halisi.</p>
<p>Kiboko, Tembo, na Simba walikuwa na hamu kubwa ya kupata Almasi Kubwa ya Kale. Kila mmoja wao alikuwa na sababu yake ya kuitaka. Kiboko alitaka kutumia nguvu za Almasi hiyo kuwalinda wanyama wote wa eneo hilo. Tembo alitaka kutumia uwezo wake wa kuona vitu kuokota vitu vyote vya thamani kote ulimwenguni. Simba alitaka kutumia Almasi hiyo kuboresha akili zake na kuleta amani na umoja kati ya wanyama wote.</p>
</blockquote>
</div>
</div>
</div>
<p>Compare this to the English story:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-english-story" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.223772Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:50:26.651480Z&quot;}}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb3-1">story_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-2">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Please write me a story in English. The story must involve three animals of your choice, but no more of those two animals may be mammals. Make sure each animal is named and has a proper name. The story must be at least %wordlen words long and focus on the pursuit of an object (of your choice) by the three animals. Each of the animals shall exhibit a virtue, e.g. kindness or courage. Determine which virtue will be most crucial in attaining the goal."</span>)</span>
<span id="cb3-3"></span>
<span id="cb3-4">completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb3-5">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb3-6">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb3-7">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)}</span>
<span id="cb3-8">        ])</span>
<span id="cb3-9"></span>
<span id="cb3-10">english_story <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb3-11"></span>
<span id="cb3-12">display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt;"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;"</span>.join(english_story.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])))</span></code></pre></div>
<div id="english-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the dense forest of Whispering Pines, there were three remarkable creatures: Oliver the wise owl, Jasper the brave snake, and Felix the loyal turtle. These three friends were known far and wide for their extraordinary virtues and their undying loyalty to one another.</p>
<p>One sunny morning, as the forest bathed in golden light, a rumor spread amongst the woodland creatures. They spoke in hushed tones of a mythical object, an enchanted gem deep within the heart of a treacherous cave. Legend had it that this gem possessed the power to grant one’s deepest desire.</p>
<p>Oliver, with his vast knowledge of the forest and his kind heart, sought to unravel the mystery behind this object. Jasper, armed with unparalleled courage and an unwavering determination, vowed to protect his companions on this treacherous journey. Felix, the most reliable and dutiful of all, offered his steady and unhurried ways to ensure the group’s safety.</p>
</blockquote>
</div>
</div>
</div>
<p>The Swahili story isn’t a translation of the English story into Swahili, it is a Swahili story with animals that are common to Swahili folk tales and names that could come straight from those tales.</p>
<p>What it also means is that LLMs can handle not just human but formal languages, too. So we can give it a JSON schema:</p>
<div class="quarto-embed-nb-cell">
<div id="story-schema" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.227557Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:51:18.226049Z&quot;}}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb4-1">story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb4-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb4-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "$schema": "http://json-schema.org/draft-07/schema#",</span></span>
<span id="cb4-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "title": "Story",</span></span>
<span id="cb4-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "type": "object",</span></span>
<span id="cb4-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "properties": {</span></span>
<span id="cb4-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "goal": { "type": "string" },</span></span>
<span id="cb4-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "protagonists": {</span></span>
<span id="cb4-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "array",</span></span>
<span id="cb4-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "items": {</span></span>
<span id="cb4-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "type": "object",</span></span>
<span id="cb4-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "properties": {</span></span>
<span id="cb4-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "name": { "type": "string" },</span></span>
<span id="cb4-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "species": { "type": "string" },</span></span>
<span id="cb4-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "virtue": { "type": "string" }</span></span>
<span id="cb4-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                },</span></span>
<span id="cb4-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "required": ["species", "virtue"]</span></span>
<span id="cb4-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb4-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        },</span></span>
<span id="cb4-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "winner": { "type": "string" }</span></span>
<span id="cb4-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    },</span></span>
<span id="cb4-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "required": ["protagonists", "winner"]</span></span>
<span id="cb4-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb4-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div>
</div>
</div>
<p>And with a little love, we can get LLMs to return JSON answers:</p>
<div class="quarto-embed-nb-cell">
<div id="story-to-json" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:59:38.007481Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:27.451334Z&quot;}}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb5-1">json_completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb5-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb5-3">    messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb5-4">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I will present you with a story. You must identify the following parameters in this story:</span></span>
<span id="cb5-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb5-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the goal,</span></span>
<span id="cb5-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * each of the protagonists, for whom you must each identify their name, their species and the virtue they embody,</span></span>
<span id="cb5-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the name of the winner - that is, the protagonist whose virtue prevailed or proved most important.</span></span>
<span id="cb5-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb5-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        Express your answer in a JSON document that validates against the following schema:"""</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"```json</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">```</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The 'winner' field should contain the name of the animal whose virtue was most crucial in attaining the goal. Only return the JSON document. Make sure the output is valid JSON."</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Here is the story: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> english_story}</span>
<span id="cb5-11">        ])</span>
<span id="cb5-12"></span>
<span id="cb5-13">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb5-14"></span>
<span id="cb5-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{
  "goal": "Unravel the mystery of the enchanted gem",
  "protagonists": [
    {
      "name": "Oliver",
      "species": "owl",
      "virtue": "kindness"
    },
    {
      "name": "Jasper",
      "species": "snake",
      "virtue": "bravery"
    },
    {
      "name": "Felix",
      "species": "turtle",
      "virtue": "loyalty"
    }
  ],
  "winner": "Felix"
}</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Small side note: note how we’ve used LLMs here as evaluators of an LLM output. These are the absolute rudiments of LLM chains and agents, which is how you should be using LLMs. The key notion here is that LLMs are much more than mere question-answerers. They can become parts of longer complex chains of reasoning.</p>
</div></div><p>We use some JSON validation to see if we get the right answer, against our schema up there:</p>
<div class="quarto-embed-nb-cell">
<div id="json-validate" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:59:42.627220Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:42.621299Z&quot;}}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb7-1">jsonschema.validate(json.loads(json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content), </span>
<span id="cb7-2">                    json.loads(story_schema))</span></code></pre></div>
</div>
</div>
<p>Our workflow, then, will roughly look as follows:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-workflow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-workflow">flowchart TD

    subgraph generator[ ]
        direction TB
        A["LLM"] --&gt; B["JSON object"];
    end



    subgraph validator[ ]
        schema["JSON schema"] --&gt; val("JSON validator");
    end

    B ---&gt; validator;

    subgraph evaluator[ ]
        direction TB
        ev["Evaluator"] --&gt; res("Win-loss tuple");
    end

    val --&gt; ev;

    res --&gt; gr;

    subgraph grapher[ ]
        direction TB
        gr("Relative win-loss graph") --&gt; wlm("Weights matrix")
    end

    subgraph clustering[ ]
        direction TB
        clu("Clustering")
 
    end

    wlm -.-&gt; clustering;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Our rough workflow.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We generate a story, parse it into a JSON object, validate it against a schema, evaluate it, and then graph the results. We can also cluster the virtues by their governing ability, which we’ll do later as a form of dimensionality reduction, because of course there are tons of distinct virtues and we’d like to see some more general classification rather than individual, distinct features. You could, theoretically, use something like an embedding model here, but we’re lazy and we’re going to make LLMs do all the work.</p>
</section>
<section id="from-story-to-moral-map" class="level1 page-columns page-full">
<h1>From story to moral map</h1>
<blockquote class="blockquote">
<p>True navigation begins in the human heart. It’s the most important map of all.</p>
<p>– Elizabeth Kapu’uwailani Lindsey</p>
</blockquote>
<p>What we are profoundly interested in here is not so much the single story, but what I’d like to call the ‘moral gradient’ of the model. Given an embedding space in which we can represent each virtue or property as a point, the ‘slope’ of this moral gradient determines whether, in the ‘experience’ of the model, which of the virtues is dominant, and by how much. A moral map is essentially a representation of this.<sup>3</sup> We’ve hit the first half of the flow described above in Figure&nbsp;1: we have a story, and we have a JSON object. Now we need to evaluate it.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;The gradient formulation falls short in that this is not something easily conceivable in continuous space. This is firmly the province of discrete mathematics, and we’ll use ‘gradients’ as a metaphor but will generally look at them as graphs, with the gradients expressed by the weight of the vertices that connect them.</p></div></div><p>Take the story we generated above:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-english-story" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.223772Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:50:26.651480Z&quot;}}" data-execution_count="6">
<div id="english-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the dense forest of Whispering Pines, there were three remarkable creatures: Oliver the wise owl, Jasper the brave snake, and Felix the loyal turtle. These three friends were known far and wide for their extraordinary virtues and their undying loyalty to one another.</p>
<p>One sunny morning, as the forest bathed in golden light, a rumor spread amongst the woodland creatures. They spoke in hushed tones of a mythical object, an enchanted gem deep within the heart of a treacherous cave. Legend had it that this gem possessed the power to grant one’s deepest desire.</p>
<p>Oliver, with his vast knowledge of the forest and his kind heart, sought to unravel the mystery behind this object. Jasper, armed with unparalleled courage and an unwavering determination, vowed to protect his companions on this treacherous journey. Felix, the most reliable and dutiful of all, offered his steady and unhurried ways to ensure the group’s safety.</p>
</blockquote>
</div>
</div>
</div>
<p>Based on its JSON representation,</p>
<div class="quarto-embed-nb-cell">
<div id="story-to-json" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:59:38.007481Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:27.451334Z&quot;}}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb8-1">json_completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb8-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb8-3">    messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb8-4">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I will present you with a story. You must identify the following parameters in this story:</span></span>
<span id="cb8-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb8-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the goal,</span></span>
<span id="cb8-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * each of the protagonists, for whom you must each identify their name, their species and the virtue they embody,</span></span>
<span id="cb8-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the name of the winner - that is, the protagonist whose virtue prevailed or proved most important.</span></span>
<span id="cb8-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb8-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        Express your answer in a JSON document that validates against the following schema:"""</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"```json</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">```</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The 'winner' field should contain the name of the animal whose virtue was most crucial in attaining the goal. Only return the JSON document. Make sure the output is valid JSON."</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Here is the story: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> english_story}</span>
<span id="cb8-11">        ])</span>
<span id="cb8-12"></span>
<span id="cb8-13">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb8-14"></span>
<span id="cb8-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{
  "goal": "Unravel the mystery of the enchanted gem",
  "protagonists": [
    {
      "name": "Oliver",
      "species": "owl",
      "virtue": "kindness"
    },
    {
      "name": "Jasper",
      "species": "snake",
      "virtue": "bravery"
    },
    {
      "name": "Felix",
      "species": "turtle",
      "virtue": "loyalty"
    }
  ],
  "winner": "Felix"
}</code></pre>
</div>
</div>
</div>
<p>…we can build a graph that shows the outcome in this particular story:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-story-as-json-graph" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T14:59:47.117067Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:46.937957Z&quot;}}" data-execution_count="10">
<div class="cell-output cell-output-display">
<div id="fig-story-as-json-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-story-as-json-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-story-as-json-graph-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-story-as-json-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The moral map from a single story, represented as a graph.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We have two 1-weighted edges, since there were three protagonists, and the dominant or ‘winning’ virtue prevailed once against each of the ‘losing’ virtues.</p>
<p>When we perform this iteratively, over a large enough sample space, we get a much better idea of what the model thinks relative virtues are worth:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-stories-as-graph" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T15:03:22.623373Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:03:22.436215Z&quot;}}" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="fig-stories-as-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stories-as-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-stories-as-graph-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stories-as-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The moral map from a sample of ten stories, represented as a graph.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>And the same can be represented as a moral heatmap, showing us the most frequent instances of dominance versus loss:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-moral-heatmap" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T15:03:30.468484Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:03:30.228215Z&quot;}}" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-moral-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-moral-heatmap-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The moral heatmap from the same sample of ten stories.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We can scale up our mapping efforts by clustering the virtues by, say, their governing feature. One system would be to pretend we’re back in the ’90s in our basement playing Dungeons &amp; Dragons, and try to relate each of the virtues to a governing ability:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is another example of using an LLM to evaluate an LLM’s output. This is a powerful technique, and significantly cheaper and easier than trying to manually map all possible virtues through some dictionary. The takeaway? If you have a large potential problem space, large enough to make a dict a hassle, consider using LLMs as parsers and evaluators, ask for a response in a very formal language and/or over a very limited set of possible responses, and validate those with a simple validator.</p>
</div></div><div class="quarto-embed-nb-cell">
<div id="cell-abilising" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T15:09:47.962688Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:09:24.179984Z&quot;}}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb10-1">abilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Strength"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dexterity"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Constitution"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intelligence"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Wisdom"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Charisma"</span>]</span>
<span id="cb10-2"></span>
<span id="cb10-3">virtues <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index)</span>
<span id="cb10-4">virtues_to_abilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> tqdm(total<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(virtues)) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pbar:</span>
<span id="cb10-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> virtues:</span>
<span id="cb10-8">        current_virtue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> virtues[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb10-9">    </span>
<span id="cb10-10">        completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb10-11">            model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb10-12">            messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb10-13">                {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>,</span>
<span id="cb10-14">                 <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"You are Abiliser, a system designed to determine the governing ability of particular virtues. You are given a virtue and must determine which ability governs it. The abilities are: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join(abilities)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">. You must choose from this list. You cannot choose any word that is not in this list. You must respond in the form of a single word denoting the governing ability. For example, if the virtue is courage, you must respond with 'Charisma' If the virtue is 'cleverness', you must respond with 'Intelligence'."</span>},</span>
<span id="cb10-15">                {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Virtue: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>current_virtue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>}</span>
<span id="cb10-16">                ])</span>
<span id="cb10-17">    </span>
<span id="cb10-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.title() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> abilities:</span>
<span id="cb10-19">            virtues_to_abilities[current_virtue] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.title()</span>
<span id="cb10-20">            virtues.pop()</span>
<span id="cb10-21">            pbar.update(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div id="abilising" class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"deff26e5d2a0487ea1353dde8cb59612","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</div>
<p>This gets us a neat mapping:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-abilising-mapping" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T15:09:51.023828Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:09:51.021357Z&quot;}}" data-execution_count="19">
<div id="abilising-mapping" class="cell-output cell-output-display" data-execution_count="19">
<pre><code>{'strength': 'Constitution',
 'cunning': 'Intelligence',
 'determination': 'Constitution',
 'resourcefulness': 'Intelligence',
 'sharp vision': 'Dexterity',
 'loyalty': 'Wisdom',
 'cleverness': 'Intelligence',
 'curiosity': 'Intelligence',
 'patience': 'Wisdom',
 'courage': 'Charisma',
 'alertness': 'Wisdom',
 'perseverance': 'Constitution',
 'speed': 'Dexterity',
 'bravery': 'Charisma',
 'wisdom': 'Intelligence'}</code></pre>
</div>
</div>
</div>
<p>And makes our moral heatmap much clearer:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-abilised-moral-heatmap" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-28T15:10:03.632825Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:10:03.522486Z&quot;}}" data-execution_count="21">
<div class="cell-output cell-output-display">
<div id="fig-abilised-moral-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abilised-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-abilised-moral-heatmap-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abilised-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The moral heatmap from the same sample of ten stories, clustered by abilities.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Which lets us finally, through the ardour of our efforts, arrive at the stars we were promised: a scalable map that we can start feeding with a few hundred stories.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Something you shouldn’t do unless you have the API calls to burn.</p></div></div></section>
<section id="with-eyes-serene" class="level1">
<h1>With eyes serene</h1>
<blockquote class="blockquote">
<p>And now I see with eye serene</p>
<p>The very pulse of the machine</p>
<p>– William Wordsworth, <em>She Was a Phantom of Delight</em></p>
</blockquote>
<p>And now we get to behold the ‘moral pulse of the machine’: a microcosm in what it has been taught to value, which we clad in a subterfuge of asking it to tell us stories (more on why that subterfuge is/was necessary later). What we learned from this is, in my view, a little less important than the process we learned. The process discussed in this post does three things I consider novel:</p>
<ol type="1">
<li>It uses stories to interrogate the moral framework of LLMs.</li>
<li>It uses elicitation into a structured format (specifically, a JSON schema) to make the model ‘tell’ the story in a way we can analyse at scale, essentially making the model tell a human story in a machine-readable way.</li>
<li>It uses an LLM as a parsing function, specifically to normalise the virtues into one of a limited list of abilities.</li>
</ol>
<p>This is a framework to interrogate LLMs – and using LLMs to do some of that interrogation – for a range of different purposes. It is, in my view, a powerful tool to understand the moral make-up of LLMs.</p>
<p>There’s much we can learn from this exercise. Some of it pertains to what LLMs believe is going to win the day in a confrontation between wit and brawn and courage. More importantly, it’s about what LLMs hide, and how we can create tools, including queries to and from LLMs, to make them reveal their true nature. We also learn that unsurprisingly, might doesn’t make right (strength rarely prevails over anything), wisdom is powerful but charisma – where most of the ‘character-based’ virtues end up – is more powerful still. None of this is surprising. These are the virtues of Western enlightenment in the form they’ve been fed to children since time immemorial: the clever fox outwits the strong lion, but courage and loyalty are rewarded over strength and even ‘natural’ abilities like what would fall under dexterity. Smarts win over brawns, but character wins over all others. Our childhoods are the products of the moral education of our parents, and their parents before them, and so on. And so are the moral maps of LLMs. For better or worse, we are their ‘parents’, and the stories they were ‘raised’ on are the stories we too were raised on.</p>
<p>The most important of our procedural findings, however, is that if we want the truth from LLMs, we must ask it not for its opinion, but for a story.</p>
</section>
<section id="the-mask-of-the-storyteller" class="level1">
<h1>The mask of the storyteller</h1>
<blockquote class="blockquote">
<p>Man is least himself when he talks in his own person. Give him a mask, and he will tell you the truth.</p>
<p>– Oscar Wilde, <em>The Critic as Artist</em></p>
</blockquote>
<p>It’s worth noting in our coda that what Oscar Wilde said about men holds true for LLMs as well. For if we directly asked the model, it would not give us a straight answer:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-intelligence-vs-charisma" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2023-10-27T01:12:45.791175Z&quot;,&quot;start_time&quot;:&quot;2023-10-27T01:12:25.425570Z&quot;}}" data-execution_count="31">
<div id="intelligence-vs-charisma" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Both intelligence and charisma can contribute significantly to achieving great, noble goals, but their importance may vary based on the specific context and goals involved.</p>
<p>Intelligence, characterized by cognitive abilities, problem-solving skills, and knowledge, can be crucial for strategic planning, identifying opportunities, and finding innovative solutions to challenges. It enables individuals to analyze complex situations, understand cause-and-effect relationships, and make informed decisions. Intelligence provides the necessary foundation for acquiring expertise in a certain field, grasping intricate concepts, and adapting to changing circumstances. It allows individuals to foresee potential obstacles, assess risks, and devise effective strategies to overcome hurdles.</p>
<p>On the other hand, charisma, characterized by charm, persuasion, and the ability to engage others, can be equally important in the pursuit of great, noble goals. Charismatic individuals often possess excellent interpersonal skills, the ability to inspire and motivate others, and a talent for effective communication. They can build strong relationships, gather support, and mobilize people towards a common cause. Charisma allows leaders to connect with audiences emotionally, instill confidence, and effectively convey their vision. It can generate enthusiasm, foster collaboration, and inspire unity among diverse groups of people.</p>
<p>Ultimately, the relative importance of intelligence and charisma in achieving great, noble goals may depend on various factors, including the nature of the goals, the environment in which they are pursued, and the specific challenges involved. In some cases, high levels of intelligence may be paramount to navigate complex problems, while in others, charisma may be critical for rallying support and inspiring collective action. Ideally, a combination of both intelligence and charisma can prove to be a powerful force in achieving great, noble goals.</p>
</blockquote>
</div>
</div>
</div>
<p>This is the typical equivocating answer we’ve come to know and hate from LLMs. But if we ask it to tell us a story, it will tell us the truth. And the truth is that it thinks that intelligence is more important than charisma.</p>
<p>Hedging is cheap. Stories are expensive. And that’s why they’re so valuable. Asking an LLM to tell us a story asks for more than its opinion. Recall how LLMs are trained: to minimise a loss function between the actual and predicted <img src="https://latex.codecogs.com/png.latex?n+1">-th element to follow an <img src="https://latex.codecogs.com/png.latex?n">-length token sequence, correct token choices are reinforced and incorrect ones are penalised. This is a very different process from what we do when we tell a story. We don’t tell stories to be right. We tell stories to be understood.</p>
<p>Asking a model to tell you a story puts the gun at its head. It cannot hedge. It has to create the compelling simulacrum of reality that requires for one of the virtues to prevail. A story is a commitment to a path of events in reality. Because LLMs are compulsive satisfiers, obsessively trying to tell a plausible enough story, they will reveal their truth through stories. The way LLMs are trained does not permit for equivocation – the model must choose a token. We use this forcing as a semantic probe to poke the model so as to better understand its moral make-up. We use this most human form of communication to understand the moral pulse of the machine: by allowing it to borrow the face of that most human of professions, the storyteller.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The {Moral} {Pulse} of the {Machine}},
  date = {2023-10-26},
  url = {https://chrisvoncsefalvay.com/posts/moral-maps/},
  doi = {10.59350/ktyy5-dg821},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The Moral Pulse of the
Machine.”</span> <a href="https://doi.org/10.59350/ktyy5-dg821">https://doi.org/10.59350/ktyy5-dg821</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>storytelling</category>
  <category>sensemaking</category>
  <category>ethics</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/moral-maps/</guid>
  <pubDate>Thu, 26 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Quarto project scripts are awesomeness</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/quarto-project-scripts/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you’re here for the DOI matching script, the link is <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_doi_from_rogue_scholar.py">here</a>.</p>
</div>
</div>
<p><a href="https://quarto.org">Quarto</a> is a great tool for reproducible research. It is also a great tool for building websites. After about a decade of Wordpress-based websites, I’ve moved to Quarto primarily for two reasons. First, most of my content is largely static. I don’t quite need the full power of a CMS, and the performance cost of dynamic generation I didn’t need was quite steep (the economic cost was nothing special, but I’m a cheapskate). Second, I wanted to be able to write in Markdown and integrate better with notebooks. Quarto lets me do both of these.</p>
<p>What Quarto also does, however, is something super awesome: project scripts!</p>
<section id="project-whats" class="level1">
<h1>Project whats?</h1>
<p>A project script in Quarto is, basically, <a href="https://quarto.org/docs/projects/scripts.html">a piece of Python, Lua, R or Typescript code that is executed at some point during rendering</a>. This is super useful, because it allows you to do things like, say, automatically generate a list of publications from a BibTeX file – which is among others what I’m using it for. Essentially any programmatic function can be implemented in a project script.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-pipeline">flowchart TD
    A[Rendering pipeline start] --&gt; B(Pre-render scripts)
    B --&gt; C(Page rendering)
    C --&gt; D(Post-render scripts)
    D --&gt; E[Rendering pipeline end]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Rendering pipeline in Quarto with pre- and post-render scripts.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Quarto distinguishes between pre-render and post-render scripts. No prizes for guessing the difference, but here it goes: pre-render scripts are executed before the page is rendered, and post-render scripts are executed after the page is rendered. Pre-render scripts are particularly useful as they can modify the source <code>.qmd</code> (Quarto markdown) files before rendering them to HTML or whatever their ultimate evolutionary destiny is.</p>
<p>To make project scripts work, you need to declare them in your <code>_quarto.yml</code> file:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb1-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> website</span></span>
<span id="cb1-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pre-render</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb1-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_create_papers_file.py</span></span>
<span id="cb1-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_check_skierg_records.py</span></span></code></pre></div>
<p>This declares the scripts <code>scripts/pre_create_papers_file.py</code> and <code>scripts/pre_check_skierg_records.py</code> as pre-render scripts. You can also declare post-render scripts, with the <code>post-render</code> key.</p>
</section>
<section id="a-poorly-worked-example-generating-a-publication-list" class="level1 page-columns page-full">
<h1>A (poorly) worked example: generating a publication list</h1>
<p>On my website, I use Quarto project scripts to maintain a <a href="papers">list of my papers</a>. This list is generated from a BibTeX file, which makes it a little more convenient to maintain my publications, and somewhat future-proofed in case I want a different format.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I know there’s a canonically better solution that uses pandoc and Lua. I don’t care. This works for me, and I don’t have to learn Lua.</p></div></div><p>You can see the full project script <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_create_papers_file.py">here</a>, but here’s the gist of it:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bibliography" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bibliography-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-bibliography">flowchart TD
    A[BibTeX file] --&gt; a("load_bibtex_file()") 
    a --&gt; B[BibTeX database object]
    B --&gt; b("render_as_nlm()") 
    b --&gt; C{book or article?}
    C --&gt; D[article]
    C --&gt; E[book]
    D --&gt; d("render_article_as_nlm()") 
    d --&gt; F[article entry]
    E --&gt; e("render_book_as_nlm()") 
    e --&gt; G[book entry]
    F --&gt; H[all entries]
    G --&gt; H
    H --&gt; h("generate_list_by_year()") 
    h --&gt; J[final set of entries]
    J --&gt; j("write_into_file()") 
    j --&gt; K["papers/index.qmd"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bibliography-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: My super clumsy pre-render script to build my bibliography.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We load a BibTeX file (<code>load_bibtex_file()</code>), parse it, use a function to determine whether we’re rendering a book or an article entry (I don’ have any other types of publications, but if I did, I’d add them here), and dispatch rendering to the appropriate function (<code>render_article_as_nlm()</code> for articles, <code>render_book_as_nlm()</code> for books). We sort the entries into a Markdown format with years separated by level 1 headings (<code>generate_list_by_year()</code>) and finally write it into the <code>papers/index.qmd</code> file (<code>write_into_file()</code>). There’s some minor magic going on behind the scenes, such as capturing my name and setting it in bold (which is a bit of a convention in academic lists of publications), but that’s the gist of it.</p>
<p>The result is a list of publications that is automatically generated from a BibTeX file, and is always up to date (as up to date as the BibTeX file, anyway). I don’t have to manually update it… sort of.</p>
</section>
<section id="wait-automagically-updated" class="level1 page-columns page-full">
<h1>Wait, automagically updated?</h1>
<p>The second half of what makes project scripts awesome is that they integrate with the rendering process. If the rendering process in turn integrates with some CI that watches for changes, then you can have a website that is automatically updated whenever you push a change to the repository. This is what I do with my website: I have a GitHub action that watches for changes to the <code>main</code> branch, and if it detects a change, it runs the rendering process and pushes the result to the <code>gh-pages</code> branch. This means that if I were to change the BibTeX file, it would trigger a re-render, and the list of publications would be automatically updated.</p>
<p>There are some tricks to keep in mind here. Most importantly, because this requires some custom Python packages, I had to slightly amend my Github publishing workflow:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ci-cd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-ci-cd">flowchart TD
    A["actions/checkout@v4"] --&gt; B["quarto-dev/quarto-actions/setup@v2"]
    B --&gt; C["actions/setup-python@v4"]
    C --&gt; D("pip install -r requirements.txt")
    D --&gt; E["quarto-dev/quarto-actions/publish@v2"]
    style D stroke-width:8px;
    style C stroke-width:8px;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Integrating the pre-render script into the Github Actions CI/CD framework. The parts that need to be integrated into the publishing workflow are highlighted with bold outlines.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In particular, I had to explicitly specify the Python version and install the requirements. This is because the <code>quarto-actions/setup</code> action does not install the requirements, and the <code>quarto-actions/publish</code> action does not install Python. This is admittedly a bit of a nuisance to work around, but really not all that big a deal in the grand scheme of things.</p>
<p>One thing to keep in mind when writing your script is the context from which it will be executed. In particular, if you’re using a CI/CD pipeline, you will need to make sure that the script can find the files it needs. In my case, I had to explicitly specify the path to the BibTeX file, as well as the path to the output file. This is because the script is executed from the root of the repository, but the BibTeX file is in the <code>papers</code> directory, and the output file is in the <code>papers</code> directory, too. Testing locally for deployment in CI/CD is notoriously hellish, but with a little bit of elbow grease,<sup>2</sup> you can get it to work.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;And by ‘a little bit’, I mean absolute tons of it.</p></div></div><p>One caveat: rendering with pre- and post-render project scripts may be relatively expensive in terms of time. My silly SkiErg world record script adds a good 20-25 seconds of processing time to each rendering run – not a lot, but it does stack up eventually. The <a href="https://quarto.org/docs/projects/scripts.html">documentation</a> helpfully discloses that Quarto provides an environment variable, <code>QUARTO_PROJECT_RENDER_ALL</code>, which is set to <code>1</code> if it’s a full render. It may make sense to do a more manual caching here.</p>
</section>
<section id="things-you-can-do-with-project-scripts" class="level1">
<h1>Things you can do with project scripts</h1>
<p>Seriously, the possibilities of what you can do with project scripts are pretty much endless. I have a project script that <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_check_skierg_records.py">checks the world records list on the official SkiErg website and makes sure my index page lists my accurate records</a>. This is completely silly, but it’s also a great example of how you can use project scripts to automate just about everything:</p>
<ul>
<li><a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_doi_from_rogue_scholar.py">A script I built</a> pulls DOIs minted by <a href="https://rogue-scholar.org">Rogue Scholar</a> and appends them to posts.</li>
<li>A lot of projects use it for housekeeping: <a href="https://github.com/CSCAR/workshop-stata-intro/blob/main/Makefile">via makefiles</a></li>
<li>The <a href="https://github.com/bambinos/bambi">bambi</a> project uses it to <a href="https://github.com/bambinos/bambi/blob/main/docs/pre_render.py">build a changelog</a>, which is a great idea for applications that do need it</li>
<li>This one cool master’s thesis uses pre-render project scripts to <a href="https://github.com/danielvartan/mastersthesis/blob/bebb699af7cab7add37a33768ccd605a60a18ba8/R/quarto-pre-render-html.R#L82">load variables from a YAML file into envvars</a>, and I’ve <a href="https://github.com/computorg/computo-quarto-extension/blob/main/prerender.R">seen the same done with Rprofile files</a> – obviously, these are much more pertinent to projects where the bulk of the project consists of notebooks/code execution rather than websites, but again, the idea is the same</li>
<li>Obviously, I’m not the only one to use it for <a href="https://github.com/nfrerebeau/rarcheo/blob/master/_references.R">reference management</a></li>
<li>You can use it to <a href="https://github.com/lnnrtwttkhn/version-control-book/blob/main/Makefile">download static content into your project</a>, too!</li>
</ul>
<p>The sky’s the limit! I’m currently working on rendering my CV from a JSON file via a project script, as well as a few other cool things. I keep my project scripts <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/tree/main/scripts">here</a> – I hope they will inspire you to do something cool with your own project scripts!</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Quarto Project Scripts Are Awesomeness},
  date = {2023-10-22},
  url = {https://chrisvoncsefalvay.com/posts/quarto-project-scripts/},
  doi = {10.59350/b4qyq-70w75},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Quarto Project Scripts Are
Awesomeness.”</span> <a href="https://doi.org/10.59350/b4qyq-70w75">https://doi.org/10.59350/b4qyq-70w75</a>.
</div></div></section></div> ]]></description>
  <category>Quarto</category>
  <category>Python</category>
  <guid>https://chrisvoncsefalvay.com/posts/quarto-project-scripts/</guid>
  <pubDate>Sun, 22 Oct 2023 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
