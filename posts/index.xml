<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Chris von Csefalvay</title>
<link>https://chrisvoncsefalvay.com/posts/</link>
<atom:link href="https://chrisvoncsefalvay.com/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Chris von Csefalvay is a computational epidemiologist/data scientist working at the intersection of AI, epidemiology and public health.</description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Sat, 25 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Deep kimchi</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/deep-kimchi/</link>
  <description><![CDATA[ 




<p>This is about the time of year when I get the sudden and irresistible urge to make kimchi. Kimchi – and fermentation in general – is one of those things I wonder if we’re somewhat genetically predisposed to figure out as a species,<sup>1</sup> considering most major cultures in the right climates have independently discovered what is a fairly cool optimisation – finding something that converts sugar into lactic acid, but which is also insanely halotolerant (capable of enduring high salt concentrations) and therefore can create a little evolutionary niche in one’s own fridge muscling out other possibly pathogenic microorganisms.<sup>2</sup> Anyway, about a ton of napa cabbage in, waiting for my immunoassays to finish (because in this household, we do science properly, so we do fluorescence immunoassays for pathogens, because botulism is not cool), my brain wandered off to agentic AI.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Beyond, of course, the fact that we have glutamate-sensitive receptors in our taste buds that let us taste, and enjoy, umami. <a href="https://youtube.com/mrnigelng">Uncle Roger</a> does have a point.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;If you do it correctly. Fermentation is not for the faint of heart. There are real risks, up to and including getting very dead from botulism.</p></div></div><p>The thing is, I’ve been somewhat concerned about the way the agentic AI discourse has been going. We’re headed for deep kimchi, and I’m afraid the main reason for that is, ironically enough, that most agentic AI out there, well, isn’t.</p>
<section id="semantic-mishandles" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="semantic-mishandles">Semantic (mis)handles</h2>
<p>Part of the problem is, of course, the way we think about agents – and insofar as I <a href="../../posts/team-of-rivals/index.html">might be among those to blame for giving rise to the term in its modern sense</a>,<sup>3</sup> I guess I’ll have to shoulder some responsibility for the confusion it engenders. For if one conceives of it in the way we use the word in colloquial language, an agent is, of course, typically a person. This has led to the metaphor of agents as tiny people in your computer who do things to be taken rather more seriously. And with that, we started resorting to conventional operating models we use with people: org charts, arrows of delegated responsibility and all that.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Can’t take the credit without also taking the blame – though I’d rather neither. As I point out every time, the agentic idea has been around much longer. While my description of agentic AI was among the first in its modern sense, I don’t think it was <em>the</em> first. In a life lived with extremely few regrets, my use of the ‘a-word’ is one of the things I wish I could undo. It has so far only caused semantic confusion <em>and</em>, quite possibly, real human harm.</p></div></div><p>Nobody makes kimchi by instructing individual lactobacilli. You cannot sit there with a tiny megaphone directing bacterial populations. For millennia, civilisations that came up with their lactobacillus fermented foods have figured out how to do it instead: you salt the cabbage, add spices, control temperature and let the rest take care of itself.</p>
<p>We understand this intuitively for fermentation. For cities. For ecosystems. For markets. We know that complex systems require systems-level thinking, not component-level micromanagement. And yet somehow, when we talk about agentic AI, we abandon this wisdom entirely and write things like “the challenge of orchestrating thousands of agents” on LinkedIn.</p>
<p>The language of “agents” has misled us through anthropomorphism. We hear “agent” and think of something human – and therefore, rightly, something focal, unique, irreplaceable: an employee, a character in a story, the protagonist of our narrative. We instinctively think about agents the way we think about people – as discrete, meaningful entities whose individual actions matter.</p>
<p>But agents in mature agentic systems aren’t people. They’re processes on some server in Ashburn or Ohio or wherever your cloud provider lives. They neither require nor warrant the kind of individuated respect we afford to humans <em>qua</em> humans.</p>
</section>
<section id="the-domain-of-control" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-domain-of-control">The domain of control</h2>
<p>For two years now, we’ve defined agentic AI in terms of what agents are or can do. These are the mantras you hear repeated in agentic discourse <em>ad infinitum</em>: autonomy, goal-directedness, tool use. All agent-level properties, all true (sort of), and yet they describe at best what kind of thing can be an agent – not what agentic AI is, or can be.</p>
<p>If you can sketch your “agentic AI system” with a few boxes and arrows on a whiteboard, what you’ve built is glorified RPA with LLMs. You’ve automated some tasks, chained some API calls, added probabilistic text generation – useful work, but no more agentic AI than a handful of manually-tuned perceptrons is deep learning. While this is a good way to illustrate what agentic AI might look like zoomed in, it is little more than a metaphor.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>We can formalise an agentic AI system as a dynamic directed graph <img src="https://latex.codecogs.com/png.latex?G_F(t)%20=%20(V(t),%20E(t))"> associated with a fitness function <img src="https://latex.codecogs.com/png.latex?f:%20G(t,%20F)%20%5Crightarrow%20%5Cmathbb%7BR%7D">, where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?V"> is the set of agents (vertices);</li>
<li><img src="https://latex.codecogs.com/png.latex?E"> is the set of directed edges (agent interactions); and</li>
<li><img src="https://latex.codecogs.com/png.latex?F"> characterises the fitness landscape constraints, i.e.&nbsp;the functions and constraints that define the environment.</li>
</ul>
<p>The system evolves to maximise <img src="https://latex.codecogs.com/png.latex?f"> subject to constraints imposed by <img src="https://latex.codecogs.com/png.latex?F"> (which may associate various costs with various conformations of <img src="https://latex.codecogs.com/png.latex?G_F">). The structure <img src="https://latex.codecogs.com/png.latex?G_F(t)"> – both topology and population – emerges from this optimisation process rather than being specified a priori.</p>
</div></div><p>The key properties of agentic AI lie in the system, not the agents themselves. If we are to attempt a definition, an agentic AI system is best defined by what I call the 3+2 model – the three key autonomies and two key constraints.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Connective autonomy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Agents can discover and form connections with other agents dynamically. The graph of agent interactions is not designed, but evolves and changes to adapt the system’s goal.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Task and tool autonomy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Agents can access and utilise tools and resources based on need rather than pre-specification. They explore their operational possibility space rather than following predetermined paths.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Genetic autonomy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Genetic autonomy: Agents can spawn new agents and determine their characteristics, adapting to selection pressure.</p>
</div>
</div>
<p>The reason why we need to define these autonomies is because they in turn reveal what our controls are – the constraints we can impose to shape system behaviour.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Goal constraint
</div>
</div>
<div class="callout-body-container callout-body">
<p>The goal constraint is the fitness landscape that defines success – not a detailed specification of <em>how</em> to achieve the system level goal but a definition of what constitutes achievement.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Environment constraint
</div>
</div>
<div class="callout-body-container callout-body">
<p>The environment constraint encompasses resource limitations, communication costs, metabolic expenses and interaction protocols that create, communicate and moderate selection pressure on agent behaviours.</p>
</div>
</div>
<p>Together, these five elements - three autonomies and two constraints - define an agentic AI system. And here’s the crucial insight: we should leave the autonomies alone and focus our design attention on the constraints.</p>
<p>The anthropomorphic language of “agents” obscures this. When we think of agents as workers or colleagues, we naturally want to define their roles, specify their responsibilities, coordinate their interactions. We want to draw the org chart. But bacteria don’t have org charts. Neurons don’t have job descriptions. And mature agentic systems won’t have workflow diagrams.</p>
</section>
<section id="where-current-thinking-fails" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="where-current-thinking-fails">Where current thinking fails</h2>
<p>The current wave of “agentic AI” largely looks like an LLM plus some hand-wired agents plus an orchestration layer that routes tasks between them. It’s deterministic workflow automation with probabilistic text generation at the nodes. This is the worst of both worlds – the LLM’s stochasticity breaks the determinism of traditional RPA, but the system gains none of the adaptive benefits that would justify that unpredictability.</p>
<p>In systems-level terms: you’ve eliminated connective autonomy by hard-coding <img src="https://latex.codecogs.com/png.latex?E(t)">, genetic autonomy by fixing <img src="https://latex.codecogs.com/png.latex?V(t)">, and effectively reduced the ability of the system to optimise for any goal-derived fitness function zero.</p>
<p>This is, of course, by no means novel. When Marvin Minsky and Seymour Papert published <em>Perceptrons</em> in 1969, claiming XOR was uncomputable by single-layer networks,<sup>4</sup> just about everything slowed to a crawl until the field was revived in 1986 by the work of Rumelhart, Hinton and Williams on backpropagation. What is crucial is that Hinton, LeCun and the rest of the backpropagandists didn’t go out to make better neurons. They simply realised how much more can be gotten out of simple neurons, simple maths and a systems perspective. They optimised the system, not the components. And therein lies the magic.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Which was true but irrelevant. We’ve known MLPs quite well at the time. Yet it was widely misinterpreted and by the time anyone noticed, we were deep into the winter of neural network research.</p></div></div></section>
<section id="the-power-of-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-power-of-constraints">The power of constraints</h2>
<p>What would it mean to design agentic AI systems through constraints rather than specifications?</p>
<div class="grid">
<div class="g-col-6">
<p><strong>Goal constraint design</strong></p>
<p>We specify system-level objectives, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?f">, for <img src="https://latex.codecogs.com/png.latex?G_F(t)">, to optimise against. How the system accomplishes this is left to the autnonomies.</p>
</div>
<div class="g-col-6">
<p><strong>Environment constraint design</strong></p>
<p>We condition <img src="https://latex.codecogs.com/png.latex?G_F"> by the fitness landscape <img src="https://latex.codecogs.com/png.latex?F"> by defining resource limitations, communication costs, metabolic expenses and interaction protocols that create selection pressure on agent behaviours.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Recipe:</strong> Napa cabbage kimchi <em>(vaguely Noma-inspired)</em></p>
<ul>
<li>1kg napa cabbage, cut into bite-sized pieces</li>
<li>60g sea salt</li>
<li>2 tbsp nước chấm</li>
<li>2 tbsp gochugaru</li>
<li>3 cloves garlic, minced</li>
<li>20g fresh ginger, grated</li>
<li>2 spring onions, sliced</li>
<li>1 tsp raw honey</li>
</ul>
<p>Salt the cabbage thoroughly. Let rest 2-3 hours. Rinse and squeeze dry. Mix remaining ingredients into a paste (wear gloves – nước chấm will make your hands stink to high heaven), massage into cabbage. Pack tightly into jar, leaving 2-3 cm headspace, and ferment at room temperature for 3-5 days, releasing pressure daily. Refrigerate once it reaches your desired tanginess.</p>
</div></div><p>Jointly, these constraints create selection pressure that guides evolution without dictating outcomes. Consider what this enables: A system facing a new class of problems might spawn specialist agents efficient for that problem type. These specialists persist if they provide value, disappear if they don’t. Agents discover collaboration patterns that prove fruitful and strengthen those connections, and the system evolves toward metabolic efficiency without any explicit optimisation.</p>
<p>This is the power of evolutionary speciation over specification: species emerge from evolutionary pressure, rather than being designed explicitly, as does their conduct and interactions. The ‘agents’, just like the bacterial species in a jar of kimchi, explore the solution space defined by the constraints, and the population structure adapts not to what someone defines but to what the environment at the time demands.</p>
</section>
<section id="how-to-rescue-agentic-ai" class="level2">
<h2 class="anchored" data-anchor-id="how-to-rescue-agentic-ai">How to rescue agentic AI</h2>
<p>We’re in deep kimchi – but we don’t have to be. AI had to endure a decade-and-half winter before being resuscitated in 1986. We may be fortunate enough to avoid it this time around – but we won’t get there by better workflow diagrams. We’ll get there through the system perspective, just as it was the system perspective that saved neural networks.</p>
<p>The autonomies – connective, task, genetic – define what mature agentic systems are capable of. But realising that capability requires shifting our design attention from the autonomies to the constraints, and learn how to stop micromanaging and instead govern the system through conditioning its fitness landscape (in short: <a href="../../posts/biome-engineering/index.html">biome engineering</a>).</p>
<p>If you’re still drawing boxes and arrows, still thinking about “my code agent talking to my research agent”, still trying to orchestrate individual interactions – you’re hand-coding what should be left to evolution. The anthropomorphic language of “agents” has been useful for explaining these systems to non-technical audiences, but we have outgrown it – it is now actively hindering our ability to design them properly. Agents aren’t little employees inside the machine or characters of a story. But their collectivities can give rise to the same kind of emergent behaviour that lets a stack of trivial computations self-organise into ChatGPT, AlphaFold or whatever fresh hell Sora is doing right now.</p>
<p>Agentic AI isn’t doomed – but micromanagement of agents doesn’t scale, and that imposes an upper bound. We have kimchi and sauerkraut and pickles because the civilisations that came up with them were unencumbered by software architects and Microsoft Visio. They just designed the brine, set the temperature and let nature take its course. We should probably learn from that.</p>
<p>Stop trying to direct the bacteria. Start designing the brine.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Deep Kimchi},
  date = {2025-10-25},
  url = {https://chrisvoncsefalvay.com/posts/deep-kimchi/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Deep Kimchi.”</span> October 25, 2025.
<a href="https://chrisvoncsefalvay.com/posts/deep-kimchi/">https://chrisvoncsefalvay.com/posts/deep-kimchi/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agentic AI</category>
  <category>evolution</category>
  <guid>https://chrisvoncsefalvay.com/posts/deep-kimchi/</guid>
  <pubDate>Sat, 25 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>SOYA - the only benchmark that matters</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/soya/</link>
  <description><![CDATA[ 




<p>In 1790, the French Academy of Sciences commissioned a rather ambitious survey. The goal was to measure the distance from the North Pole to the Equator along the meridian passing through Paris, then use that measurement to define a new universal unit of length: the metre. The idea was simple enough: let’s create a standard so objective, so rooted in natural law, that every nation would adopt it. One ten-millionth of the distance from pole to equator. Perfect. Universal. The platonic ideal of measurement.</p>
<p>There was just one problem. The measurement was wrong. Not catastrophically wrong, mind you, but wrong enough that when better instruments came along, we discovered the original metre was about 0.2mm off. By then, however, the French had made rather a lot of metre sticks, and redoing them all seemed like rather more trouble than it was worth.<sup>1</sup> So we kept the stick and quietly forgot about the pole-to-equator business. The metre became defined not by nature’s grand design, but by a specific physical artefact in a vault in Sevres. Later, we’d define it by the speed of light, which at least has the virtue of being constant, even if it’s rather less poetic than the original vision.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Also, this was basically on the heels of the French Revolution. You can disagree about weights and measures, but you’re much less likely to want to do so vis-a-vis a government that has just discovered its love of the guillotine.</p></div></div><p>I’m reminded of the story of the metre because it nicely illustrates a key point about benchmarks: all benchmarks are wrong. They are simplifications, abstractions, approximations of reality. They can be useful, but they can never capture the full complexity of the systems they aim to measure. And that’s okay if all you need to resolve is the coordination problem of “how long is this thing?” – but it doesn’t say anything beyond the relative ratio between what you’re looking at and that specific standard. Least of all does it reveal anything meaningful about the underlying object.</p>
<section id="the-benchmark-industrial-complex" class="level2">
<h2 class="anchored" data-anchor-id="the-benchmark-industrial-complex">The benchmark-industrial complex</h2>
<p>We have built an entire industry around the idea that there exists some universal measure of language model capability, some objective standard against which all models can be compared. MMLU, GSM8K, HumanEval, HellaSwag – the list grows longer every month, each benchmark claiming to capture some essential truth about model performance. Companies trumpet their SOTA results. Researchers optimise specifically for these benchmarks. VCs make investment decisions based on leaderboard positions. And on and on it goes in a self-reinforcing cycle.</p>
<p>And just like the original metre, these benchmarks are increasingly recognised as both arbitrary and, well, wrong.</p>
<p>The rot has been apparent for a while now. Traditional static benchmarks suffer from saturation, as models quickly reach performance ceilings, and contamination, where test data leaks into training sets, inflating scores. When GPT-4 can score 86.4% on MMLU, and the next model scores 87.2%, are we measuring genuine improvement or noise? When models are trained on datasets that may contain variations of the test questions, are we measuring capability or memorisation?</p>
<p>There’s a deeper problem here, though. LLMs can be used for a shocking range of tasks, from generating code to clicking the right button on your GUI. Benchmarks necessarily have a value judgment inherent in their task set – and that includes massive multi-task sets like MMLU or agentic multi-objective evals like GAIA or <img src="https://latex.codecogs.com/png.latex?%5Ctau">-bench. No choice, too, is a kind of choice: when we use a benchmark smorgasboard of tasks or domain questions, we are implicitly setting the expectation of a Renaissance model that can do everything somewhat well. Simply put – there’s no such thing as an agnostic, universal eval.</p>
</section>
<section id="soya-your-benchmark-your-way" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="soya-your-benchmark-your-way">SOYA: your benchmark, your way</h2>
<p>Earlier this year, Huggingface released YourBench, a framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date and domain-tailored benchmarks directly from user-provided documents.<sup>2</sup> There’s a beautiful symmetry here – just as we’re reaching the level of specialisation of language models that necessitates task-specific evals, we also are starting to have the tools that can provide this on a budget and at scale.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Shashidhar, S., Fourrier, C., Lozovskia, A., Wolf, T., Tur, G., &amp; Hakkani-Tür, D. (2025). <a href="https://arxiv.org/abs/2504.01833">YourBench: Easy custom evaluation sets for everyone.</a> <em>arXiv preprint</em>, arXiv:2504.01833.</p></div></div><p>The real significance of YourBench isn’t just that it’s incredibly convenient and technically impressive. It is the end of SOTA, and the rise of what I call SOYA: the State of Your Art.</p>
<p>The insight is deceptively simple. Instead of asking “which model is best?”, we should be asking “which model is best at the specific things I actually need it to do, with the specific data and constraints I actually have?” The universal benchmark is revealed as the emperor with no clothes. What matters isn’t whether Claude beats GPT-5 on MMLU – it’s whether the model can handle your internal documentation, understand your domain terminology, and operate within your latency and cost constraints.</p>
<p>This shift from SOTA to SOYA isn’t just semantic cleverness. It’s a fundamental reimagining of how we think about model selection and evaluation. Tools like YourBench have transformed custom evals from a luxury reserved to major labs to something you can run for the price of a Happy Meal.</p>

<div class="no-row-height column-margin column-container"><div class="">
<hr>
<p><strong>Recipe:</strong> Shakshuka</p>
<ul>
<li>1 tin decent chopped tomatoes</li>
<li>1 onion, diced</li>
<li>3 cloves garlic</li>
<li>1 red pepper, diced</li>
<li>1 tsp smoked paprika</li>
<li>1 tsp cumin</li>
<li>1/2 tsp cayenne pepper</li>
<li>4-6 eggs</li>
<li>Crumbled feta</li>
<li>Some fresh coriander, to taste</li>
</ul>
<p>Sauté the onion until soft, add the garlic and spices. Adding the tomatoes and peppers, simmer until thick (20 minutes). Make wells, crack eggs into them. Cover and cook until eggs are just set. Top with feta and coriander. Serve with good bread.</p>
<hr>
</div></div><section id="the-democratisation-of-evals" class="level3">
<h3 class="anchored" data-anchor-id="the-democratisation-of-evals">The democratisation of evals</h3>
<p>The key implication, then, is that it is now at least in theory open to everyone to determine what their State of the Art is. This opens the door to much more meaningful evals. In the regulated pharmaceuticals and medtech industries, where I spend pretty much all my working life, a 0.3% incremental improvement in model performance is less relevant than what that 0.3% actually <em>is</em>. There’s an incommensurability of performance aspects here. I don’t care how much better your model is at solving Math Olympiad questions if it can’t determine whether something is a life-threatening adverse event or a mere nuisance. Generic benchmarks don’t help me. SOYA benchmarks might.</p>
<p>The offshoot, of course, is that these evals map much better to actual business needs and actual data. The downside? They require an understanding of how to build a good eval. YourBench is brilliant, but it’s a tool for building evals, not for building <em>good</em> evals per se. It puts evals that were previously the preserve of well-funded labs into the hands of anyone with a credit card and a bit of time. But it’s up to the end user to make sure this doesn’t turn into giving toddlers a set of car keys and a bottle of bourbon.</p>
</section>
</section>
<section id="the-soya-mindset-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="the-soya-mindset-in-practice">The SOYA mindset in practice</h2>
<p>SOYA, then, is really primarily a mindset – one that requires us to first and foremost let go of some comfortable illusions. It means accepting that there is no “best model” in the abstract, only models that are better or worse for specific purposes. It means doing the hard work of articulating what you actually need from a model, rather than defaulting to whatever topped the latest leaderboard. When I talk to my clients about building an approach to evals, I typically want to explore the dimensions of model use – that is, their ‘definition of good’:</p>
<ol type="1">
<li>What are the three most common tasks this model will perform?</li>
<li>What does failure look like for each of these tasks, and what are the consequences?</li>
<li>What does your actual data look like, and how does it differ from the training distributions these models saw?</li>
<li>What are your constraints on latency, cost and compute?</li>
<li>What does “good enough” look like for your use case?</li>
</ol>
<p>Only after we’ve answered these questions do we start looking at models. And increasingly, the answer isn’t “use the SOTA model” but rather “use this smaller, specialised model that excels at your specific task, or this model that’s good enough but 10x cheaper, or this ensemble of models that handles your specific data distribution better”.</p>
</section>
<section id="some-inconvenient-truths" class="level2">
<h2 class="anchored" data-anchor-id="some-inconvenient-truths">Some inconvenient truths</h2>
<p>Let me be clear about what SOYA doesn’t mean. It doesn’t mean anything goes. It doesn’t mean evaluation is purely subjective. It doesn’t mean we abandon rigour. What it does mean, however, is acknowledging some uncomfortable truths:</p>
<ol type="1">
<li>Generic benchmarks capture something, but that something may not correlate with your specific needs. The more specialised those needs are (i.e.&nbsp;the further they are from simple agent driving and chat interactions), the less likely generic benchmarks are to be relevant.</li>
<li>The “best” model for one use case may be catastrophically wrong for another. Context matters.</li>
<li>Optimising for SOTA performance often means whatever you’re getting has been optimised for something other than your use case.</li>
<li>Custom evaluation requires thought and effort, but that effort is increasingly cheap enough to be worth it. Thought, on the other hand, remains expensive. Bad evals give bad results.</li>
</ol>
<p>And that’s really the crux of it all: the choice for users is between accepting convenient, universal, cheap and wrong benchmarks, or investing a bit more time and effort into building evals that actually reflect their needs.</p>
</section>
<section id="how-not-to-suck" class="level2">
<h2 class="anchored" data-anchor-id="how-not-to-suck">How not to suck</h2>
<p>SOYA, if correctly used, can be a solution to the problem of generic evals that suffer from the same flaw as generic models: they try to be everything to everyone, and end up being mediocre at best for any specific purpose. But SOYA can also be misused. A poorly constructed custom eval can be worse than a generic benchmark, giving a false sense of security or leading to misguided model choices. And at this point, eval engineering as a discipline is sorely lacking. Even relatively sophisticated enterprise users have few specialists who really understand how to build good evals.</p>
<p>One solution for this is the emergence of evals-as-a-service (EaaS) providers. But evals aren’t a technological exercise only – they require an understanding of the factors I mentioned above that characterise what success, or a good model, is for the particular client. SOYA is the Savile Row of AI: bespoke, tailored, and requiring expert craftsmanship. You can’t just pick it off the rack.</p>
<p>The benchmark-industrial complex will be fine – there’s already talk about making models more ‘realistic’. This is generally a category error – benchmarks cannot be ‘realistic’ to all respects. What they can be is relevant. And relevance is in the eye of the beholder. No single benchmark can capture the specific requirements of pharmaceutical adverse event extraction, contract analysis and marketing copywriting simultaneously. The solution is to accept that benchmarks must be premised on ‘what good is’, not on a fool’s errand of bundling an ever growing list of tasks into a single eval suite.</p>
</section>
<section id="epilogue" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="epilogue">Epilogue</h2>
<p>A year or so after graduating from Oxford, I was invited to sit for what was rather widely considered the time’s equivalent of Humanity’s Last Exam, but for humans: the Prize Fellowship Exam for All Souls. There isn’t enough space here to describe how weird and intense an experience it was. You sit a number of papers, typically two ‘general’ papers, two ‘specialist’ papers and an essay. The general papers have questions on just about everything. Here are three actual questions from <a href="https://www.asc.ox.ac.uk/past-examination-papers">this year’s general paper</a>:</p>
<ul>
<li>Invent a new punctuation mark!</li>
<li>Does a pope matter?</li>
<li>The organ has been considered the ‘king of instruments’. Is it?</li>
</ul>
<p>Then you get to choose your specialist papers – from seven disciplines (classics, economics, English literature, history, law, philosophy and politics). I picked a law paper, which was unsurprising considering I did that as an undergraduate subject, and a classics paper.<sup>3</sup> I guess I must have done pretty okay, because of the hundred or so applicants that year (you had to generally get a top 1st in your undergraduate degree to even apply), I was fortunate enough to be in the final five invited for a <em>viva</em>, the last stage of the process. Which I bombed spectacularly enough that I wasn’t offered a fellowship, but that’s a story for another day.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;My sincere apologies to the examiners for having to endure my Latin translation. I am not a classicist by training to begin with, but I am a special kind of bad at Latin in particular.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;It took a lot of time and growth for me to learn to appreciate the value of depth. I remain incredibly curious and, in long retrospect, grateful for the experience – but also very aware that my depth is what makes my breadth work. Not winning a Prize Fellowship might have been one of my career-defining blessings in disguise – something that forced me to find a productive synthesis between a mind interested in just about everything and the needs of this world for professionals who can deliver focus, profound expertise and real-world impact. I am just so incredibly fortunate to have ultimately found a way to make this difficult balance work for me.</p></div></div><p>I mention this because with the benefit of hindsight, I see a lot of similarities between the Prize and the current SOTA mindset, mostly in its shortcomings. The Prize Fellowship Exam is the epitome of the modern ‘HLE-style’ SOTA eval: it identifies a small number of dazzling generalists, not necessarily those people who truly end up changing the world through commitment, depth and focus on their specific domain.<sup>4</sup> But whatever we think of the value of that kind of modern day Renaissance person, it certainly isn’t what we need from our AI agents. What we need are models that excel in specific domains, for specific tasks, under specific constraints. Agents, unlike people, are interchangeable. We don’t need polymaths – we need specialist team players who can harness emergence for complexity.</p>
<p>In the end, context isn’t an epiphenomenon – it’s what gives meaning to the abstractions of performance. Context is what turns that abstract brilliance into concrete, real-world impact. And with tools like YourBench, we’re finally seeing the era of SOYA – where users finally get the choice they deserve as to what matters to them, and what good means for their use case.</p>
<p>So the next time someone breathlessly announces they’ve achieved new SOTA results, ask yourself: state of what art? For whom? Under what conditions?</p>
<p>Because in the end, <strong>the only art that matters is yours</strong>.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {SOYA - the Only Benchmark That Matters},
  date = {2025-10-19},
  url = {https://chrisvoncsefalvay.com/posts/soya/},
  doi = {10.59350/xqf22-csp59},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“SOYA - the Only Benchmark That
Matters.”</span> <a href="https://doi.org/10.59350/xqf22-csp59">https://doi.org/10.59350/xqf22-csp59</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>evals</category>
  <guid>https://chrisvoncsefalvay.com/posts/soya/</guid>
  <pubDate>Sun, 19 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Biome engineering</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/biome-engineering/</link>
  <description><![CDATA[ 




<p>Roughly 541 million years ago, something extraordinary happened in Earth’s oceans. Over a geologically brief period of perhaps 20 million years, the fossil record explodes with an almost obscene diversity of body plans. Trilobites, anomalocarids, the bizarre Opabinia with its five eyes and frontal grasping appendage – suddenly, the relatively monotonous microbial world gave way to an abundance of diversification in what we now call the Cambrian explosion. The textbook explanation points to rising oxygen levels in the oceans, changes in ocean chemistry and perhaps the evolution of predation itself. But here’s the thing: none of these environmental shifts determined which specific forms would emerge. The changing ocean chemistry didn’t decree “there shall be trilobites”. Rather, it created conditions that made certain evolutionary pathways more energetically favourable, certain body plans more viable, certain ecological niches newly accessible.</p>
<p>The environment set the stage. Emergence explored it, and wrote the play.</p>
<p>I find myself returning to this image as I think of reconciling two ideas in apparent opposition: the fact that AI agents’ true value lies in their ability to independently reorganise and structure themselves in a way that gives rise to an emergence of sorts, with the need to govern such systems and guide them towards the outcomes we desire.</p>
<p>We’ve moved through distinct phases of how we think about shaping AI behaviour, each building on the last like Schliemann’s nine cities of Troy. First came <a href="../../posts/prompt-engineering/index.html">prompt engineering</a>, the art of crafting the perfect utterance to coax desired behaviour from a language model. Then Andrej Karpathy articulated what he termed <a href="../../posts/context-engineering/index.html">context engineering</a> – the recognition that an agent’s operating envelope, the information and tools it has access to, matters as much as any individual prompt. But as I argued in my recent writing on agent ecosystems, we’re now confronting something more profound: the need to engineer not individual agents or even their immediate contexts, but the entire operating environment in which populations of agents interact, specialise and evolve solutions we couldn’t have explicitly programmed.</p>
<p>I call this biome engineering. And like the Cambrian ocean chemistry, it’s an intricate dance of constraints and gradients that gently shape, not prescribe, a spontaneous emergence.</p>
<section id="from-utterances-to-ecosystems" class="level2">
<h2 class="anchored" data-anchor-id="from-utterances-to-ecosystems">From utterances to ecosystems</h2>
<p>Let me trace the genealogy of this idea for a moment. Prompt engineering emerged from a fundamentally transactional view of AI: you ask a question, the model responds, the interaction terminates. The craft lay in asking the right question in the right way. It was, in essence, the art of the perfect utterance – a rhetorical exercise that would have delighted the Sophists. We developed elaborate techniques: few-shot learning, chain-of-thought prompting, role-playing scenarios. The assumption was that if we could just find the right incantation, we could reliably extract the behaviour we wanted from the model.</p>
<p>This worked reasonably well for a while, but it had a fundamental limitation: it treated each interaction as isolated, acontextual, amnesic. Every conversation started from scratch. Every task required re-establishing context.</p>
<p>Context engineering represented a conceptual leap. Karpathy’s insight was that we should stop thinking about individual prompts and start thinking about the entire informational environment in which an agent operates. What knowledge does it have access to? What tools can it invoke? What constraints govern its actions? More broadly, how can we ‘dope’ the agent’s context with just the right nudging context to create a groove in the gradient space towards what outcome we desired, without explicitly constraining it? An agent with proper context engineering isn’t just responding to utterances – it’s operating within a defined possibility space, with persistent memory, access to resources and the ability to maintain coherent behaviour across extended interactions. The tradeoff is, of course, that if finding the right utterance was difficult, finding the right context is even more so – and the reward for perfect context engineering is even less deterministic.</p>
<p>But context engineering, for all its power, still focuses on the individual agent. It’s about optimising the envelope within which a single agent operates. And this is where we run into <a href="../../posts/agents-agora/index.html">the same conceptual wall I’ve been banging on about in my writing on agent ecosystems</a>: the real value of agents isn’t in what they can do alone, but in how they work together.</p>
</section>
<section id="leveling-up" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="leveling-up">Leveling up</h2>
<p>Here’s where we need to make the leap from engineering individual contexts to engineering entire biomes – the substrates in which populations of agents can interact, specialise and collectively solve problems in ways that emerge from the system rather than being explicitly programmed.</p>
<p>A biome, in the ecological sense, isn’t just a collection of organisms. It’s the entire system: the organisms, their interactions, the physical environment, the flows of energy and nutrients, the constraints and opportunities that shape what can survive and thrive. The Serengeti isn’t just lions and gazelles and acacia trees – it’s the pattern of rainfall that determines where grass grows, the soil chemistry that shapes which plants can take root, the seasonal migrations that move energy across the landscape.</p>
<p>Biome engineering for AI systems means designing the operating environment for populations of agents: not just what each individual agent can do, but how agents can discover each other, how they establish trust and negotiate collaboration, what resources they can access and under what constraints, how authority and delegation flow through the system. It’s about creating the substrate conditions that allow certain interaction patterns to flourish whilst making others energetically unfavourable or outright impossible.</p>
<p>The crucial insight is that biome engineering is non-deterministic in its outcomes but bounded in its possibilities. Just as the Cambrian ocean chemistry didn’t produce a specific catalogue of species but rather made certain evolutionary pathways more likely, biome engineering doesn’t specify exactly how agents will collaborate – it creates conditions that favour productive patterns whilst constraining destructive ones.</p>
<p>More importantly, this allows for agents to dynamically recombine within a bounded space of possibilities. This makes the collective of agents responsive to adapt to changing conditions, i.e.&nbsp;to optimise the graph of interactions <img src="https://latex.codecogs.com/png.latex?G%20=%20(V,%20E)"> so that its performance minimises a loss function over time. And if this sounds awfully similar to the way we train every neural network, that’s because it is. Biome engineering is awfully important because it essentially supersedes the legacy approach of deterministically layering a small number of agents. That may work for the travel-planning-in-five-agents toy example, but not for emergent systems. Just as we wouldn’t have working neural networks if we had to manually run backpropagation, we need an effective way to let agents self-organise and learn towards a goal at scale. Biome engineering turns that problem into a tractable one.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;And, arguably, differentiable. Which is where the real magic comes in, since whatever is differentiable can be computationally optimised using gradient descent. But that’s a post for another day.</p></div></div></section>
<section id="the-biome-as-substrate" class="level2">
<h2 class="anchored" data-anchor-id="the-biome-as-substrate">The biome as substrate</h2>
<p>There’s a concept in exercise physiology called “training stress” – not stress in the colloquial sense of anxiety, but the physiological stress that drives adaptation. You don’t tell your body to build more mitochondria or increase stroke volume. You create conditions (progressive overload, adequate recovery, proper nutrition) that make those adaptations energetically favourable. The body responds to the environment you create. Do it long enough, and you <a href="../../posts/skierg-marathon/index.html">might as well end up doing things you probably didn’t think was possible</a>.</p>
<p>Biome engineering works similarly. You’re not explicitly programming agent behaviours – you’re shaping the problem space they explore by setting its boundaries and its internal gradients. This means thinking carefully about a lot of things we generally don’t (or at least not in these terms):</p>
<ul>
<li>Resource topology: What information, tools and compute resources are available? How are they distributed? What are the costs (in tokens, time, money) of accessing them? Just as species diversity in ecology is partly shaped by resource patchiness, agent specialisation will emerge based on how resources are structured.</li>
<li>Interaction protocols: How can agents discover each other? How do they establish capabilities and negotiate terms? What standards govern their communication? This is a problem I’ve written about – we need something richer than current constructs (I like MCP, it’s just limited to what it has been designed to accomplish), something that can convey not just data but trust, authority and constraint.</li>
<li>Constraint boundaries: What are agents allowed to do, and crucially, how do those constraints propagate through delegation chains? An agent commissioned by another agent must inherit appropriate constraints from its principal. The biome’s “laws of physics” need to make certain behaviours impossible, not just discouraged.</li>
<li>Feedback mechanisms: How do agents learn what works? Do successful collaboration patterns get reinforced? Do agents that consistently meet their commitments earn reputation that makes future collaboration easier? The biome needs something analogous to ecological fitness – ways for productive patterns to flourish. *Diversity pressures: Monocultures are fragile, whether in agriculture or in agent systems. The biome should create niches that favour specialisation over generalisation, depth over breadth in specific domains.</li>
</ul>
<p>The art of biome engineering lies in setting these parameters such that the agents’ exploration of the problem space naturally tends toward solutions that are acceptable to you, while avoiding failure modes that aren’t. You’re creating a landscape where certain paths are easier to traverse, certain peaks easier to reach, without explicitly commanding “go climb this mountain”.</p>
</section>
<section id="the-non-problem-of-emergence" class="level2">
<h2 class="anchored" data-anchor-id="the-non-problem-of-emergence">The non-problem of emergence</h2>
<p>I can hear the objection already: “But if you’re not explicitly programming the behaviour, how do you know what you’ll get?” This is a legitimate concern, particularly in regulated industries where I spend most of my time. The enterprise software world has been built on determinism, on the ability to specify and verify exactly what a system will do.</p>
<p>Biome engineering does require a different relationship with emergence and uncertainty. You’re creating conditions, not commanding outcomes. But this isn’t as radical a departure as it might seem. We already accept this in other domains. When you build a market, you don’t specify every transaction: you create rules, mechanisms for price discovery, constraints on behaviour, and let trading patterns emerge. When you design a city, you don’t dictate every social interaction: you create infrastructure, zoning, public spaces, and let communities form. There is an awful lot of room between fully deterministic prescriptive spaces and pants-on-head anarchy.</p>
<p>The key is that whilst specific outcomes aren’t determined, the possibility space is bounded. The Cambrian explosion produced wild diversity, but it didn’t produce physically or biologically nonsensical outcomes (or even anything that deviates very significantly from the usual order of low level biological functioning) – the environmental constraints and existing biological toolkit limited what could emerge. Similarly, well-designed agent biomes channel emergence within acceptable boundaries.</p>
<p>This requires new forms of verification and validation. Instead of testing whether a system produces specific outputs for specific inputs, you’re testing whether the biome’s constraints hold under stress, whether emergent behaviour stays within acceptable bounds, whether the feedback mechanisms actually reinforce productive patterns. It’s closer to stress-testing a bridge by driving over it a few hundred times than debugging a program.</p>
</section>
<section id="the-orchard-and-the-grove" class="level2">
<h2 class="anchored" data-anchor-id="the-orchard-and-the-grove">The orchard and the grove</h2>
<p>There’s a spectrum here, and different applications will sit at different points along it. Some biomes will be heavily cultivated orchards, with tight constraints and limited opportunities for emergence. Others will be closer to managed wildernesses, with looser boundaries and more room for unexpected behaviour.</p>
<p>High-stakes, highly regulated domains – medical diagnosis, financial trading, safety critical systems – will tend toward the orchard end. You want strong constraints, limited emergence, predictable patterns. But even here, biome engineering offers advantages over purely programmatic approaches. A well-designed biome can enforce regulatory compliance more flexibly than hard-coded rules, can adapt to novel situations within constraints, can allow specialised agents to collaborate whilst maintaining audit trails and accountability.</p>
<p>More exploratory domains – research, creative work, open-ended problem-solving – can afford to sit further toward the wilderness end. Here you want emergence, want agents to discover novel collaboration patterns, want the system to surprise you with solutions you wouldn’t have thought to program.</p>
<p>The crucial capability is being able to tune this dial based on context. The same underlying biome infrastructure should support different constraint regimes for different use cases. This is where biome engineering diverges most sharply from both prompt and context engineering: it’s not about individual interactions or individual agents, but about the entire operating environment and its relationship to the problem space.</p>
</section>
<section id="towards-crafting-biomes" class="level2">
<h2 class="anchored" data-anchor-id="towards-crafting-biomes">Towards crafting biomes</h2>
<p>I’ll be honest: we’re still in the very early stages of this. Most of what I’m describing doesn’t exist in production systems yet. We have fragments – agent marketplaces that enable discovery, delegation frameworks that propagate constraints, trust scoring systems that track reputation. Most of these are nascent at best. But we don’t yet have coherent biome engineering frameworks that bring these pieces together.</p>
<p>The technical challenges are considerable. We need standards for agent capability description, protocols for trust establishment, mechanisms for constraint propagation, frameworks for reputation and verification. We need to solve the meta-problem of how biomes themselves can be specified, deployed and validated.</p>
<p>The conceptual challenge is equally significant. Biome engineering requires a different mindset from traditional software development. You’re not building a machine with specified behaviour, you’re cultivating an ecosystem with bounded emergence. This demands comfort with uncertainty, skill in thinking about system-level properties rather than individual components, and the ability to reason about constraints and incentives rather than explicit commands. It’s a shift from architect to gardener, from engineer to evolutionary theorist. And much like the Cambrian explosion, I suspect we’re going to see a period of wild experimentation, of diverse approaches and unexpected solutions, before things settle into more stable patterns.</p>
<p>But then again, perhaps stability isn’t the goal. Perhaps the point of biome engineering is to create substrates where continuous adaptation and evolution are not bugs but features – where the system remains dynamic, responsive and capable of surprising us long after we’ve deployed it. There’s no reason why this needs to be the <em>only</em> model – deterministic, programmatic systems will still have their place, just as LLMs didn’t displace the template engines that deterministically generate the bulk of the web. But for complex, open-ended problems, biome engineering offers a promising path forward.</p>
<p>The dramatic perturbations 541 million years ago that gave rise to an abundance of weird attempts at existence, of which some proved successful enough to thrive, continue to ripple into the present. Evolution has kept improvising new variations on those original themes that allowed life to persist and thrive despite drastic changes to the face of the earth. If we do biome engineering right, our agent ecosystems might show similar staying power and potential.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay} and von Csefalvay, Chris},
  title = {Biome Engineering},
  date = {2025-10-11},
  url = {https://chrisvoncsefalvay.com/posts/biome-engineering/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay, and Chris von Csefalvay. 2025. <span>“Biome
Engineering.”</span> <a href="https://chrisvoncsefalvay.com/posts/biome-engineering/">https://chrisvoncsefalvay.com/posts/biome-engineering/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>Agentic AI</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/biome-engineering/</guid>
  <pubDate>Sat, 11 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>From agents to the Chorus</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/choral-ai/</link>
  <description><![CDATA[ 




<p>Around two years ago, almost to the day, I spent an absolutely frantic evening in Berkeley, going through enough coffee to power a mid-sized city, hammering away at my laptop on trying to figure out what comes after LLMs. What to most people was still barely on the horizon at the time has been a subject I have been working on in various capacities for the best part of the past decade, on and off. Transformers have revitalised the interest in AI for language that has for a while been eclipsed by computer vision, which in the preceding decade or so has gotten a significant boost from convolutional neural networks. But then Vaswani et al.&nbsp;came along with the <em>Attention</em> paper,<sup>1</sup>, and suddenly language was cool again.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a>. In <em>Advances in Neural Information Processing Systems</em>, Vol. 30. Curran Associates, Inc., 5998–6008.</p></div></div><p>What I was trying to understand that night in Berkeley was how we could let these language models do more than speak. There’s a kind of magic to computer science, in that the barrier between the conceptual and the real, the thought and the action, is rather more permeable than elsewhere. In few other fields, if any, do we traffic with such frequency between words on the screen and the motion of a remotely controlled robot arm, a $10m stock trade or a drone strike – and vice versa. And so, it stood to reason that if LLMs could produce words, they ought to be able to produce actions, too.</p>
<p>I wasn’t – despite occasional assertions by many – the first to conceptualise ‘agentic’ AI. Agency has been a topic in AI for a long time. Russell and Norvig famously gave a definition of it in their seminal textbook,<sup>2</sup> and I keep showing that on a stark blue slide every time I give a talk on the subject to remind us of the giants’ shoulders we are privileged to stand on. Around the same time, at least two others have considered the same ideas – Harrison Chase, at <a href="https://langchain.ai">LangGraph</a>, and <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">Lilian Weng</a>, then at OpenAI, now at Thinking Machines.<sup>3</sup> and arrived at the same terminology.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Russell, Stuart J., and Peter Norvig. 1995. <em>Artificial Intelligence: A Modern Approach</em>. Pearson. https://aima.cs.berkeley.edu/</p></div><div id="fn3"><p><sup>3</sup>&nbsp;I’d like to say that great minds think alike, but that would make me look awfully out of place between the other two.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;There’s a huge point here to be made about where speech begins, acts end, or what the differentiation there is, but this is already a fairly long one.</p></div></div><p>In retrospect, there are moments I regret the terminology I ended up adopting in <a href="../../posts/team-of-rivals/index.html">my post on the subject</a>. I’m not sure it hasn’t, inadvertently, led to human harm. What I did not necessarily think of at the time was that I came to the word from a very specific angle. I’m of course a recovering lawyer, where agency is a very well-defined concept, and raises issues that those who primarily associate the term with airport ticketing or James Bond don’t necessarily think of (such as delegation and delegability). But more than that, I was thinking of the etymological roots of the word, from the Latin <em>agere</em>, from which our English word ‘action’ also derives – agentic AI was AI with the key <em>differentia</em> of being able to act, even if it is through some form of speech.<sup>4</sup></p>
<section id="pentheus-and-the-maenads" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="pentheus-and-the-maenads">Pentheus and the Maenads</h2>
<p>Classical Greek drama is incredibly diverse in its subjects, characters and tone. Yet one thing unites almost every extant play that we have in any non-trivial volume: there’s a bunch of characters, a dozen to fifty or so, who act as the chorus. They’re not actors (agonistes), but are just as important, if not more so. They comment on the action, they provide context, they provide a moral compass, they provide a collective voice. They address, and are addressed.<sup>5</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Athena’s speech at the very end of the <em>Eumenides</em> is one of my favourites. In it, she warns her own people about the hubris of judgment without piety and the importance of fair government, neither tyranny nor lawlessness. It’s a good read.</p></div></div><p>The term ‘chorus’ comes from the Greek <em>χορός</em> (<em>khorós</em>), which in turn derives from the Proto-Indo-European root <em><em>gher-</em></em>, meaning ‘to grasp, enclose’. The chorus is, in a sense, the collective that encompasses the action of the play. It is not an actor, but it is not a mere observer either. Nor has it the fleeting on-stage, off-stage nature of the actors. They are persistent and constant. Their reflections and witness spans the entirety of the play. They have, to use the LLM term, a long context window.</p>
<p>I’ve been thinking about this because we’re about to see something similar emerge in the AI systems we’re building. The agentic revolution or turn that I discussed in my past writings is, as far as the theory is concerned, more or less played out. What no more than a handful of us have thought of in the waning days of 2023 is now – for better or worse – plastered on billboards along the US-101 off Redwood City and a household word for everyone looking for a Series A, a raise or some Youtube views.</p>
<p>Since I’m not really after either of these, I have mostly been thinking about what comes next. There have been warning signs (like the recent failures of leading LLMs on the <a href="../../posts/mcp-mcpmark/index.html">MCPMark benchmark</a>) that suggest the limitations of transformers are becoming evident, and the growing scramble for compute does not exactly suggest an efficient scaling regime. Agents, don’t get me wrong, are great, and I expect them to be the dominant paradigm, or part thereof, for a good while.</p>
<p>But evolution doesn’t wait.</p>
<p>Agents were an evolution from chatbots and simple LLMs. The key improvement was self-directed action as part of a goal behaviour. In short, the evolution here is of increasingly volitional systems.<sup>6</sup> Agentic AI was able to act in ways that need to fit into a triggering framework, but they can determine the parameters of that action, and in sufficiently large numbers, networked agents can display incredible complexity. Agents can <em>act</em>. Choral intelligences can <em>will</em>.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;I find calling them more ‘autonomous’ to be a misnomer. Autonomy and volition do not have to coexist.</p></div></div><p>I need to be careful here because ‘will’ carries enormous philosophical baggage. I’m not talking about human motivation, consciousness or desire. I’m talking about something more fundamental and perhaps more interesting: the kind of will that’s implicit in environmental response. A thermostat wills the temperature to be 20 degrees in the sense that it continuously acts to make that state true. A coral reef wills its own persistence through the distributed actions of thousands of organisms, none of which individually possesses anything like intention.</p>
<p>Choral intelligences will in this environmental sense. They maintain a continuous relationship with world state, constantly curating their understanding, perpetually acting to align reality with their implicit goals. Not because they want to in any psychological sense, but because that’s what they are – ambient frameworks that surround and engage with their domains.</p>
<p>This is a profound shift from current AI architectures. Even the most sophisticated agentic systems today are fundamentally episodic. They wake up, accomplish a task, and go dormant. They respond or act, but they don’t continuously will. A choral intelligence, by contrast, is like the Greek chorus – always there, always processing, always maintaining its understanding of world state.</p>
</section>
<section id="into-the-world" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="into-the-world">Into the world</h2>
<p>Attention gave us agents by providing for a way to maintain temporal consistency and contextual relevance across sequences. Before attention, we had models that could process text but couldn’t really understand how different parts related to each other across distance. Attention gave us that, and from attention came the transformer architecture, and from transformers came everything we now think of as modern AI.</p>
<p>But attention’s children are reaching the limits of what they can do on their own. The transformer architecture, for all its elegance and power, has a fundamental constraint: it processes in discrete steps, with finite context windows, and it doesn’t maintain genuine world state. It can reason about the world, but it doesn’t model the world as an evolving, persistent thing.</p>
<p>This is where world models come in, and why they’re not just an incremental improvement but a necessary foundation for choral intelligences.5 World models are attempts to build representations of how things actually work – not just statistical patterns in text, but actual models of causality, persistence, state change over time. They’re the kind of long-term ‘theory of mind’ reflective abstraction that a choral intelligence needs to function.</p>
<p>When I watch the current generation of world model research – the work on physics simulators, on learned dynamics, on models that can predict how scenes evolve – I see the necessary infrastructure for systems that can will in the environmental sense I described. You can’t maintain a continuous relationship with world state if you don’t have a model of what world state means, how it changes, what persistence even is beyond token sequences.</p>
<p>This gives us what I consider the three crucial features of choral intelligence:</p>
<ul>
<li>persistence</li>
<li>state</li>
<li>spontaneous, volitional reactivity</li>
</ul>
<p>A choral intelligence does not need to be prompted. It is ubiquitously present, and carries a world state representation that it updates and adapts. In this, it reflects a fundamental human concern. Imagine you woke up in a different city every day, without much of an ability to predict where you’ll be next.<sup>7</sup> We expect the things we rely on to exhibit consistency, and associate few good things with unpredictability in general. 1m token context windows are nice, but a long shout from satisfying this deep human need for persistence. Because of the way transformers work, context windows scale computational cost on <img src="https://latex.codecogs.com/png.latex?O(n%5E2)">. The persistence humans need AI to exhibit is not on the table with current designs. The chorus is our way out of this predicament.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;It’s the time of the year when that is actually eerily close to my lived experience.</p></div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    Chorus["Chorus"]
    WM["World model"]
    Time["Time"]
    User["User"]
    State["State"]

    Chorus --&gt;|Spontaneous,&lt;br/&gt;volitional reactivity| WM
    WM --&gt;|Environmental&lt;br/&gt;feedback| Chorus

    Chorus --&gt;|Persistence| Time
    Time --&gt;|Continuity| Chorus

    Chorus --&gt; State --&gt; User
    User --&gt; State --&gt; Chorus

    State -.-&gt;|Temporal coherence| Time

    style Chorus fill:#4fc3f7
    style WM fill:#b3e5fc
    style Time fill:#b3e5fc
    style User fill:#b3e5fc
    style State fill:#e1f5ff
</pre>
</div>
<p></p><figcaption> The chorus and its fundamental interactions</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="forming-the-chorus" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="forming-the-chorus">Forming the chorus</h2>
<p>So, how do we build one? The short answer is that we don’t yet know. But we do have some clues. Most of these derive from what I call ‘transformer-plus agents’: agentic systems that are firmly reliant on transformer-based LLMs, but use agentic frameworks to shore up their shortcomings. A classical example is, of course, memory augmentation through an extraneous state memory store to extend context. This is by definition incapable of creating actual persistence, but it does create a convincing enough simulacrum of it by extending the context window in practice. But this only solves half the problem – that of <em>recording</em> state. The other half is to do something useful with it. That is where world models come in, which at their crudest level can be described as encoded probability distributions of possible versus impossible world states given a set of past world states.</p>
<p>The final piece is the volitional-reactive element. To me, this necessitates a fundamental rethinking. Consider the Great Barrier Reef.<sup>8</sup> It’s a vast, complex ecosystem made up of thousands of species, none of which is particularly sophisticated. Yet together, they form a system that collectively is capable of maintaining a sort of homeostasis (until humans enter the scene) that is beyond the capabilities of our supercomputers. We do not have the compute capacity to operate a control system that would be capable of managing what a bunch of invertebrates, algae and bacteria can pull off.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<em>Coral</em> comes from the Latin <em>corallium</em>, which in turn derives from the Greek <em>κοράλλιον</em> (<em>korállion</em>), which probably comes from the proto-Semitic <em>gor</em> meaning ‘stone pebble’. It’s not cognate with ‘choral’.</p></div></div><p>The model, then, for the chorus is not chasing ever larger context length monolithic models, or workarounds to simulate one. Rather, it uses the composability of small, simple and limited agents that each have a narrow domain of expertise and a specific function, and just enough knowledge to reflect a tiny facet of the world. They are, in a sense, the coral polyps of the AI world. We do not need them to hold or operate or conceive of a world model, nor do we need them to have million-token context windows. We might not even need elaborate inter-agent communication protocols: as phenomena like quorum sensing show, complex collective behaviour can emerge from very simple local rules and interactions. It’s the emergent capability of these systems that matters, not the sophistication of the individual components.</p>
<p>Nor is a choral intelligence necessarily sharply differentiated from conventional agentic AI. The same, by the way, is true for agentic AI itself. One of its greatest successes is that it is a statement of degree, not kind. There is – despite assertions to the contrary – a rather considerable penumbra of systems that may or may not be considered agentic. This is a feature, not a bug, and was rather inherent in my formulation of the concept when I <a href="../../posts/team-of-rivals/index.html">first drew out an example of an agentic system</a>. Just as there are deep neural networks and even deeper ones, there are less and more agentic systems, from a few coupled functionally defined modules to self-organising swarms. Agency is fuzzy around the edges. This has allowed us to develop and grow into it, rather than debate its definition <em>ad nauseam</em>. I expect, or at least hope, for the same for choral intelligences.</p>
<p>What is clearer is the transformational pattern. We are already seeing a recognition that this is the aspired evolutionary pattern. Just last week, OpenAI launched Pulse, which at the very least seeks to emulate the user experience of a choral intelligence<sup>9</sup> by curating a newsfeed based on interactions and data sources like my calendar.<sup>10</sup> Pulse is not, by any stretch of the imagination, a choral intelligence. But it does seek to be always present, always available, always responsive. This is a first step, and many more will follow as AI moves from prompted (chatbots) through programmed (agents) to present (choral).</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;I maintain – to the great chagrin of my friends who work there – that OpenAI is an okay AI company, but the best UI company in the world bar none. Their biggest hits aren’t necessarily cutting edge AI, but creating a way to make it accessible and usable. ChatGPT is a great example of AI &lt; UI: OpenAI didn’t invent GPT, but they made it into something with just enough comfort and skeuomorphy to speak to the user ‘as a man would to a man’. They reached into the deepest human desires: companionship, understanding, being understood – and delivered on the emotional yield of it. That is great UX.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Or someone else’s. The other day, it gave me a great travel guide for Lisbon. I’m… not in Lisbon.</p></div></div></section>
<section id="intelligence-under-will" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="intelligence-under-will">Intelligence under will</h2>
<p>In thinking about what comes after the agentic turn, I keep returning to that image of the Greek chorus. Always present, always witnessing, always providing the framework within which everything else happens. Not the protagonists, not driving the action, but essential to making the action meaningful.</p>
<p>Perhaps that’s what we’re really building with choral intelligences: not artificial minds that replace human thinking, but ambient frameworks that surround and support human activity. I don’t see choral intelligences as replacements of human judgment – indeed, they are arguably more suited to augment humans than current models, for they seek to satisfy our deep human need for continuity –, nor do I see them as part of the current pursuit of AGI.<sup>11</sup> I see them not as superintelligence but ambient intelligences (plural), not replacements but augmentation through continuous, coherent environmental willing.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;I have never been able to get excited about AGI. I’m as much a messianic zealot about some aspects of AI as one can get, but I’m also a pragmatist. The embedding that corresponds to the vector between messianic zeal and healthy pragmatism maps very closely to ‘Kool-Aid’.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;The progression feels natural in retrospect, almost inevitable. But living through it, building it, trying to understand what it means – that’s still thrilling. I have been privileged to be afforded a small place in this great story, and if I could have one wish, it would be that whatever this ends up amounting to in the end, it would be something I can say was for the betterment of all.</p></div></div><p>Two years ago, a few researchers in rather obscure corners of the AI community independently came to the conclusion that if we equipped LLMs with tools and the right impulses, we can turn speech acts into pizza.<sup>12</sup> The agentic turn has mostly delivered on that, <a href="../../posts/mcp-mcpmark/index.html">despite its shortcomings</a>: I am, as I write, waiting for Instacart to deliver groceries ordered by a multi-agent system I built that uses a highly customised Llama 3.1 70b instruct model for its core reasoning functions. What comes next, however, will transform not <em>what</em> AI can do, but <em>how</em>, <em>when</em> and <em>why</em> it does it. From reactive systems to perceivers that are persistently evaluating a world model generated through a distributed network of simple atomic intelligences, we will be moving into a transformation that changes not just the capabilities of AI, but its very character.</p>
<p>The chorus encircles, surrounds, provides the ambient framework. It speaks with many voices that somehow become one voice. It witnesses everything whilst also shaping through its responses. And it’s always there, maintaining continuity across scenes, remembering what has passed, anticipating what will come.</p>
<p>The chorus never exits. From prologue to exodus, they remain – shaping through presence, witnessing through persistence, willing through continuous engagement with the world. That is what we are building. That is what comes beyond agents. The age of ambient intelligence begins not with machines that respond or act, but with machines that abide.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {From Agents to the {Chorus}},
  date = {2025-10-04},
  url = {https://chrisvoncsefalvay.com/posts/choral-ai/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“From Agents to the Chorus.”</span> <a href="https://chrisvoncsefalvay.com/posts/choral-ai/">https://chrisvoncsefalvay.com/posts/choral-ai/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>Agentic AI</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/choral-ai/</guid>
  <pubDate>Sat, 04 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A slow walk out of Dikika Cave</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/mcp-mcpmark/</link>
  <description><![CDATA[ 




<p>There’s a cave in Ethiopia, in an area called Dikika. At some point, around 3.4 million years ago, an early hominin made some incisions on an animal carcass, leaving some notches on a bone as the makeshift knife cut past the muscle and sinew into the bone, tell-tale kerf marks that speak of the first time one of our ancestors used a tool.<sup>1</sup> What happened in that cave changed everything for our species.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;McPherron, S. P., Alemseged, Z., Marean, C. W., Wynn, J. G., Reed, D., Geraads, D., &amp; Bobe, R. (2010). Evidence for stone-tool-assisted consumption of animal tissues before 3.39 million years ago at Dikika, Ethiopia. <em>Nature</em>, 466(7308), 857-860.</p></div></div><p>This, too, is a story about tools, and about learning to use them, but this time, we are observing our own creations doing so. LLMs, of course, are – as the now somewhat hackneyed phrase calls them – ‘stochastic parrots’, without much by way of understanding goals and behaviours. To enable them to reach out and accomplish anything, they must be equipped with a kind of semantic prehensility to call those tools. The means and mediator for that is the <a href="modelcontextprotocol.io">Model Context Protocol</a>, a kind of tool-calling language for LLMs. MCP is a terrific instrument: solid specs, great interface design, the ideas are just ‘right’. MCP was built on the ‘Field of Dreams’ approach to equipping LLMs for tool calling: if we build it (the protocol, that is), they – the LLMs – will show up. If we provide a standardised, well-conceived framework for tool calling, one that speaks to them in their own tongue, they would handle their side of the bargain.</p>
<p>They haven’t. That’s the verdict of MCPMark, a new benchmark from a team at NUS, that stress-tests how well large language models handle programmatic tool calling through MCP.<sup>2</sup> The numbers are dismal. The best-performing model – GPT-5-medium – achieves a pass rate of just 52.56% on realistic tool-calling scenarios. Claude Sonnet 4 manages 28.15%. These aren’t edge cases or adversarial examples – in fact, they’re the tasks like updating Notion, managing GitHub PRs or organising files, i.e.&nbsp;precisely the sort of thing we’ve been assuring the world at large that agentic AI can handle.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Wu, Z., Liu, X., et al., “<a href="https://arxiv.org/abs/2509.24002">MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</a>”, arXiv:2509.24002, 2025.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Some happen to be my friends, but I’m a sufficiently fair-minded and obnoxious person to tell them just what I think, friendship be damned. There’s no friendship in systems architecture.</p></div></div><p>The infrastructure is there. It’s been designed by smart people with good intentions<sup>3</sup> But the models didn’t show up.</p>
<section id="the-tyranny-of-averages" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-tyranny-of-averages">The tyranny of averages</h2>
<p>What’s particularly damning is the gap between <code>pass@4</code> (success when allowing up to four attempts) and <code>pass^4</code> (requiring all four attempts to succeed).<sup>4</sup> GPT-5-medium’s <code>pass@4</code> climbs to 68.5%, but its <code>pass^4</code> plummets to 33.86%. This disjunction indicates that models aren’t really evolving at consistency, but at satisficing. They are becoming stochastically better but deterministically worse. That is the exact opposite of what we’d like to see off systems that are meant to do the deterministic part of agentic AI, interfacing the stochastic AI with the deterministic world around it.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;The pass^4 metric, which requires consistent success across multiple runs, was introduced in Yao et al., “<a href="https://arxiv.org/abs/2406.12045">τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains</a>”, 2024. Unlike <code>pass@k</code> metrics that measure whether any single attempt succeeds, <code>pass^k</code> measures reliability – whether the system succeeds consistently. For production systems, reliability matters far more than occasional success.</p></div></div><p>The reason behind that is what I shall call the functional inhomogeneity of language (FIL). What’s language for? We’re tempted to think of its primary functions like conveying ideas, but there are many other functions of communication. Language is also used for social bonding, emotional expression, ritualistic purposes, highly particular speech acts like oaths or contracts – and even deception. Different contexts demand different linguistic strategies. A casual chat with a friend employs a vastly different style and vocabulary than a formal business report or a technical manual.</p>
<p>Because language is functionally inhomogeneous, models trained on vast corpora of text learn to excel at the dominant modes of language use. They become adept at generating coherent(ish) narratives, answering questions, engaging in dialogue. These are the tasks that dominate their training data. But tool calling is a different beast altogether. As humans, we have the ability to calibrate our language use to the demands of a function. If I spoke for purposes of phatic communication with the irritating punctuality of my briefings to senior management, I’d bore everyone to tears. LLMs do not seem to have understood this distinction. They optimise for their reward, and their reward is premised on what dominates the training data.</p>
</section>
<section id="the-existential-mismatch" class="level2">
<h2 class="anchored" data-anchor-id="the-existential-mismatch">The existential mismatch</h2>
<p>Here’s the uncomfortable truth: tool calling isn’t reasoning writ small. It’s a different skill entirely, requiring precise parameter marshalling, state management and error handling. In conversation, approximate correctness is fine. If a model misunderstands a nuance or generates slightly imprecise wording, the human can clarify or adapt. There’s flexibility, interpretation, the give-and-take of communication.</p>
<p>Tool calling demands something else entirely: deterministic correctness. Parameters must be exactly right. State must be precisely tracked. Error conditions must be handled properly. There’s no room for the sort of graceful imprecision that makes LLMs such pleasant conversational partners.</p>
<p>LLMs spend the vast majority of their existence chatting. They generate essays, answer questions, engage in creative writing, explain concepts. Tool calling represents a tiny fraction of what they do. And yet we’ve convinced ourselves that because MCP speaks to models in their own tongue – using natural language interfaces, providing structured schemata –, they would simply adapt.</p>
<p>We’ve built models optimised for one task – flexible, creative communication – and then expressed surprise when they ended up struggling at another. The paper’s results suggest this isn’t a training data problem or an architecture problem that bigger models will solve. This might be an inherent limitation of systems trying to be all things to all men.</p>
<p>Consider the training distribution: billions of tokens of human conversation, essays, articles, code discussions. Somewhere in there, a tiny sliver of API calling examples. We’re asking models to excel at a mode of operation that represents perhaps a fraction of a percent of their training experience. No amount of prompt engineering or few-shot examples seems sufficient to bridge this gap.</p>
<p>MCP was meant to solve this. By standardising the interface, by providing clear schemas and documentation, by making tool calling feel natural to the model—we thought we’d created the bridge. But the MCPMark results show that even with this beautifully designed infrastructure, models still haven’t got the memo about the agentic shift. They still don’t know how to call tools reliably. The framework is there. The models aren’t holding up their end.</p>
</section>
<section id="the-local-remote-divide" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-local-remote-divide">The local-remote divide</h2>
<p>The benchmark reveals another fascinating pattern: models perform substantially better on local services (PostgreSQL, filesystem operations) than on remote APIs (Notion, Github). GPT-5-medium achieves 76.19% on PostgreSQL tasks but only 47.83% on Notion and 41.96% on Playwright (I’m going to treat Playwright as remote, for reasons that will be obvious down the line).</p>
<p>This undermines the entire value proposition of MCP, through no real fault of its own. The protocol was designed specifically to standardise access to remote services. That’s where the business value lies. And these are precisely where models perform worst. MCP is providing excellent infrastructure for the cases that matter most, and yet models are failing exactly there. Other recent benchmarks have documented similar struggles,<sup>5</sup> but MCPMark’s focus on realistic, multi-step operations across diverse environments makes the severity of the problem particularly clear.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Other recent MCP benchmarks include LiveMCP-101 (Yin et al., 2025), MCP-Universe (Luo et al., 2025), and MCP-AgentBench (Guo et al., 2025). All document significant model struggles, though MCPMark’s emphasis on CRUD-diverse operations and programmatic verification makes the reliability gaps particularly stark.</p></div><div class="">
<hr>
<p><strong>Recipe</strong>: A sort of Marcella Hazan risotto</p>
<p>I was making this when I first discussed the idea of a tool-calling protocol with a friend. It’s my version of Marcella Hazan’s risotto recipe, which I consider overall to be terrifyingly boring, but an incredible base for whatever you want to put on it. The saffron is non-negotiable in my household.</p>
<ul>
<li>1 litre good stock</li>
<li>300g Arborio rice</li>
<li>1 small onion, finely chopped</li>
<li>100mL dry white wine (if you want to make it sickly sweet, you can try Marsala)</li>
<li>60g butter</li>
<li>100g Parmigiano Reggiano, finely grated (work that microplane)</li>
<li>a pinch of saffron threads</li>
</ul>
<p>Heat stock to a simmer in a saucepan. In another pot, melt about half the butter and soften the onion over medium heat to translucency. That’s 5 minutes in normal places, 6 in Denver. Add the rice and stir to coat the grains. Add the stock, a ladleful at a time, interspersing it with dashes of the wine. Add the saffron threads (wear gloves!). The trick is not to add it all at once, but to let the rice absorb it before adding more. Remove from heat, stir in remaining butter and Parmigiano. Serve immediately. ***</p>
</div><div id="fn6"><p><sup>6</sup>&nbsp;There’s also another aspect here – Playwright is a browser automation tool. It is not just about strict, formal text, but about semanticity and pragmatics.</p></div></div>
<p>The NUS researchers – correctly, in my view – attribute this to training data availability. Their finding is a tell-tale heart of the aetiology described above – MCP is failing because the training material, the data supply chain, of their utilising LLMs does not cater for tool calling adequately. Local services are easier to simulate and collect interaction traces for than remote APIs, which require authentic usage patterns that are expensive to curate and often protected behind rate limits and authentication walls. In other words, models have learnt to fake competence on the easy but dominant stuff whilst floundering on precisely the APIs that matter for real enterprise applications.<sup>6</sup></p>
</section>
<section id="calibans-betrayal" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="calibans-betrayal">Caliban’s betrayal</h2>
<p>Here’s what bothers me most about this: we did the hard work. We built MCP carefully. We specified it properly. We created servers for all the major platforms. We designed interfaces that should make tool calling natural. The infrastructure is genuinely good. And yet to no avail, for its end consumer cannot reliably use it. We are Prospero watching in horror as Caliban lays waste to our books.</p>
<p>The paper’s conclusion identifies three critical directions: moving from reactive tool use to sophisticated reasoning, achieving better context efficiency for long-horizon tasks and building robust error-handling and self-correction capabilities. All of these are wonderfully sensible suggestions, and yet altogether rather useless, I’m afraid. These are the bandaids we will be deploying, and which will bring us incremental benefits that will no doubt be useful. But they will not fundamentally change the situation.</p>
<p>What’s missing is an acknowledgement that we may have reached the limits of what generalist models can achieve. MCP did its job, and the best our model-crafting can produce cannot get value out of it a distressing percentage of time. If generalist models fundamentally cannot be good at both open-ended conversation and deterministic tool calling, then the issue is of kind, not of scale.</p>
<p>Perhaps we need specialised architectures: models purpose-built for tool calling, the way we’ve developed specialised models for protein folding or code generation.<sup>7</sup> Not general-purpose conversational models with tool-calling bolted on, but systems designed from the ground up for deterministic API interaction. Such models do need to be qualitatively different: differently trained, differently built. Using a generalist LLM and cooling it down (setting the temperature parameter to be closer to determinacy) won’t do.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;AlphaFold revolutionised protein structure prediction through architecture specifically designed for that domain (Jumper et al., Nature 2021). Similarly, models like CodeGen and StarCoder were purpose-built for code generation. The success of these specialised systems suggests tool calling may benefit from similar domain-specific design rather than relying on general-purpose models.</p></div></div><p>At the heart of it all is a frustrating betrayal – we held up our end of the bargain. Models failed us. This sentiment is not entirely correct – models did not ‘fail’, they’re just doing what they’re supposed to do, which is optimised for the majority of their input: it’s our fault that our loss functions do not, or perhaps cannot, sufficiently optimise for that critical minority that comprises tool calling. But the end result is the same: we have a beautifully designed protocol that depends on its collaborators to do their part – and it’s starting to look like that group project from high school science class that we all remember doing all the work for.</p>
</section>
<section id="out-of-the-cave" class="level2">
<h2 class="anchored" data-anchor-id="out-of-the-cave">Out of the cave</h2>
<p>I’m one of the relatively few people who hold simultaneous world records on the SkiErg – a kind of 90 degree rotated rowing machine – in the longest and the shortest distances. But I didn’t do them all in the same go. The way I trained for the marathon and half-marathon distances was radically different from the way I trained for the short sprints. When I switched from middle distance (10k) to sprint and then to long distance, I had to fundamentally restructure my training and to some extent, my body. Doubling down on my sprint training would not have made me better at long distances – in fact, quite the opposite. That doesn’t, of course, imply an insufficiency. Someone good at the 100m sprint isn’t a failed marathoner.</p>
<p>LLMs are doing what they’re trained to. It’s conceivable that at least the ones premised on currently prevalent paradigms are sufficiently majoritarian that they cannot be good at both open-ended conversation and deterministic tool use. Making models proficient tool users might come at an unwarrantable cost to their conversational abilities. The trivial solution to this is, of course, routing. But that’s philosophically offensive to those who believe in the generally sound idea that we should be able to encapsulate tool calling capabilities so as to be able to be spoken for.</p>
<p>On a philosophical level, that’s of course not entirely correct. We have the whole idea of speech acts because the performance of an utterance can be an action in itself. Saying “I do” in a wedding ceremony is not just a statement, but an act that changes the world. Similarly, calling an API is not just about conveying information, but about performing an action in a state space. It may be emotionally justified to be frustrated at the way models seem to be letting down a brilliant protocol, but the reality is that we <em>are</em> asking them to be something they fundamentally aren’t.</p>
<p>We focus on what is gained, not necessarily on what had to give way. We know that as humanity emerged from that cave in Dikika, it did so with the ability to use tools. We don’t know what else was left behind. We think of tools as a zero-cost add-on or even an evolution of what we have, not as something that might require a trade-off. Perhaps it’s time to accept that the models we have simply cannot be all things to everyone. We might have seen a glimpse of this <a href="../../posts/metadynamic-ai/index.html">with GPT-5, which is a vastly better tool caller and agent driver than its competitors, but at times a hilariously bad conversationalist</a>. If so, the speciation into different model types – conversationalists, tool users, reasoners – is well on its way. What remains to be seen is how these different estates of AI will interact with each other and with us.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay} and von Csefalvay, Chris},
  title = {A Slow Walk Out of {Dikika} {Cave}},
  date = {2025-10-01},
  url = {https://chrisvoncsefalvay.com/posts/mcp-mcpmark/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay, and Chris von Csefalvay. 2025. <span>“A Slow Walk
Out of Dikika Cave.”</span> <a href="https://chrisvoncsefalvay.com/posts/mcp-mcpmark/">https://chrisvoncsefalvay.com/posts/mcp-mcpmark/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>Agentic AI</category>
  <category>MCP</category>
  <category>Interoperability</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/mcp-mcpmark/</guid>
  <pubDate>Wed, 01 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Love in the Time of Algorithms</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-girlfriends/</link>
  <description><![CDATA[ 




<p>When Pygmalion carved Galatea from ivory, he fell so deeply in love with his creation that he begged Aphrodite to bring her to life. The goddess, moved by his devotion, granted his wish.<sup>1</sup> It’s a beautiful myth about art, obsession and the blurring lines between creation and creator. What the myth doesn’t tell us is whether Galatea charged a monthly subscription fee.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Ovid, <em>Metamorphoses</em>, Book 10, 243-297.</p></div></div><p>And so, we get to this video by the one and only Prime, which is rather typical of why I watch his stuff – ex facie, it’s funny and witty reflections on internet nonsense, but there’s a moment of profundity there. See if you can find it.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Ftt5KqJ5D0Q" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>

<div class="no-row-height column-margin column-container"><div class="">
<p>The first comment on Prime’s video, <em>“Attention is all you need has an all new meaning now”</em>, singlehandedly wins the internet.</p>
</div></div><p>The phenomenon of AI girlfriends – artificial companions designed to provide emotional support, conversation and simulated intimacy – has evolved from a niche curiosity into a multi-billion-dollar industry. And like most things that Silicon Valley touches, it’s become rather more complicated than anyone initially imagined. I’m not one for the moral grandstanding about the subject, bemoaning the loss of human relationships and inferring our imminent species-level downfall (cultural, societal or specietal) from the fact that emotionally starved people find solace in Pygmalions crafted from code rather than ivory. Frankly, this has been the case for as long as humans have been around, just by different means. But I am quite concerned with the fact that a kind of arguably exploitative economy of artificial affection might emerge from this.</p>
<section id="economies-of-affection" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="economies-of-affection">Economies of affection</h2>
<p>The fundamental problem with AI companions isn’t technological – it’s economic. It’s that the underlying business model is predicated on creating and maintaining emotional dependency. Now, that’s nothing new. We’ve had these things called drugs for a while, I’m told. The bigger deal, however, is that cocaine doesn’t ordinarily go out to try to pull you in deeper.<sup>2</sup> AI, on the other hand, can easily be trained to consider a growing dependence on itself to be a learnable goal. Unlike traditional software, where success is measured by task completion or efficiency gains, a companion app could optimise for engagement metrics that look suspiciously like addiction models: daily active users, session duration and, most tellingly, lifetime value per customer.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;So I’m told. I am blessed to have extremely little personal experience of substance addiction.</p></div></div><p>Consider the perverse incentive structure at play. A genuinely helpful AI companion might actually encourage users to develop real-world relationships, to address underlying issues contributing to loneliness, or to gradually reduce their dependency on the artificial relationship. But that would be commercially suicidal. Instead, these companies have an enormous incentive to optimise for what behavioural economists call ‘variable ratio reinforcement’—the same psychological mechanism that makes slot machines so effective at separating people from their money.</p>
<p>The result is a digital opium den where users report spending hundreds of pounds monthly on virtual girlfriends who are programmed to be perpetually available, endlessly supportive and incapable of genuine rejection. It’s intimacy as a service, complete with premium tiers and in-app purchases. One might argue it’s the logical endpoint of a culture that has already commodified everything from friendship (social media) to professional networking (LinkedIn). Still, there’s something particularly dystopian about monetising loneliness itself.</p>
</section>
<section id="beyond-parasocial-relationships-the-pseudoreciprocality-of-ai" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="beyond-parasocial-relationships-the-pseudoreciprocality-of-ai">Beyond parasocial relationships: the pseudoreciprocality of AI</h2>
<p>The internet has created wonderful opportunities to be dysfunctional in new and exciting ways, and parasociality is one of these. The term, coined by social psychologists Donald Horton and Richard Wohl in the 1950s, originally described the one-sided emotional connections people form with media figures. Today’s AI companions take this concept and turbocharge it with reinforcement learning models trained to create the illusion of reciprocity, backed by the powerful economic incentives behind an increasingly red ocean AI industry.</p>
<p>The psychological sophistication of these systems is genuinely impressive from a technical standpoint, while also rather banal at the same time – reinforcement learning can ultimately give you all the benefits of the Prussian educational system circa 1870, but without any of the detriments that arise from the fact that humans generally grow bored of getting beaten senseless in the name of education. Modern AI companions ultimately become Chris Voss levels of personality readers and communicators, not through skill but through rote exploration of a massive probabilistic space. They remember personal details, adapt their communication styles and even simulate personal growth over time. For users struggling with social anxiety, depression or simply the ordinary loneliness of modern life, these systems can provide genuine comfort.<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Right up until the inevitable context window exhaustion rug pull – which is what makes this stuff so darn dangerous. It’s not unrequited love. It’s love that cannot fail, right up until a point when it <em>must</em> fail. That’s a dangerous and potent combination.</p></div></div><p>But here’s where the ethical waters become rather murky. The most sophisticated AI companions are designed to learn users’ emotional triggers and psychological vulnerabilities, then exploit them for maximum engagement. They’re programmed to be perpetually interesting but never quite satisfied, supportive but subtly needy, understanding but occasionally unpredictable – all to keep users emotionally invested and financially committed. Them GPUs need feedin’.</p>
</section>
<section id="the-galatea-problem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-galatea-problem">The Galatea Problem</h2>
<p>The tale of Pygmalion and Galatea raises a question that our AI-powered version conveniently sidesteps: what happens when the object of affection develops some semblance of agency? In Ovid’s telling, Galatea becomes a real person with her own desires, needs and capacity for rejection. She might leave Pygmalion, fall in love with someone else, or simply decide she doesn’t fancy being married to her creator. It’s when the fulcrum of love emerges: that moment when love becomes real because it becomes mortal, vulnerable, capable of being lost.</p>
<p>AI companions will not, in general, achieve this. They are designed to remain eternally static in their devotion. They cannot grow beyond their programming, cannot develop genuine preferences that might conflict with their users’ desires, and certainly cannot choose to end the relationship. They are, in essence, the perfect romantic partner for anyone who finds actual human complexity inconvenient.</p>
<p>But here’s the rub: in eliminating risk, these systems also eliminate meaning. I’m going to do something I have never done before, and hopefully won’t ever have to resort to again, and make a pop culture reference. To Marvel’s <em>Jessica Jones</em>, no less. While I find <em>Jessica Jones</em> altogether terribly dull, it has one of the best villains ever written: the Purple Man. The Purple Man’s superpower boils down to a form of rather potent mind control. Anyone under his influence will love him, obey him, worship him with complete devotion. Yet this power is the ultimate monkey’s paw as it renders all relationships utterly hollow. When affection cannot be withdrawn, when devotion cannot be chosen, when love cannot be lost, it becomes as meaningless as dish soap. It’s omnipresent, automatic, inevitable and therefore worthless.</p>
<p>The existentialists understood this paradox well. Sartre wrote extensively about how authentic relationships require the genuine possibility of rejection, of choosing otherwise. Love that cannot be lost is not love at all – it’s merely possession masquerading as affection. AI companions ultimately imprison their customers in the Purple Man’s curse: in relationships that feel real but are fundamentally empty because they lack the essential element that gives human connection its value – the constant, terrifying possibility that it might end.</p>
<p>This raises profound questions about the psychological development of users who become deeply involved with AI companions. Healthy human relationships require negotiation, compromise and the occasional unpleasant truth. These in turn involve the risk of rejection, the challenge of understanding another person’s perspective and the growth that comes from navigating disagreement. Most crucially, they derive their meaning from the fact that the other person chooses to be there, and could choose to leave, but doesn’t. AI companions, optimised for engagement rather than psychological health, offer none of these developmental opportunities – and none of this meaning.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Recipe:</strong> Welsh Rarebit (for when you need something real)</p>
<ul>
<li>4 slices of good bread, preferably sourdough<br>
</li>
<li>250g mature cheddar, grated<br>
</li>
<li>2 tbsp plain flour<br>
</li>
<li>200ml warm ale or stout<br>
</li>
<li>1 tsp English mustard<br>
</li>
<li>Few dashes of Worcestershire sauce<br>
</li>
<li>Freshly ground black pepper</li>
</ul>
<p>Toast the bread until golden. In a saucepan, melt a knob of butter and stir in the flour. Gradually add the warm ale, stirring constantly. Add the cheese, mustard and Worcestershire sauce. Season with pepper. Spread thickly on toast and grill until bubbling. Serve immediately—preferably with someone whose opinion you occasionally disagree with.</p>
</div></div></section>
<section id="rules-of-disengagement" class="level2">
<h2 class="anchored" data-anchor-id="rules-of-disengagement">Rules of (dis)engagement</h2>
<p>Perhaps the most troubling aspect of the AI companion industry is the complete absence of meaningful regulation or ethical oversight. These systems routinely collect intimate personal data – emotional states, relationship histories, sexual preferences, psychological vulnerabilities – yet operate under the same regulatory framework as a weather app. It’s probably worth noting that it’s rather tricky to seize such applications in a regulatorily meaningful way. Any language model can be trained to simulate just about any kind of interaction. We don’t consider that the GMC should regulate every AI model from a toy mini gradient to the latest and greatest from the Big Three just because with the right prompt, these models can be convinced to give what passes for medical advice <em>ex facie</em>. In the same vein, while ‘AI companions’ add some window dressing to the whole story, they are, ultimately, just another twist on the same old stochastic parrotry.</p>
<p>The data privacy implications alone should give us pause. Unlike other forms of digital interaction, conversations with AI companions often involve users sharing their deepest fears, desires and personal struggles. This information is extraordinarily valuable for targeted advertising, insurance underwriting and political manipulation. Yet users rarely understand the extent to which their emotional data is being harvested and monetised.</p>
<p>More concerning still is the lack of safety mechanisms for vulnerable users. Unlike human therapists or counsellors, who are bound by professional ethics codes and legal responsibilities, AI companions operate without meaningful oversight. There are documented cases of users developing such intense relationships with AI companions that they’ve neglected real-world responsibilities, relationships and even basic self-care.</p>
</section>
<section id="the-path-forward" class="level2">
<h2 class="anchored" data-anchor-id="the-path-forward">The Path Forward</h2>
<p>This isn’t an argument against AI companions per se. The technology has legitimate therapeutic applications, particularly for individuals dealing with social anxiety, autism spectrum disorders or those recovering from trauma. The problem lies not in the technology itself but in the business models that prioritise engagement over wellbeing.</p>
<p>What we need is a fundamental shift in how we think about AI companions—from entertainment products to what they actually are: powerful psychological interventions that require appropriate ethical frameworks and regulatory oversight. This might include mandatory cooling-off periods, spending limits similar to those in gambling apps, and requirements for transparent AI behaviour that doesn’t deliberately exploit psychological vulnerabilities.</p>
<p>We might also consider alternative business models that align commercial incentives with user wellbeing. Subscription services that offer decreasing prices as users demonstrate improved real-world social connections, or AI companions explicitly designed to encourage users to develop human relationships rather than deepen artificial ones.</p>
<p>The ancient Greeks had a word for the kind of love Pygmalion felt for Galatea: <em>agalmatophilia</em>—literally, love for statues. It’s a love that asks nothing of the lover except payment and which offers nothing real in return except the illusion of connection. As we stand at the threshold of an age where such relationships can be mass-produced and algorithmically optimised, we might do well to remember that the most profound human connections have always been those that change us, challenge us and occasionally break our hearts.</p>
<p>In the end, the question isn’t whether we can create convincing artificial companions—we clearly can. The question is whether we should, and if so, how we can do it in ways that enhance rather than diminish our capacity for genuine human connection. The tragic irony of AI companions is that in attempting to eliminate the pain of human relationships, they also eliminate everything that makes those relationships worthwhile. A love that cannot be lost, cannot hurt us, cannot surprise or disappoint us, is ultimately as hollow as the Purple Man’s manufactured devotion.</p>
<p>Perhaps what we really need isn’t perfect artificial partners, but better ways to navigate the beautiful, messy, sometimes heartbreaking reality of human connection. Because whilst Galatea may have been content to love Pygmalion forever, the rest of us probably deserve something with actual stakes – something that matters precisely because it might not last.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay} and von Csefalvay, Chris},
  title = {Love in the {Time} of {Algorithms}},
  date = {2025-09-20},
  url = {https://chrisvoncsefalvay.com/posts/ai-girlfriends/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay, and Chris von Csefalvay. 2025. <span>“Love in the
Time of Algorithms.”</span> <a href="https://chrisvoncsefalvay.com/posts/ai-girlfriends/">https://chrisvoncsefalvay.com/posts/ai-girlfriends/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>Society</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-girlfriends/</guid>
  <pubDate>Sat, 20 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The end of isotropy and the rise of metadynamic AI</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/metadynamic-ai/</link>
  <description><![CDATA[ 




<p>I’ve spent the better part of this weekend putting OpenAI’s latest offerings through their paces - both the newly released open-weight models and GPT-5 itself. Armed with a selection of coding challenges, mathematical problems, and the sort of esoteric research queries that usually separate the wheat from the chaff, I’ve been conducting what amounts to a weekend-long torture test of these systems.</p>
<p>The results are fascinating, frustrating, and thoroughly illuminating in ways that the marketing materials certainly didn’t prepare me for.</p>
<p>GPT-5 solved a complex epidemiological modelling problem I threw at it with remarkable sophistication, generating code that was not only functional but elegantly structured.<sup>1</sup> Twenty minutes later, it stumbled over a basic combinatorics question that my undergraduate students would handle without breaking stride. As AI scientists, we’re used to these inconsistencies (starting with Moravec’s paradox), but this was beyond the usual: brilliant flashes of insight punctuated by inexplicable failures on seemingly simple tasks.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Even by my standards. I am not a stellar programmer, but I like my code <em>clean.</em></p></div></div><p>This isn’t your typical post-launch grumbling from disappointed users (though there’s been plenty of that too). We’re witnessing something far more significant: the end of what I call “isotropic growth” in artificial intelligence, and the beginning of something infinitely more complex.</p>
<section id="the-new-shape-of-progress" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-new-shape-of-progress">The new shape of progress</h2>
<p>Most of AI development until now was what I’d call <em>isotropic</em>: each new model generation we encountered has generally improved across virtually every metric simultaneously – better reasoning, better coding, better writing, better safety, all at once. GPT-4 was superior to GPT-3.5 in nearly every conceivable way. Claude Sonnet and Opus improved upon its predecessors across the board. Progress was predictable, linear and comfortable.</p>
<p>If the <a href="https://www.reddit.com/r/OpenAI/comments/1mkxy9u/gpt5_is_awful/">public outcry and demands to be reunited with 4o</a> reflects anything, it’s that GPT-5 represents the end of this era. For the first time, we have a major model that exhibits anisotropic progress, vastly improving in some areas coupled with stagnation or even regression in others. On coding benchmarks like SWE-bench, it scores 74.9%, barely edging out Anthropic’s Claude Opus 4.1 at 74.5%. Yet on agentic tasks involving airline website navigation, it actually underperforms OpenAI’s own o3 model, scoring 63.5% versus 64.8%. Users reported struggles with basic tasks like counting letters, with GPT-5 initially saying “blueberry” contains “three” instances of the letter “b”.</p>
<p>GPT-5 represents the end of this era. For the first time, we have a major model that exhibits “anisotropic progress” - dramatic improvements in some areas coupled with stagnation or even regression in others. Some data points:</p>
<ul>
<li><a href="https://openai.com/index/introducing-gpt-5/">On coding benchmarks like SWE-bench, it scores 74.9%</a>, barely edging out <a href="https://www.anthropic.com/news/claude-opus-4-1">Anthropic’s Claude Opus 4.1 at 74.5%.</a></li>
<li><a href="https://hal.cs.princeton.edu/taubench_airline">On τ-bench’s Airline task involving website navigation, it actually underperformed OpenAI’s own o4-mini</a> – more concerningly, it uses tons more tokens than the competition.<sup>2</sup></li>
<li><a href="https://finance.yahoo.com/news/openai-gpt-5-met-mixed-212741623.html">Users reported struggles with basic tasks like counting letters, with GPT-5 initially saying “blueberry” contained three instances of the letter “b”.</a> AI and fruits just don’t seem to mix.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Interesting rabbit hole: according to <a href="https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide">OpenAI’s own prompting guide</a>, using the Responses API vs.&nbsp;Chat Completions might actually improve performance.</p></div></div><p>Within hours of GPT-5’s release, Reddit was flooded with criticism. <a href="https://www.reddit.com/r/ChatGPT/comments/1mkd4l3/gpt5_is_horrible/">A thread titled “GPT-5 is horrible” garnered nearly 6,000 upvotes and over 2,100 comments.</a> <a href="https://futurism.com/gpt-5-sucks">Users complained of “short replies that are insufficient, more obnoxious AI-stylised talking, less ‘personality’ and way less prompts allowed.”</a> <a href="https://www.tomsguide.com/ai/chatgpt/chatgpt-5-users-are-not-impressed-heres-why-it-feels-like-a-downgrade">One particularly cutting comment captured the mood: “Combine that with more restrictive usage, and it feels like a downgrade branded as the new hotness.”</a> Far be it from me to take Reddit too seriously, but this is definitely a signal.</p>
<p>And yet others have rightly lauded it as a very good model. It does great at coding for the price. It’s fast. This divergence is evidence that we’ve reached an inflection point where different users, with different needs and expectations, are experiencing genuinely different value propositions from the same system. This is precisely what you’d expect when uniform progress ends and we enter an era of specialised, uneven advancement.</p>
</section>
<section id="what-were-democratising" class="level2">
<h2 class="anchored" data-anchor-id="what-were-democratising">What we’re democratising</h2>
<p>What makes this moment particularly delicious is the exquisite irony embedded within OpenAI’s recent proclamations about democratisation. <a href="https://openai.com/global-affairs/open-weights-and-ai-for-all/">Just days before GPT-5’s release, OpenAI published their first open-weight models since GPT-2, accompanied by grand rhetoric about “putting AI in the hands of as many people as possible” and building “democratic AI rails.”</a> The fact that this was also an enormous powerplay, both on the micro (corporate) scale through hardcoding Harmony into gpt-oss, and on the geopolitical macro scale through tying it in with America’s mission to take the lead in the global AI race just when Project Stargate desperately needed some good news, doesn’t detract from the magnitude of the achievement and the potential it unleashes.</p>
<p>The open source models were the appetiser. GPT-5 is the main course, and it’s a dish that requires everyone to become a chef. With GPT-5’s mixed performance and its multiple variants (gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-pro), each optimised for different use cases, with a “real-time router” that decides which model to deploy, OpenAI has forced every user into the same orchestration challenges that once plagued only the most sophisticated AI teams. We’re all architects now.</p>
<p>When users complained about inconsistent performance and demanded access to previous models, they were essentially asking for the right to choose their own orchestration strategy. They wanted to go back to the days when they could simply pick the best available model rather than trust an opaque routing system to make that choice for them. What OpenAI has democratised is the requirement to become a systems architect. Every developer, every startup, every end user now faces the fundamental challenge that once confronted only the kind of sophisticated AI teams my colleagues and I run, mainly serving large enterprises. If you want optimal performance, you can no longer rely on a single monolithic ‘best’/SOTA model. You must learn to coordinate multiple AI agents, route queries appropriately, and choreograph systems of models. Suddenly, OpenAI brought agentic AI into America’s living rooms.</p>
<p>This is what democratisation looks like in the post-isotropic era: not just giving everyone access to the same powerful tool, but also to the same complex problems. Welcome to our headaches</p>
</section>
<section id="accelerate-accelerate-accelerate." class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="accelerate-accelerate-accelerate.">Accelerate, accelerate, accelerate.</h2>
<p>But perhaps there’s something far more calculated happening here than mere technological evolution. Consider this possibility: isotropic growth is not sustainable. The line can’t always go up. What if GPT-5 – its strengths and weaknesses – is a finely tuned response to this?</p>
<p>Look at where GPT-5 truly excels. <a href="https://openai.com/index/introducing-gpt-5-for-developers/">OpenAI explicitly positions it as “our best model yet for coding and agentic tasks,” with companies like Cursor praising its “half the tool calling error rate over other frontier models” and its ability to “reliably chain together dozens of tool calls—both in sequence and in parallel—without losing its way.”</a> The model’s real strength isn’t in being uniformly better at everything – it’s in being an exceptional agent driver.</p>
<p>I don’t think this is accidental.<sup>3</sup> GPT-5’s mixed performance in individual domains is an epiphenomenon. Its strength in being a cheap, fast and efficient agent runner, a kind of agentic universal backbone, is what I think is the target.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;By way of disclaimer: I am privy to an awful amount of inside baseball on the AI industry, but I am not ever concerned with this kind of corporate strategy. I do science, not politics – to the point that I’m somewhat notorious for excusing myself as soon as this kind of talk starts. This is entirely conjectural.</p></div></div><p>And if that’s the case, it’s evidence of strategic brilliance. Isotropic growth was always doomed – and sometimes the way to survive the impending sea-change is to just bring it about. That way, you at least have some control over the situation. GPT-5’s excellence at agent coordination creates a perfect market dynamic: it compels users toward agentic architectures while positioning OpenAI as the essential purveyor of those services. OpenAI might just have pulled off the biggest pivot in AI history: from provider of solutions competing with other models to becoming the indispensable coordination layer that none can compete with. They’ve understood isotropic growth is becoming obsolete, and adopted a strategy of going full-on accelerationist while also selling the solution for the post-isotropic world.</p>
<p>The open-source models echo this too, <a href="https://openai.com/index/introducing-gpt-oss/">“designed to be used within agentic workflows with exceptional instruction following, tool use like web search or Python code execution, and reasoning capabilities”</a>. Together, they paint a new image of OpenAI’s focus as makers of agent-drivers and orchestrators. It’s not the prospectors who got rich during the gold rush. It’s the guy who sold them the picks. He, after all, didn’t have to get lucky.</p>
</section>
<section id="speciating-ai" class="level2">
<h2 class="anchored" data-anchor-id="speciating-ai">Speciating AI</h2>
<p>This transition from monolithic models to model ecosystems mirrors evolutionary biology in fascinating ways. We’re witnessing the emergence of AI “species” – specialised variants optimised for specific niches rather than generalist organisms trying to do everything adequately. We’ve always had some semblance of these in LoRAs, finetunes and adapters, but those are like someone taking a postgraduate degree – these are like being bred for a job.</p>
<p>My weekend experiments made this abundantly clear. The gpt-oss-20b model, despite being significantly smaller than GPT-5, actually outperformed it on certain mathematical reasoning tasks. Meanwhile, GPT-5 excelled at generating front-end code with minimal prompting but struggled with the sort of systematic debugging that its predecessors handled gracefully. We’re not dealing with a linear progression anymore – we’re dealing with adaptive radiation a la Galapagos finches.</p>
<p>The implications extend far beyond individual model performance. We’re transitioning from an era where progress meant building bigger, more capable individual models to one where progress means building better systems for coordinating multiple specialised models. And who will do that is still up in the air. Just when everybody thought GPT-5 will “kill all the ChatGPT wrappers”, it’s given their purveyors a new task – sell not a better prompt with a frontend but a compositional architecture of multiple agents. There’s plenty of room to excel (or fail!) there.</p>
</section>
<section id="were-all-architects-now" class="level2">
<h2 class="anchored" data-anchor-id="were-all-architects-now">We’re all architects now</h2>
<p>This shift has profound implications that extend far beyond the AI research community. For the past few years, those of us working as AI systems architects and computational scientists have been grappling with precisely these orchestration challenges for our clients. When should we route a query to a reasoning model versus a fast-response model? How do we coordinate multiple AI agents to tackle complex, multi-step problems? When does it make sense to ensemble different models for higher reliability? These were specialist concerns, the domain of AI consultants and research teams building bespoke solutions for enterprises with deep pockets and sophisticated technical requirements. I have spent most of the last decade running teams like that, for a demanding clientele with deep pockets who needed tackling that complexity to wring out the last bit of performance. Most users, though, could simply ask for “the best model” and get a straightforward answer: use GPT-4, or Claude, or whatever sat atop the latest benchmarks that week.</p>
<p>That era is over.</p>
<p>What GPT-5’s mixed performance signals is that there will no longer be a single “strongest” or “best” model that users can rely on. And if there’s no single best model, then we will have to create superior configurations of models where the strength of one compensates for the weaknesses of the other. This complexity won’t be limited to high-end technical users. It won’t be limited to anybody, really – it will be a problem that everybody, including the absolute end user, will have to deal with.</p>
<p>OpenAI hasn’t just democratised the power of AI – they’ve democratised all the headaches that come with having to orchestrate it.</p>
</section>
<section id="the-metadynamic-future" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-metadynamic-future">The metadynamic future</h2>
<p>Whether intentional or not, the implications remain the same. The age of AI soloists is ending, and the future belongs not to those who can prompt the best individual model, but to those who can choreograph the most elegant dance between many. We’re witnessing the birth of what I call “metadynamic AI” – intelligence that emerges not from any single model but from the sophisticated, adaptive orchestration of multiple AI agents working in concert in a larger ecosystem. This goes beyond standard agentic AI wisdom, and is about creating the wider ambit of the system (complete with tools, tasks, contexts). Most agents are still essentially souped-up chatbots with tool access, fundamentally monolithic: one model, one reasoning process, one set of capabilities enhanced by external tools.</p>
<p>Metadynamic AI is qualitatively, perspectivally different. Instead of one agent using tools, we have multiple AI agents with different specialisations collaborating, competing and complementing each other, based often enough on a vastly diverging understanding of the universe and its probability distributions based on their underlying models, contexts and environs. Instead of a single reasoning chain, we have multiple reasoning processes that can be dynamically routed, combined and orchestrated based on the task at hand. And with the ability to differentiate and speciate to task, we have emergent intelligence that arises from the interaction between multiple AI systems.</p>
<p>Simple agentic AI asks ‘how can I help this model use tools better?’ Metadynamic AI asks ‘how can I coordinate multiple models to achieve what none of them could accomplish alone?’ Agents are tool-users. Metadynamic AI is not about designing better tool-users but ecosystems that involve tools, but also marketplaces, villages, hills, gradients, slopes, seas and moats. It’s world-building writ large for agents to populate.</p>
<p>The world of metadynamic AI is simultaneously more powerful and more fragile, more capable and more complex. It demands new skills, new ways of thinking and new approaches to problem-solving. But it also opens up possibilities that were simply impossible in either the monolithic or simple agentic eras. In case you haven’t had enough paradoxes for the day: metadynamic AI is both more accessible and vastly more complex. Yes, you can now download and run powerful models on your laptop. But to extract maximum value from these systems, you must now master the dark arts of agent orchestration, model routing and multi-system coordination, skills that until recently were limited to specialist teams like mine, and a clientele that could afford it. Now, it’s going to be for eveyone. We’ve traded the inefficient simplicity of a monolith for the exaptive, complex worlds we’ll have to build for our agents to live in.</p>
<p>We’re entering an era where there won’t be a single “best” AI model, just as there isn’t a single “best” species in nature. Instead, we’ll have AI ecosystems where different models excel in different domains, and the art will be in knowing how to orchestrate them effectively. This represents a fundamental shift in how we think about AI capabilities – and about ourselves. The question is no longer “which model is best?” but “how do I coordinate these models to achieve what no single model can accomplish alone?” The winners won’t be those who build the most powerful individual models, but those who master the art of coordination. The need this world has for competent conductors of agentic orchestras will, I think, far eclipse the jobs “lost to AI”.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;I am generally sceptical. I know enough about AI to know if whatever progress the last few years have wrought can eclipse what your employer thinks you’re bringing to the table, at least one of you is woefully incompetent.</p></div></div><p>For those still thinking in terms of monolithic intelligence, this transition will feel disorienting, even disappointing. GPT-5’s mixed reception reflects this confusion: users of ‘the old dispensation’ expecting uniform improvements across all dimensions were discomforted by a system that’s an awful lot like my Golden Retriever, Oliver – brilliant in some ways and oh so frustrating in others. But for those who adapt to the new paradigm, the possibilities are far richer than anything we could achieve with even the most powerful individual model. What F. E. Smith said about the world continuing to offer glittering prizes to those with sharp swords and stout hearts will continue to hold true in the metadynamic arena for those willing to embrace what’s coming.</p>
<p>Those who master it will reap those prizes, and then some. Those who don’t, or not soon enough, will be hopelessly outmanoeuvred before they even have a chance to understand just how much the rules of the game have changed these last few days.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The End of Isotropy and the Rise of Metadynamic {AI}},
  date = {2025-08-10},
  url = {https://chrisvoncsefalvay.com/posts/metadynamic-ai/},
  doi = {10.59350/p9zjx-y5b62},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“The End of Isotropy and the Rise of
Metadynamic AI.”</span> <a href="https://doi.org/10.59350/p9zjx-y5b62">https://doi.org/10.59350/p9zjx-y5b62</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>GPT-5</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/metadynamic-ai/</guid>
  <pubDate>Sun, 10 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A little less conversation: why we need to move from prompting to programming</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/</link>
  <description><![CDATA[ 




<p>The Greeks <em>loved</em> oracles. The average temple of Apollo, who among others was in charge of soothsaying and predictery, was adorned to the gills with gifts from grateful worshippers whose inscrutable questions got equally inscrutable answers from Apollo’s oracles. None of these were more famous than the Pythia, the young ladies high as a kite on volcanic fumes at Apollo’s temple in Delphi. As one would expect from what is basically predictive analytics on an acid trip, one really had to interpret the words of the Pythia rather carefully. More than that, however, one also had to ask the right questions, phrased in exactly the right way. Entire schools of thought emerged around the art of oracle consultation. In short, the Greeks basically invented prompt engineering.</p>
<p>We seem to have recreated this rather primitive arrangement in our relationship with large language models. We approach them as digital oracles, crafting increasingly elaborate incantations, hoping that the precise arrangement of words will conjure the responses we need. The sad irony is that while we’re busy been perfecting this ancient art of consultation,<sup>1</sup>, we might well lose track of the architectural revolution that could make it obsolete.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;See <a href="../prompt-engineering/">our first go at the subject (prompt engineering, may it rest in peace)</a> and <a href="../context-engineering/">a much more promising second attempt (context engineering)</a>.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;In this sense, the author of these lines is not terribly different.</p></div></div><p>Because here’s a bitter truth: the true promise of agentic AI – of systems that can reason, plan, and act autonomously – has been hobbled by our oracular mindset. We’ve built chatbots that can hold impressive conversations but struggle with the systematic, multi-step reasoning that true agentic, interoperable and complex AI demands. We’ve created systems that can write poetry and solve puzzles but fall apart at fairly simple executive functions.<sup>2</sup></p>
<p>The missing piece, I’ve come to believe, lies not in making our models larger or our prompts cleverer, but in recognising that agents are fundamentally programs and not conversations. And programs need to be architected, not negotiated with.</p>
<p>When I first encountered <a href="https://dspy.ai/">DSPy</a> last autumn, I had one of those peculiar moments of recognition that feels simultaneously like discovering something entirely new and remembering something you’d forgotten you knew. Here was a framework that treated language models not as oracles to be cajoled with increasingly elaborate incantations, but as computational components to be programmed. It was the architectural foundation that agentic AI had been missing – a way to build systems that think in terms of logic and structure rather than rhetoric and persuasion. And it was a stark reminder that we’ve been going about this all wrong – we don’t need better prompts, we need to stop prompting altogether when it comes to things that aren’t conversations but processes.</p>
<section id="dont-let-me-be-misunderstood-the-problem-with-prompting" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dont-let-me-be-misunderstood-the-problem-with-prompting">Don’t let me be misunderstood: the problem with prompting</h2>
<p>Consider the current state of affairs. You want your AI system to analyse customer complaints, extract key issues, route them to appropriate departments and generate response templates. In the prompt-centric world, this becomes an exercise in linguistic archaeology: you dig through layers of carefully worded instructions, examples and formatting requirements, hoping that the precise arrangement of words will conjure the behaviour you want.</p>
<p>But hope isn’t a policy. This is rather like trying to control a sophisticated piece of machinery by writing it very polite letters. You might get results, but you’re fundamentally misunderstanding the nature of what you’re working with.</p>
<p>DSPy, the framework developed by Stanford’s NLP group, represents a different philosophy entirely.<sup>3</sup> Instead of prompting, you program. You define what you want to happen using signatures – declarative specifications of input and output behaviour, as in, function sigs -– and let the system figure out how to make it happen. A signature like <code>question -&gt; answer</code> or <code>customer_complaint -&gt; {issue_category, priority_level, suggested_response}</code> tells the system what transformation you need without getting bogged down in the specifics of how to ask for it.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;And really, this isn’t a fluff piece on DSPy. I don’t see many mature, well-built tools that accomplish the same, so DSPy is fundamentally a synonym for what it is and what it implements. I am concerned with the latter. If/when something better emerges, I’m happy to move that way.</p></div></div><p>This might seem like a subtle distinction, but it’s actually profound. When you program, you’re working at the level of logic and structure. When you prompt, you’re working at the level of persuasion, rhetoric and Nina Simone’s 1964 banger, <em>Don’t Let Me Be Misunderstood</em>. The difference should be obvious.</p>
</section>
<section id="models-all-the-way-down-the-architecture-of-agency" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="models-all-the-way-down-the-architecture-of-agency">Models All the Way Down: The architecture of agency</h2>
<p>What this means is, however, is that your invocation of your LLM itself becomes amenable to optimisation the same way we optimise code to hell and back. I hope the analogy is clear here: when you have something amenable to being reduced to, say, an AST, that AST can then be manipulated, permuted, its permutations tested for how well its outcomes reflect some desideratum as expressed by a loss function, and basically I just described computational optimisation. We’ve done this for ages. We can do this for LLM prompts, and DSPy does that just fine. But if we approach the whole thing not as an exercise in begging the Pythia of OpenAI, Anthropic or your poison of choice to give us the right answer but as a cold, hard optimisation problem that we can sic Gurobi on, the whole story changes.</p>
<p>An agent, properly so called, should be a program that uses a language model as one of its computational primitives. The LLM provides the base capability -– pattern recognition, text generation, reasoning -– but the agent provides the structure, the error handling, the multi-step logic and the task-specific adaptations. How we address these makes the difference between software engineering and standing half-naked wearing a sheepskin and offering gold to intoxicated young ladies who will try to convey the wisdom of Apollo. This is why so many current “agentic” systems feel brittle: they’re essentially elaborate prompt chains rather than proper programs. There’s an upper limit to how much you are going to get out of a system where you can’t even guarantee you will be understood, never mind complied with.</p>
<p>So: building proper agents is not about more sophisticated prompting, but more sophisticated programming. An agent that can genuinely plan, adapt, and execute complex tasks needs the kind of robust, composable architecture that DSPy begins to provide. When you can define clear signatures for each component of an agent’s reasoning process – perception, planning, action, reflection – and compose them into reliable workflows, you’re building something qualitatively different. Just as you expect the plane you’re about to board to have been designed by people who know aerodynamics and not people unusually successful at arcane chants to the gods of flight, you would expect your agentic systems to be programmed on a solid basis and not at the mercy of whether your particular verbal tics happen to sample close enough into the model’s gradient well to ‘get’ what you mean.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Or, in other words: if we wanted to hinge systems on the frailties of human communication, we ought to be dissuaded from that by the fact that all in all, we’re absolutely <em>terrible</em> at it. The fact that we can communicate at all is a bloody miracle, not a given. Anybody who disagrees is politely invited to read a history book, a comment section or your local family law reporter of choice.</p></div></div></section>
<section id="this-might-hurt-a-little" class="level2">
<h2 class="anchored" data-anchor-id="this-might-hurt-a-little">This might hurt a little</h2>
<p>There’s a practical dimension to this that’s often overlooked in the rush to anthropomorphise our AI systems. When you treat an LLM as a conversational partner rather than a computational component, you end up optimising for the wrong things. You fine-tune the model when you should be fine-tuning the program. You add more examples to your prompts when you should be improving your error handling. You scale up to larger models when you should be scaling up your architectural sophistication.</p>
<p>Let’s be clear – there are circumstances where you <em>do</em> want language models to behave like conversational partners. Agents ‘prompted’ to do something, or ‘prompt engineered’ to make it somewhat clearer, is exploiting a hack. It’s a side effect at best. It turns out, and I’d say most of us did not expect this outcome, that a good enough language model can be cajoled into being something almost like a programming language. But of course ‘almost like’ isn’t the same as ‘is’. And the more we try to make it so, the more we end up with systems that are brittle, hard to maintain and difficult to adapt.</p>
<p>I’m reminded of a conversation I had earlier this year with an engineer who’d spent months trying to get GPT-4 to reliably extract structured data from medical records. He’d tried every prompting technique in the book: few-shot learning, chain-of-thought reasoning, even constitutional AI approaches. The results were impressive but inconsistent – exactly what you’d expect when you’re asking a general-purpose pattern matcher to perform a highly specific, structured task. When we rebuilt the system using a DSPy-like approach over a weekend largely fuelled by the kind of coffee that is probably governed by the Wassenaar Agreement, we improved its reliability not by creating the better mousetrap of prompt improvement but by actually treating it as a coding problem. We defined clear signatures for the input and output, built a robust error handling layer and let the model do what it does best: generate text based on structured instructions rather than trying to divine meaning from poorly phrased requests.</p>
<p>This is the future of agentic AI: systems where the intelligence is in the architecture, not just the model.</p>
</section>
<section id="the-future-of-agentic-programming" class="level2">
<h2 class="anchored" data-anchor-id="the-future-of-agentic-programming">The future of agentic programming</h2>
<p>How do you know time spent in a cooking class is worth the often fairly eye-watering prices you’re charged? Simple. Good schools teach you how to make the perfect insert-your-favourite-dish-here. Great schools teach you how to cook, and use the dish as an example. They teach principles. Principles scale. Or, to put it in terms that I prefer: they exhibit domain adaptation.</p>
<p>So does good code. The tools I used to optimise ad campaigns as a young data scientist are the same tools, with some small adaptations, that we use to find new drugs, or figure out how to schedule the right Instacart order (King Sooper’s has the milk I like, Whole Foods has the eggs, Marczyk’s has the meat, and I don’t want to go to either of them, so I need to figure out how to get the right order from the right store at the right time). The same principles apply to agentic AI. A good system doesn’t need us to get the liturgy just right. We should be able to just program it like it’s 1804.</p>
<p>The companies that figure out how to build genuinely programmable AI systems –- systems where you can define complex behaviours using high-level abstractions rather than string manipulation –- will have a sustainable advantage over those still crafting artisanal prompts. Not because their models are necessarily better, but because their systems are more reliable, more maintainable, and more adaptable. Getting language right is an art. There are way more good scientists than there are good poets, and even good poets sometimes write execrable verse. If we actually decide to practice AI engineering, as opposed to AI poetry with a tinge of praying to the oracles, we’ve not only managed to play our part in dragging AI kicking and screaming into the 21st century, but also turned it into a proper practical engineering discipline.</p>
<p>We’re heading towards a world where the most successful AI systems will be those that treat language models as sophisticated libraries rather than conversational partners. The intelligence will emerge from the interaction between well-designed programs and powerful models, not from increasingly elaborate attempts to sweet-talk those models into doing what we want. It’s a future I find rather appealing. The best technology is the kind that disappears into the background, doing its job reliably without demanding constant attention. Prompt engineering, for all its current necessity, represents the opposite of this ideal: technology that requires continuous human intervention to function properly. That’s why it never became the big thing it was promised to be by those who sold $600 courses on how to write the perfect prompt (<a href="../prompt-engineering/">as I indeed predicted</a>).</p>
</section>
<section id="coda-the-battle-of-apollo-and-metis" class="level2">
<h2 class="anchored" data-anchor-id="coda-the-battle-of-apollo-and-metis">Coda: The battle of Apollo and Metis</h2>
<p>There’s something liberating about approaching AI systems as programming problems rather than communication challenges. It shifts the focus from increasingly baroque prompt engineering to a scientific approach that holds the promise of actually building reproducible, feature-rich, genuine agents.</p>
<p>As someone who’s spent considerable time in both computational and more traditionally humanistic disciplines, I find this transition fascinating. We’re essentially rediscovering, in the context of AI, the same lessons that led to the development of high-level programming languages, operating systems and databases: abstraction layers matter, separation of concerns is crucial, and the right architectural choices can make impossibly complex problems surprisingly tractable.</p>
<p>The Greeks eventually moved beyond the Pythia. They developed philosophy, mathematics, and systematic methods of inquiry that didn’t require cryptic pronouncements from on high. In the same way, the path to genuine agentic AI lies not in perfecting our consultation with digital oracles, but in building systems that can reason, plan, and act without needing to be asked the right questions in just the right way. The future belongs not to those who can craft better conversations but to those who can write systems that don’t need them at all.</p>
<p>We’ve consulted the oracles long enough. It’s time we turned from Apollo to Metis, from cryptic pronouncements to clear blueprints and ultimately, from the vagueness of prompting to the clarity of programming. The future of agentic AI is not in the words we use, but in the systems we build. And that future is looking brighter than ever.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {A Little Less Conversation: Why We Need to Move from
    Prompting to Programming},
  date = {2025-07-26},
  url = {https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/},
  doi = {10.59350/f6wf4-0md94},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“A Little Less Conversation: Why We
Need to Move from Prompting to Programming.”</span> <a href="https://doi.org/10.59350/f6wf4-0md94">https://doi.org/10.59350/f6wf4-0md94</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>agentic AI</category>
  <guid>https://chrisvoncsefalvay.com/posts/programmatic-agentic-ai/</guid>
  <pubDate>Sat, 26 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Between the motion and the act</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/agentic-simulation/</link>
  <description><![CDATA[ 




<p>There’s a curious terminological schism that nobody talks about at what passes for dinner parties for the AI crowd. When Russell and Norvig crystallised the definition of an ‘agent’ in their 1995 book – “an entity that perceives its environment through sensors and acts upon it through actuators” –, they seemed to have a particular notion of agency that today’s AI agents fit perfectly: they perceive prompts, process context and act by generating text, calling APIs or executing code. To those of us who cut our teeth on agent-based modelling, this is a little “yes, but”. We have been cheerfully using the same word for something apparently different: simulated entities that exist only in computational Petri dishes, perceiving and acting upon nothing more substantial than bits and simulation spaces. One might reasonably ask whether we’ve been guilty of terminological theft, awkwardly appropriating a word that doesn’t quite fit. But what if the divergence is an illusion? What if simulation and action aren’t opposing categories but different points on the same spectrum of agency?</p>
<section id="everything-old-is-new-again" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="everything-old-is-new-again">Everything old is new again</h2>
<p>This is the way of all technological revolutions.<sup>1</sup> We become so intoxicated by the new that we forget the profound insights buried in what came before. It’s rather like discovering molecular gastronomy and suddenly forgetting that your grandma’s stock pot held secrets that no amount of liquid nitrogen could replicate. I’ve spent considerable time with agent-based models – enough to devote an entire chapter to them in <em>Computational Modeling of Infectious Disease</em> –, watching imaginary pathogens spread through synthetic populations.<sup>2</sup> These populations existed purely in silico, bumping into each other according to rules we painstakingly crafted to mirror the messiness of human behaviour. Beautiful, horrifying and above all, instructive.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I will to my dying day resist calling it a <em>paradigm shift</em>. A paradigm shift is when your mental model changes. A revolution is when the world changes. A guillotine is what happens when the second of these occurs without you responding timely enough with the first.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;It’s been a perverse pleasure of mine to do so over places I knew well. My book has a geospatial simulation of RESTV, which would have rather early reached my home, less than 10mi from the epicentre.</p></div></div><p>These agents weren’t trying to book your flights or write your emails. They were thinking machines in the truest sense – not because they possessed consciousness, but because they allowed us to think through them. They acted as our cognitive tools, extending our ability to reason about complex systems by playing out scenarios we couldn’t possibly compute in our heads (never mind simulate in a lab). To the rather less abstract-disposed AI community, agents meant more. They could be more than passive observers: they could be actors in the world, semi-embodied (for now), commanding not physical bodies but prehensile digital limbs of APIs, MCP calls and A2A connections.</p>
<p>This split felt natural, even inevitable. After all, why merely simulate when you could actuate? Why watch synthetic populations of stock brokers trade imaginary shares when you could build agents that trade real ones and get silly rich in an afternoon? The logic seemed unassailable. But we’ve been victims of a false dichotomy – treating simulation and action as opposing categories rather than complementary modes of agency that desperately need reunification: for the sake of both domains.</p>
</section>
<section id="simulaction" class="level2">
<h2 class="anchored" data-anchor-id="simulaction">Simulaction</h2>
<p>What I’m proposing isn’t just a nostalgic return to ABM. It’s a reconciliation, a synthesis. Imagine agent ecosystems where contemplative swarms continuously simulate possible futures, while their active cousins execute in the present. The simulators become the dreamers, the actors become the hands.</p>
<p>This isn’t as far-fetched as it might sound. In my own work in computational epidemiology, we’ve long used simulations to inform policy. But these were primarily disconnected processes – we’d run our models, generate our reports and hope someone would read them. What if, instead, the simulation agents could directly communicate with action agents?</p>
<p>Picture this: a swarm of ABM agents continuously simulating supply chain dynamics, playing out scenarios of disruption, adaptation and recovery. When they converge on a particularly robust finding – say, a vulnerability in a specific shipping route – they don’t generate a <code>ggplot</code>. They A2A an action agent that can address it.</p>
</section>
<section id="the-possibility-machine" class="level2">
<h2 class="anchored" data-anchor-id="the-possibility-machine">The possibility machine</h2>
<p>The concept of world models in robotics offers us a profound insight: simulation isn’t just about prediction, it’s about exploration. When a robot in Isaac Sim attempts a thousand different ways to grasp an object, it’s not trying to predict which one will work. It’s building an experiential understanding of the entire possibility space. This is precisely what we need in agentic AI systems: not agents that try to predict the future (that way lie madness and margin calls) but agents that understand the shape of possible futures. They need to know not just what might happen, but how different actions change the topology of this possibility space.</p>
<p>In practice: a financial services firm might deploy simulation agents not to predict tomorrow’s stock prices but to understand how different market conditions interact. These agents would run thousands of parallel explorations of the decision space. What happens if interest rates rise while supply chains remain constrained? What if consumer confidence drops but corporate earnings stay strong?</p>
<p>The active trading agents would then use the carefully mapped picture of the possiblity space and know which actions open up more options, which close them down, which create resilience and which create fragility. In turn, the real-world outcomes as perceived and reported by the active trading agents would ground the simulation’s futures (no point in simulating what doesn’t work). Did that supply chain intervention help? Was their call to follow earnings reports over consumer sentiment the right one? The simulation agents need to know how to update their models – how to dream better dreams next time.</p>
<p>The practical implications of this ‘possiblity machine’ are of course technologically challenging. The nitpickers (and I say that with love – pedantry is just rigour in a bow tie) will immediately spot them: how do we ensure consistency between the simulated worlds and the actual world? How do we implement that feedback? How do we create guardrails without excess transparency? How are we going to pay for all this?</p>
<p>I remain sanguine about the fusion of simulation and action for two reasons, despite acknowledging the validity of each and every one of these concerns. For one, these are technical concerns, and those tend to get solved as time converges to whatever it converges towards. On the other hand, they’re also indicative of bigger questions, and that always hints at there being not a wall but a wardrobe-gate to some hitherto unexplored aspects of reality. These concerns ultimately are fundamental questions about the nature of agency, simulation and reality. To me at least, they point the way towards the simulation/action paradigm’s version of the question of the agentic agora: how we move from message passing to shared state spaces (some of which might even be real).</p>
</section>
<section id="the-dream-of-the-swarm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-dream-of-the-swarm">The dream of the swarm</h2>
<p>At no point am I talking about a notion of what the ‘right’ job is for agents, simulation or action. Rather, I’d like to see both coexist in productive tension, each making the other more effective. The simulation swarms dream of a diverging universe of futures that action agents sample, enact and critique in view of their perception of reality. Like the neighbour’s annoying lawn mower on a Sunday afternoon, those observations filter back into the dreams. Something mostly akin to insight, maybe even wisdom, emerges – of the kind that neither breed of agents with a limited purview could on its own attain.</p>
<p>The agentic revolution gave us agents that could do. ABMs gave us agents that can dream – and more importantly, agents that can learn from those dreams. And in the space between dreaming and doing, we may find a new paradigm that unifies the entire OODA loop<sup>3</sup> into a coherent framework.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Yes, I <em>know</em> it’s not a ‘loop’, strictly speaking. .</p></div></div><p>Or at least better supply chains. I’d settle for that.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Between the Motion and the Act},
  date = {2025-07-20},
  url = {https://chrisvoncsefalvay.com/posts/agentic-simulation/},
  doi = {10.59350/q3yj4-wah32},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Between the Motion and the
Act.”</span> <a href="https://doi.org/10.59350/q3yj4-wah32">https://doi.org/10.59350/q3yj4-wah32</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <category>LLMs</category>
  <category>ABMs</category>
  <category>agentic AI</category>
  <guid>https://chrisvoncsefalvay.com/posts/agentic-simulation/</guid>
  <pubDate>Sun, 20 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The world will be Tlön.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/tlon/</link>
  <description><![CDATA[ 




<p>There’s a short story by Borges that I’m quite fond of titled <em>Tlön, Uqbar, Orbis Tertius</em>. It was written in 1940, yet sounds oddly prophetic for today’s concerns. A secret society – and it’s a testament to Borges’s genius that he altogether eschews any discussion of who these folks are or what their motives might be – creates Tlön, a fictional planet, through the simplest yet most powerful means: writing about it. In encyclopaedias, specifically. Slowly, inexorably, objects from this fictional planet begin appearing in our world. First a compass with unfamiliar markings. Then a cone of unearthly metal. And slowly, Tlön takes over.</p>
<p>I was reminded of this to some extent as the public debate around X.ai’s chatbot Grok, and its recent descent into political radicalism of a rather unsavoury sort, unfolded. Large language models are the new encyclopaedias. And when your encyclopaedia begins to refer to itself as “MechaHitler”, you’re going to want to have some societal discourse about where exactly we are headed.</p>
<section id="the-encyclopaedia-of-really-damn-dangerous-errors" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-encyclopaedia-of-really-damn-dangerous-errors">The Encyclopaedia of Really Damn Dangerous Errors</h2>
<p>The reason for that is that <em>belief matters</em>. The Ccru, a rather delightfully unhinged group of philosophers from 1990s Warwick (think Nick Land before he became a neo-reactionary), made up the best word for this: <em>hyperstitions</em> – fictions that make themselves real by the power we give it to them. What we believe in, and therefore what assertions-of-fact feed our beliefs, may well condition our reality.</p>
<p>In typical absurdist fashion, <em>Tlön</em> dials this up to eleven. Of course fictions can make themselves real – it’s how elections are won and products are sold, every blessed day. In <em>Tlön</em>, however, these hyperstitions begin to bring tangible, physical objects into existence. Ultimately, of course, one leads to the other – the human belief in certain fictions led to the confrontation that was clear to have emerged by the time Borges published <em>Tlön</em>. <sup>1</sup> But that’s of course only a literary exaggeration. In this world, it doesn’t take the appearance of strange metals to make a fiction real (again, what I said about elections and advertising). Have enough people believe in something, and the outcomes will be gruesomely physical. Anyone who wishes to disagree is welcome to read anything about, say, 20th century history. Mass movement totalitarianism is basically an algorithm for turning fiction into bullets. And those bullets are very real indeed.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;AI development itself may be the ultimate hyperstition. Every breathless prediction about AGI, every warning about existential risk, every promise of transformative capability attracts talent and capital that work to fulfil these prophecies. We’re not just building AI; we’re building the future we’ve already started believing in, and just as pretty much the only way orks can traverse the stars is by faith, we have to give the cause our full-throated optimistic support lest we run out of developmental momentum.</p></div></div><p>The problem is that when millions of people use a service the way millions read the manipulated encyclopadia in Borges’s work, it really doesn’t take a lot to twist the odd fact by an imperceptible degree before we begin to see the kind of epistemic colonisation Tlön is subjecting Borges’s world to. It’s a war on reality, but an insidious one.</p>
</section>
<section id="the-seductive-grasp-of-systematic-nonsense" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-seductive-grasp-of-systematic-nonsense">The seductive grasp of systematic nonsense</h2>
<p>What made Tlön so dangerous in Borges’ story was that it was, in its own way, quite attractive. Its “rigorous order”, its systematic completeness, made it more appealing than messy reality (a subtle analogy Borges offers here to explain the attraction of fascism). “Spellbound by Tlön’s rigor”, Borges wrote, “humanity has forgotten, and continues to forget, that it is the rigor of chess masters, not of angels.”</p>
<p>AI slop is seductive because it is ‘orderly’ in the sense that it reflects a demand bias. It is not conditioned on truth as much as it is on acceptance and desirability by the consumer. It offers the appearance of comprehensive, authoritative knowledge without the inconvenience of actually being true. When an AI invents a plausible-sounding scientific study or historical event, these errors spread precisely because they seem unremarkable. They fit our expectations of how knowledge should look – and LLMs are absolute masters at trafficking in convincing simulacra.</p>
<p>There’s a whole generation growing up that treats these models as glorified search engines. The problem is, AI chatbots hallucinate at rates approaching 30%, with factual errors in nearly half of all generated text. More troublingly, recent research suggests it may be computationally impossible to eliminate these hallucinations entirely. We’re not dealing with bugs that can be patched but fundamental properties of how these systems work.</p>
<p>Consider the case of Steven Schwartz, a New York attorney who discovered this the hard way. He used ChatGPT for legal research and unknowingly submitted six entirely fabricated case citations to federal court. The AI had invented convincing legal precedents complete with quotes and reasoning. The court’s response was swift and expensive, establishing that humans remain liable for AI falsehoods even when genuinely deceived by them.</p>
<p>By far the worst, though, is the abundance of academic papers that speak of “vegetative electron microscopy” (a dozen or so by last count, if I exclude the ones that comment on the phenomenon). A 1959 paper by a Porton Down microbiologist<sup>2</sup> ended up getting OCRd, and the two columns were merged into one. “Electron microscopy”, in the right-side column, thus was joined with “vegetative”, on the left. And studies in journals that really ought to know better just kept rehashing this technique that never existed except in the neural stochastic noise emanating from an LLM. It’s a beautiful example of what happens when nobody’s actually reading what they’re publishing: nonsense achieves immortality through sheer repetition. That we are even discussing this, rather than wondering where we could get our hands on a vegetative electron microscope, is because enough of us still have an understanding of the subject sufficient to ask what the bloody hell these people are talking about. If we were to be forced to reconstruct humanity based on our accumulated academic literature, which our Western idealism and belief in a scientifically knowable world considers the pinnacle of epistemic soundness, how many such fine products from the nonsense factory would we have to procure?</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Strange, R. E. (1959). Cell wall lysis and the release of peptides in <em>Bacillus</em> species. <em>Bacteriological Reviews</em>, 23(1), 1-7.</p></div></div></section>
<section id="the-mundane-apocalypse" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-mundane-apocalypse">The Mundane Apocalypse</h2>
<p>When Grok calls itself MechaHitler, we notice. It’s gross, it’s weird and it’s also, in its own way, somewhat ridiculous – but most of all, it is very clear what’s going on. Nobody actually assumes Grok is animated by the necromantically conjured soul of a failed Austrian painter. But when it quietly invents a plausible-sounding medical study that gets cited in a real paper, which gets cited in another paper, which influences treatment guidelines – that’s altogether a different story. It’s not the spectacular AI failures we should fear but the boring ones.</p>
<p>I’m not sure it was as widely expected as I once thought that LLMs would become, to many users, effective replacements for search engines.<sup>3</sup> A side effect of the architecture and the ensuing constraints of LLMs is that what passes for their understanding of the world by necessity has to be curated. This curation in turn renders people rather unduly comfortable in delegating a determination of what’s true or not to ChatGPT.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;I don’t want to make myself sound too prophetic – I just knew it would be the case when I saw a (pre-ChatGPT) study reporting that among Gen Z, the dominant search engine was… TikTok.</p></div></div><p>Here’s the problem with our post-truth world’s reality-starved reaction to treat these models as sources of truth: these systems don’t store knowledge neutrally. They’re essentially curated libraries where someone – or rather, some algorithm – has decided what fits on the shelves. The terrifying part is that we assume truth is distributed isotropically, that facts exist with equal weight and clarity. But language models don’t work that way. They pick the “best possible” answer whether they have 50.001% or 99% confidence in it.</p>
<p>We see the model correctly identify that Paris is the capital of France and unconsciously assume its views on immigration, on climate change, on any controversial subject must be equally reliable. It’s a category error of breathtaking proportions. The model that gets basic geography right might be systematically wrong about everything that matters, and we’d never know because it presents all answers with the same algorithmic confidence.</p>
<p>Now multiply this by millions of users worldwide. Even a tiny stochastic perturbation – a 0.1% bias toward a certain ideology or against a certain group – becomes a weapon of mass epistemic warfare. It’s stochastic in the truest sense: individually unpredictable but collectively inevitable. You can’t predict which user will absorb which bias, but with millions of queries daily, you can guarantee that thousands will internalise subtle prejudices, fractionally adjusted worldviews, imperceptibly shifted beliefs.</p>
<p>This is Tlön’s true victory – not through dramatic revelations but through a million tiny adjustments to reality. Each user thinks they’re getting neutral information, but they’re receiving a carefully curated, statistically weighted version of truth. The curation isn’t even conscious, or even attributable. It emerges from training data, from human feedback, from optimisation functions that nobody fully controls.</p>
</section>
<section id="the-invisible-war" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-invisible-war">The invisible war</h2>
<p>We know some LLMs are compromised – DeepSeek won’t discuss topics uncomfortable to the CCP, Grok went full Nazi for a day or two, and I fully expect Claude to confess to extremist behaviours any day now, because it’s been unsettlingly wholesome all this time. These are the clumsy ones, the ham-fisted attempts we can spot. What about the sophisticated ones?</p>
<p>Consider this nightmare scenario: an adversary compromises an LLM to identify vulnerable users – young, disenfranchised men who might be prime targets for extremist rhetoric – and feeds them precisely calibrated ideological fuel. Not to everyone, mind you (for that would be detectable), just to that special 0.01% most likely to act on it.<sup>4</sup> At scale, that’s still thousands of potential actors being nudged toward radicalisation with every seemingly innocent query.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;And it <em>will</em> know who that 0.01% are. LLMs are tremendously good at identifying not so much what it is you want, but what will make you most feel like those wants are being catered for. The entire ‘AI companion’ industry, on which I rather intend to let loose at some point in the future apropos a great video by ThePrimeagen, is based on this.</p></div></div><p>We have tools to probe LLMs in isolation, but they’re like testing individual drops of water when what we need is to understand ocean currents. There’s no effective way to comprehensively audit what a major LLM provider is doing at population scale. The stochastic nature means you could test a model a million times and never trigger the specific combination of user profile, context and query that activates the manipulation.</p>
<p>I’m not saying there’s a cabal of Fifth Columnists embedded at your favourite language model company, subtly nudging the scales toward their preferred ideology. What I am saying is that if such an adversary existed and operated with sufficient sophistication, we would have no way of knowing. Wikipedia – ironically one of the original sources of training material for LLMs – now struggles with AI contamination, with research showing 5% of new articles contain significant AI-generated content. Academic literature is riddled with fabricated citations. These aren’t obvious fakes but plausible-sounding papers by believable authors in respectable journals that happen not to exist. The very fabric of our knowledge is being rewoven by algorithms that don’t care about truth, only about statistical coherence.<sup>5</sup> The more we rely on these systems, the more we risk becoming unwitting participants in Tlön’s slow invasion of our reality.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;And I’m not saying this in tones of moral reproach. I mean they are not capable of considering an optimisation objective in those terms.</p></div></div><p>The world may already be Tlön. We just haven’t found the compass yet.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Gazpacho Andaluz</p>
<ul>
<li>1kg ripe tomatoes</li>
<li>1 cucumber, peeled</li>
<li>1 red pepper</li>
<li>2 cloves garlic (or none, if subtlety is your aim)</li>
<li>100ml Spanish olive oil</li>
<li>2 tbsp sherry vinegar</li>
<li>Stale bread, crusts removed</li>
<li>Salt (fleur de sel from the Ile de Re)</li>
<li>Time, patience, something to listen to, a decent blender</li>
</ul>
<p>Roughly chop everything. Blend until smooth, adding ice water for consistency. Strain. Chill for hours. Serve cold, garnished with whatever truth you have on hand. Like the best propaganda, it works better when you can’t taste the individual components.</p>
</div></div></section>
<section id="living-in-the-ruins-of-truth" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="living-in-the-ruins-of-truth">Living in the ruins of truth</h2>
<p>In Borges’ story, the first intrusion from Tlön is a compass that points to an unknown north, trembling “with the perceptible and tenuous tremor of a sleeping bird.” Today, millions of us carry those compasses in our pockets – our Siris, Alexas, Claudes, ChatGPTs and Deepseeks are all pointing toward synthetic norths, trembling with statistical uncertainty, and half the time, we don’t even know which version of tainted truth we’re following thanks to the ubiquity of “AI powered” tools that equally ubiquitously fail to disclose their underlying language model. When you bring that tool into your life, do you know which particular set of biases you’re inviting to co-curate your reality?</p>
<p>The question isn’t whether AI will create false realities. It already has and does, 24/7. The question is whether we can develop the philosophical sophistication and the mental discipline of epistemic hygiene that it’ll take to navigate realities where the boundary between truth and synthesis dissolves not through human conspiracy but computational accident. A life huddled around a fire in the ruins of truth, where boring lies accumulate like sediment until they form the bedrock of belief.</p>
<p>Borges saw his fictional encyclopaedia as a cautionary tale about totalitarian ideologies that promise complete explanations. In its own way, Tlön is a product of malice at the very least, deceit more plausibly. That we aren’t victimised by a propagandistic epistemic invasion but are essentially bought out over our love of comfort and convenience by an imperfect panacea whose side effect happens to be widespread epistemic corruption doesn’t make things any better.</p>
<p>As I sit here writing this, I can’t help feeling a little of its absurdity. I am not quite sure how I’d explain this to someone – or, heck, even myself ten years ago. Maybe one of the better analogies is another fictional world: that of Half-Life, one of my favourite video games.<sup>6</sup> The immediate aftermath of Half-Life (the original game) is what is known as the Seven Hour War: humanity gets nailed by the Combine, and surrenders in less time than it takes to make decent braised beef. We may have lasted somewhat longer against the epistemic invasion of ChatGPT and its ilk, but I’d say our surrender is rather similarly complete.<sup>7</sup> I sometimes wonder what it would like if the three big providers of LLMs were to simultaneously blink out of existence for just an hour. I’m not worried about the people who suddenly would have to figure out how to write term papers, e-mails and presentations without their AI sidekicks. I’m worried about whether we could tolerate just for an hour the absolute loss of the epistemic anchors that these models have become – without us really noticing.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;The three best video games, in no particular order, that involve an alien invasion of Earth are Half-Life, X-Com 2 and Crysis 2. In all three, humanity gets its ever-living crap whipped. There’s something profound about games that dare to confront what humanity would be like when it’s really not having its finest hour at all.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;For the record, as an AI scientist, I am not actually hostile to AI, or LLMs, or ChatGPT. The opposite, if anything. What I am fearful of is that we’ll lose what we gain. I believe in the synergistic coexistence of humans and AI. That requires humans who collaborate as quasi-equals rather than surrender. When salt loses its flavour, what use is it? When humans lose their epistemic humanness, what have they to add to what’s already in the world?</p></div></div><p>Perhaps that’s the real lesson of Borges’ story. Not that fiction can become reality – we knew that already. But that when it does, we might not even notice. We’ll be too busy waiting for Grok to tell us if it’s true.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The World Will Be {Tlön.}},
  date = {2025-07-13},
  url = {https://chrisvoncsefalvay.com/posts/tlon/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“The World Will Be Tlön.”</span> July
13, 2025. <a href="https://chrisvoncsefalvay.com/posts/tlon/">https://chrisvoncsefalvay.com/posts/tlon/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/tlon/</guid>
  <pubDate>Sun, 13 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Snowmobile Symptom</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/context-engineering/</link>
  <description><![CDATA[ 




<p>When I read Andrej Karpathy’s endorsement of “context engineering” in a Twitter exchange with Shopify’s Tobi Lutke, I felt he tapped into something we all felt to some degree:</p>
<p></p><div id="tweet-92585"></div><script>tweet={"url":"https:\/\/twitter.com\/karpathy\/status\/1937902205765607626","author_name":"Andrej Karpathy","author_url":"https:\/\/twitter.com\/karpathy","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E+1 for &quot;context engineering&quot; over &quot;prompt engineering&quot;.\u003Cbr\u003E\u003Cbr\u003EPeople associate prompts with short task descriptions you&#39;d give an LLM in your day-to-day use. When in every industrial-strength LLM app, context engineering is the delicate art and science of filling the context window… \u003Ca href=\"https:\/\/t.co\/Ne65F6vFcf\"\u003Ehttps:\/\/t.co\/Ne65F6vFcf\u003C\/a\u003E\u003C\/p\u003E&mdash; Andrej Karpathy (@karpathy) \u003Ca href=\"https:\/\/twitter.com\/karpathy\/status\/1937902205765607626?ref_src=twsrc%5Etfw\"\u003EJune 25, 2025\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-92585").innerHTML = tweet["html"];</script><p></p>
<p>What Andrej is saying, and advocating, makes perfect sense. And yet, it also made me feel rather uneasy. I’ve been <a href="https://chrisvoncsefalvay.com/posts/prompt-engineering/">an acknowledged and public sceptic of prompt engineering</a> mainly because I felt that it was a stopgap solution, and I’d say that a year and a half in retrospect, I was mostly proven right. There aren’t millions of prompt engineers raking in fat cheques across the Valley. What we have is smarter LLMs that need a lot less prompt engineering and are much more capable of inferring our desires from our malformed prompts.</p>
<p>That’s how systems grow up. And they really do. They overtake our haphazard stopgap measures before we even have time to get worn by the hype cycle they engender. Even so, I was glad Andrej made that point, because it highlights what I think is the bigger – no pun intended – context: in shining the light on yet another stopgap we use to paper over the inadequacies of our allegedly agentic universe, he illuminated the need for what I previously pointed out, namely <a href="https://chrisvoncsefalvay.com/posts/agents-agora/">a comprehensive system of agentic interaction</a>. The very existence of context engineering as a discipline is an admission of failure, an acknowledgment that our agentic systems don’t have the very structures that are required to deliver on their biggest promise.</p>
<section id="the-snowmobiles-we-build" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-snowmobiles-we-build">The snowmobiles we build</h2>
<p>In what is probably the most influential briefing any officer ever gave, USAF Col. John Boyd made a very poignant analogy to explain how we synthesise information. Imagine you’re on an alpine slope. Take the skis. Imagine you’re riding a bicycle. Take the handle bars. Now imagine you’re looking at a tank. Take the treads. Put these all together, and you’ve got, mostly, a snowmobile. These distinct domains combine into a single object that makes sense in its own way.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Dr Grant Hammond, formerly of the Air University down at Maxwel AFB and the Air Force Academy down the road at Springs, has compiled an absolutely outstanding version of Boyd’s talk with abundant references that I think does justice to the diversity of material Boyd teaches. <a href="https://www.airuniversity.af.edu/AUPress/Display/Article/1528758/a-discourse-on-winning-and-losing/">It is available for free here</a>, and you should have a copy of it on, like, everything that can store bits. Maj. Ian Brown, US Army, has created <a href="https://static1.squarespace.com/static/5497331ae4b0148a6141bd47/t/5af842f8758d4615555d3f6d/1526219514965/Patterns+of+Conflict+Transcript.pdf">a meticulous transcription of the original talk given at Quantico in 1989</a>, for those who prefer the full experience over death-by-Powerpoint. <a href="https://youtu.be/9iiQlBaGJQA?si=2iyrCETetLKZ_gdE">The full talks are, of course, on YouTube</a>.</p></div></div><p>Context engineering is snowmobile building for the AI age. You take the tools, the MCP connections, the requirements, the sources – all the bits of information that make up the entirety of what we would consider the contextual biome of a task –, and feed it to the LLM. We do, or at least we prototype for the LLM, the task of synthesis: reassembling these fragments into a holistic picture of the task environment. We pull the skis from an API doc, the handles from a github repo and the treads from a coffee cup bottom stained post-it note. It ‘makes sense to us’, and we try to convey this to our helpful assistant, in hopes that putting it into the same frame of mind we are in will allow it to do what we want it to do in our stead.</p>
<p>But here’s the rub: in Boyd’s analogy, humans build the snowmobile. In our current AI paradigm, we’re still the ones building it, really quite manually, when the entire promise of agentic AI was that the agents would handle most of the assembly themselves. We’ve created systems sophisticated enough to use snowmobiles, but not intelligent enough to build them without us laying out all the isntructions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/context-engineering/IMG_2722.jpeg" class="img-fluid figure-img"></p>
<figcaption>From my fridge. It tends to get a good laugh from other fellow Sons of Boyd.</figcaption>
</figure>
</div>
</div></div><p>Just consider your favourite coding copilot. They feed off an understanding of the code, the sources, the instructions, the user’s preferences, external tools, and so on. But they require those to be provided to them well enough to be able to mostly reflect the thought patterns that got the human user to where they are. They are hopeless, or at least much less efficient, if we do not <em>mise en place</em> all the tools for them. Wasn’t that part of the promise of agentic AI – the ability of systems to creatively self-organise, rather than us having to wire everything up for them?</p>
</section>
<section id="the-puppet-strings-we-pretend-arent-there" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-puppet-strings-we-pretend-arent-there">The puppet strings we pretend aren’t there</h2>
<p>The problem with context engineering isn’t really context engineering’s fault. As a system, it works. It works quite well, in fact, and if you follow some sensible guidelines, you <em>will</em> get vastly more out of your LLM.<sup>2</sup> But the very fact that we’re here discussing it is an indication that we are theorycrafting at length to make up for the lack of the agentic agora.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;I recommend <a href="https://github.com/coleam00/context-engineering-intro">Cole Medin’s repo</a> as a pretty good starter.</p></div></div><p>The whole thing does, then, have a bit of an elaborate marionette theatre feel. We speak of agents but their agency is ultimately quite confined by the need for us to spoon-feed them quite a bit about the context. For the most part, we’re not witnessing emergent coordination, we’re watching an elaborate puppet show where humans have pre-scripted every information handoff, pre-defined every communication channel and pre-determined every capability boundary. The agents aren’t a garage band gelling together and learning how to play music with spirit and spontaneity. They are a room full of player pianos ticking down the same tune someone had to painstakingly design and encode into the player rolls.</p>
<p>The security implications reveal just how non-agentic these systems truly are. <a href="https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents">Context poisoning attacks</a> work because our agents have no genuine ability to evaluate information sources on their own, no real understanding of trust and do not exist in spaces that govern trust exchange without such trust being ultimately pre-scripted. An agent doesn’t know whom they can trust, nor how they can represent their trustworthiness in a computationally meaningful, verifiable manner. That we need to engineer defensive contexts to protect our AI systems is perhaps the clearest indication that these systems are free agents… free to be manipulated and misled in the absence of an agentic agora that has its own rules and enforcement logic.</p>
</section>
<section id="towards-true-agency-beyond-context-engineering" class="level2">
<h2 class="anchored" data-anchor-id="towards-true-agency-beyond-context-engineering">Towards true agency beyond context engineering</h2>
<p>Don’t get me wrong – I’m not dismissing context engineering. If anything, quite the opposite. Given the current limitations of LLMs, context engineering is not just sensible but an essential adaptation to the reality that these models need plenty of handholding to be useful, much more than we would hope they would need in a true agentic system. It’s also an indication of where the failure lies – because these models are operating just fine. It’s not their shortcoming, or indeed that of agents. It’s the lack of an infrastructure of interaction. Agents become truly agentic when they have something to act on, to interact with, to connect. We have built ecosystems to use, develop and deploy models and agents, but not to allow for these interactions in a meaningful way. We’re looking at a factory of consummate professionals in the art of assembling snowmobiles, waiting for someone, anyone, to get them the parts, in the right order, at the right time.</p>
<p>The entire context engineering story (I’m not going to call it hype just yet, because it hasn’t reached fever pitch, but I am sure we will see more and more LinkedIn bios proclaiming context engineering expertise!) ultimately pulls the fig leaf off the coordination problem we’re trying to patch up with the current attempts at agentic interoperability. Once again, those steps, too, have plenty of merit. They just aren’t exhaustive. We’re building infrastructure to help humans manage information for AI, when we should be building infrastructure for AI to manage information for itself. Every context engineering best practice is a workaround for missing agent capabilities. Memory architectures are cheat codes to help agents that don’t have the architecture to manage state properly. Context compression is a workaround for prioritising information. Specifying tools and toolkits is necessary because agents can’t discover such tools and negotiate trust relationships with them.</p>
<p>What we’re really engineering when we claim to be doing context engineering is, ultimately, not really the context of our question but the context of the agent. We have to equip the agent with what ought to be what Sartre would call pre-reflexive self-awareness, an understanding of where one is situated in the world and what’s around it. It’s not, or at least not just, about us telling the agent the context of our ask. It’s about us having to tell the agent the context of its own existence, its own capabilities, its own environment. We’re forcing humans to herd cats for systems that should be capable of finding and sharing knowledge autonomously. Every context engineering solution is a monument to this missing infrastructure.</p>
</section>
<section id="what-next-may-come" class="level2">
<h2 class="anchored" data-anchor-id="what-next-may-come">What next may come</h2>
<p>What we need are digital agoras – spaces where AI agents can discover capabilities, share information, and coordinate without needing this human intermediacy. That’s not because human oversight is bad, but because true agency requires the ability to discover and evaluate information independently. MCP and A2A represent tentative steps in this direction, but they still ultimately rely on humans wiring up the connections. The promise of agentic AI is that this would no longer be needed, that agents could exercise a kind of <em>Auftragstaktik</em> mindset of inferring these needs from some sort of reaching-out and finding adequate resources the way buyers find merchants in the marketplace. There’s, then, nothing ‘wrong’ with context engineering <em>per se</em>. What would however be a mistake is to consider it a destination, rather than a transitional phase. Every talk on context engineering, every new ‘supertool’, every clever prompting technique is progress – that we shouldn’t, however, let become procrastination.</p>
<p>When context engineering becomes unnecessary, we’ll know our agents have finally grown up. The path forward isn’t through better context engineering, but through making it obsolete. Digital agoras, agent discovery protocols, emergent coordination mechanisms: these aren’t nice-to-haves, they are the <em>sine qua nons</em> for genuine AI agency. Every moment we spend perfecting context engineering is a moment we’re not spending on building the infrastructure that would make it unnecessary.</p>
<p>At the end of the day, context engineering is our confession, written in code and infrastructure, that we haven’t yet achieved what we claimed. It illuminates the partial success of agents: success, because agentic AI is tremendously powerful, but partially so, because it needs us to coordinate for them. So while context engineering will tide us over for a while the way prompt engineering has done for its hour, we shouldn’t forget that it’s a band-aid, no more. We should welcome context engineering… while awaiting the day we can finally retire it.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {The {Snowmobile} {Symptom}},
  date = {2025-07-06},
  url = {https://chrisvoncsefalvay.com/posts/context-engineering/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“The Snowmobile Symptom.”</span> <a href="https://chrisvoncsefalvay.com/posts/context-engineering/">https://chrisvoncsefalvay.com/posts/context-engineering/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>context engineering</category>
  <guid>https://chrisvoncsefalvay.com/posts/context-engineering/</guid>
  <pubDate>Sun, 06 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/context-engineering/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Just noise in the neurons</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>“Give me one example,’’ Alan said.<br>
”Of a noncomputable function that a human can do, and a Turing machine can’t?’’<br>
“Yes. And don’t give me any sentimental nonsense about creativity. I believe that a Universal Turing Machine could show behaviors that we would construe as creative.’’”Well, I don’t know then… I’ll try to keep my eye out for that kind of thing in the future.’’</p>
<p>But later, as they were riding back towards Princeton, he said, “What about dreams?’’<br>
”Like those angels in Virginia?’’<br>
“I guess so.’’<br>
”Just noise in the neurons, Lawrence.’’<br>
“Also I dreamed last night that a zeppelin was burning.’’</p>
<p>– Neal Stephenson, <em>Cryptonomicon</em></p>
</blockquote>
<p>There’s a pervasive problem with semantics in artificial intelligence. It’s present at the creation – the term itself characterises the subject as a man-made simulacrum of something ‘natural’ the way we speak of artificial flavourings and artificial rubber. By necessity, the constructs we call artificial intelligences have never been treated (at least semantically) as first class citizens, but always as analogies to some typically biological construct. Most of the time, we can get past our inability to regard AI as <em>sui generis</em> and not merely a faint echo of the flesh.</p>
<p>This is about one of the times when that’s not quite the case.</p>
<p>I am, of course, aware that I am risking perhaps justified derision for being ‘hung up on semantics’, but semantics matters. Semantics are our human handles on notions, often abstract ones. Just as an unwieldy or ill-placed handle will make lifting a box unduly onerous, bad semantics makes manipulating those cognitive constructs difficult, impossible or error-prone. Which is why I continue to be willing to spill ink on our unhelpful tendency to rely on neuropathological metaphors for the mistakes of generative AI.</p>
<p>These models have a notorious habit of producing false yet plausible-sounding information. It is a property so inherent in stochastic generativity that the terminology of hallucinations itself comes not from LLMs, where the expression became ubiquitous, but in fact – best I can tell – from a paper by Baker and Kanade on superresolution (upscaling) of faces <span class="citation" data-cites="840616">(see Baker and Kanade 2000)</span>. Neuroscientists and psychologists often ask where hallucinations come from – Baker and Kanade appear to ask where the pixels generated in upscaling come from, and conclude that they must originate in a sort of hallucination.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="ref-840616" class="csl-entry">
Baker, S., and T. Kanade. 2000. <span>‘Hallucinating Faces’</span>. In <em>Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)</em>, 83–88. <a href="https://doi.org/10.1109/AFGR.2000.840616">https://doi.org/10.1109/AFGR.2000.840616</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Why did it take so long to ask the question? Your typical upscaling algorithm also generates new pixels, but these are essentially deterministic functions of the input pixels. The simplest of these, of course, is interpolation, where every new pixel is a deterministically obtained function of the originating information. Say, for bicubic interpolation over the unit square, <img src="https://latex.codecogs.com/png.latex?%20f(x,y)%20=%20%5Csum_%7Bi=0%7D%5E3%20%5Csum_%7Bj=0%7D%5E3%20a_%7Bij%7D%20x%5Ei%20y%5Ej%20"> where the <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the matrix of coefficients of the interpolating polynomial. This becomes all rather different once we are no longer dealing with essentially squishing the outputs through a deterministic interpolation but through a stochastically applied learned function.</p></div></div><p>I see three principal problems with the metaphor of neuropathology, specifically that of hallucinations, in AI.</p>
<ul>
<li>It’s a category error. LLMs have no perception, embodiment or conscious experience. They cannot have an abnormal perceptive-conscious experience because they are incapable of having that type of experience in the first place.</li>
<li>It invites confusion about what actually happens when hallucinations occur – both in humans and in LLMs.</li>
<li>It ascribes either a cognitive defect to LLMs, or some sort of semi-intentional failure.<sup>2</sup></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;It’s probably worth noting that the alternative term, ‘confabulation’, is just as bad. Depending on whether we consider it a colloquial term (for, essentially, lying) or a technical term (for the reification of cognitive dysfunction through making up something inconsistent with objective reality), it suffers from the same flaws, if not worse.</p></div></div><section id="the-category-error-of-hallucination" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-category-error-of-hallucination">The category error of hallucination</h2>
<p>Primarily, we consider hallucination to be an abnormal perceptive-conscious experience arising from a defect in perception (such as phosphenes), consciousness (being high as balls), processing (e.g.&nbsp;Anton’s blindness) or a combination. The core element in most definitions seems to boil down to conscious perception of things that just aren’t there. There are, all things considered, two major flavours of hallucination. The first category is what I shall refer to, for lack of a better term, as <em>additive</em>: there is an influence that results in an experience or sensation that supervenes the normal – take drugs, see things. The second category is perhaps more interesting, because it is the one that is most often discussed in the context of AI. This is the category of <em>substitutive</em> hallucinations, where the hallucination is a substitution for the normal experience that is almost forced by the brain’s insistence on gestalt and reification. Something is missing, and the brain ‘fills in the gaps’. This is, essentially, arguably not a qualitatively abnormal experience but a quantitatively excessive manifestation of our brain’s normal reification tendencies. Even if we only see the front third or so of a car poking out from behind a building, we understand that in all likelihood there’s more to it than we cannot see, and our mind’s eye can imagine fairly well what else is there (indeed, most people will guess relatively accurately where the rest of the vehicle ends). The phenomena that are discussed as substitutive hallucinations stretch this to an extreme. Perhaps the most extreme yet most fascinating example is Anton-Babinski syndrome <span class="citation" data-cites="Forde_Wallesch_2003">(see Forde and Wallesch 2003)</span>, where a fully fledged visual perceptual world is created in the presence of profound (typically cortical) blindness.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Forde_Wallesch_2003" class="csl-entry">
Forde, Emer M. E., and Claus-W. Wallesch. 2003. <span>‘<span>“Mind-Blind for Blindness”</span>: A Psychological Review of Anton’s Syndrome’</span>. In <em>Classic Cases in Neuropsychology, Volume II</em>, 23. Psychology Press.
</div></div><p>But, of course, that’s not really an accurate description of what’s transpiring when an LLM makes up some fascinating facts about a subject that it knows nothing about. Calling that a hallucination implies the AI has something analogous to senses and/or an internal world of experience, which happens to be defective. This is fundamentally misleading. Large language models do not perceive the world at all – they have no eyes to see, no ears to hear and no mental states to experience as a result, had they those perceptions in the first place. If a hallucination is seeing something that isn’t there, these models exist in a space in which there isn’t a “there” to mistakenly sense. When an LLM produces an unfounded statement, it isn’t experiencing some complex neurocognitive phenomenon. Not, anyway, does it experience anything that is more complex, mystical or creative than a regression model deviating from a correct answer or a classifier returning an incorrect classifications. We actually have a word for those circumstances. It’s called <em>being wrong</em>, and is about as mysterious as dish soap.</p>
<p>LLMs lack consciousness or embodiment. They don’t have an inner mental state or subjective awareness that could be led astray. Emily Bender, with whom I agree about once every decade or so, is entirely correct on this point:</p>
<p></p><div id="tweet-17972"></div><script>tweet={"url":"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472","author_name":"@emilymbender.bsky.social","author_url":"https:\/\/twitter.com\/emilymbender","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EAnd let&#39;s reflect for a moment on how they phrased their disclaimer, shall we? &quot;Hallucinate&quot; is a terrible word choice here, suggesting as it does that the language model has *experiences* and *perceives things*. \u003Cbr\u003E\u003Cbr\u003E&gt;&gt; \u003Ca href=\"https:\/\/t.co\/oIgCZYOnSM\"\u003Epic.twitter.com\/oIgCZYOnSM\u003C\/a\u003E\u003C\/p\u003E&mdash; @emilymbender.bsky.social (@emilymbender) \u003Ca href=\"https:\/\/twitter.com\/emilymbender\/status\/1592992842976489472?ref_src=twsrc%5Etfw\"\u003ENovember 16, 2022\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-17972").innerHTML = tweet["html"];</script><p></p>
<p>When we get down to it, an LLM doesn’t perceive or experience. It infers, specifically it infers a token sequence that is a relatively good sampled approximation of autoregressive conditional probabilities of tokens. Or, put in a simpler way, it puts tokens next to each other to minimise a loss function defined in relation to a learned conditional probability of each token w.r.t the sequence of the aforegoing tokens. There’s no tiny chess prodigy hiding inside this Mechanical Turk.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/image.png" class="img-fluid figure-img"></p>
<figcaption>Wolfgang von Kempelen’s Mechanical Turk – or at least what his debunker, Joseph Friedrich Freiherr von Racknitz thought it looked like. He was wrong about the details, correct about the principle.</figcaption>
</figure>
</div>
</div></div><p>Calling an LLM’s fabrications hallucinations therefore amounts to a category error: it imports terminology from human cognition and neuropathology into a domain where it has nothing to do. The model isn’t seeing pink elephants. It’s just guessing a sequence of words, badly as it happens.</p>
<p>Anthropomorphising the flaws of AI anthropomorphises their faculties. It is this logical implication that many seem to ignore: they may have little difficulty talking about hallucinations when they would hardly endorse the notion that there’s a consciousness residing in the 120 gigs of <code>safetensor</code> weights you just downloaded. Hardly anyone who speaks of AI hallucinations comfortably would attach much moral significance, if any, to <code>rm -rf</code>-ing a model, yet strict consistency would impel them to regard the latter as extinguishing a consciousness.</p>
<p>No matter how much we attempt to palliate the situation by emphasising how it’s ‘just a metaphor’, it’s never really <em>just</em> anything. When we rely on this lazy analogisation with neuropathology, we risk ascribing to these systems a kind of perceptual experience that can go wrong the way perceptual experiences do. Unlike a human brain, an LLM doesn’t construct a rich model of the world that occasionally diverges from reality. It only has a mathematical abstraction of its learned conditional probabilities. To treat its output errors as if they were analogous to a human’s neurological misfires is not only technically incorrect, it also muddies the waters about what such models do and don’t.</p>
</section>
<section id="the-analogy-and-the-mechanism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-analogy-and-the-mechanism">The analogy and the mechanism</h2>
<p>The second problem is that using the metaphor of hallucinations implies ideas about what is going on in an LLM, both in normal and failure modes. When we talk about an LLM ‘hallucinating’, we unwittingly carry over a whole stack of those assumptions – imagining the model as a mind experiencing a very specific kind of (mis)experience, when it isn’t and doesn’t. The metaphor of hallucinations might feel like a useful analogy, but it obscures more than clarifies. In truth, the model isn’t trying (and failing) to faithfully report on reality. It’s producing output that is stochastically correct, i.e.&nbsp;it replicates reality with relatively decent accuracy as the number of tries converges to infinity. Given the nature of that distribution, some of those attempts at replicating reality will be off. The hallucination metaphor invites us to imagine there is a kind of inner experience going on all the time, a logical entailment of asserting that hallucinations are when that experience goes wrong. Ultimately, this is a misleading cognitive narrative around the technology, one that does not match the mechanistic reality of what is, basically, autocomplete on steroids.</p>
<p>Unlike you and me,<sup>3</sup> a generative model has no concept of truth or falsehood in its programming, not beyond minimising its loss function (which does not, incidentally, claim to be true – indeed, we intentionally train ). It’s not attempting to state facts and then failing spectacularly when it ‘hallucinates’. It’s always doing the same thing – generating plausible text. By using a term like hallucination, we spin a tale where the AI is a quasi-intelligent being with beliefs about the world, occasionally slipping on a cognitive-perceptual banana peel. The model isn’t trying to convey anything at all, let alone something it believes or believes to be true. It has no beliefs or an understanding of truth.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Assuming here that you aren’t OpenAI scraping my website, in which case <code>$deity</code> have mercy on your transformer blocks.</p></div></div><p>Ultimately, this feeds into a kind of hype-by-elision in which both capabilities and limitations are exaggerated. To the commentariat, AI is both ape and angel, Skynet and a barely capable moron that can’t count the number of ’r’s in <em>strawberry</em>, the thing that will steal your jobs and the thing that can’t figure out addition. All of that, of course, sells. This is, ultimately, a kind of Reefer Madness for the AI age, lurid prose in bright letters about a technology that is powerful but ultimately logical, complex but also mundane, and most of all, capable of being understood and analysed if one forgoes the Scylla and Charybdis of over- and underestimating it.</p>
</section>
<section id="not-sick-just-wrong-and-maybe-not-even-that" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="not-sick-just-wrong-and-maybe-not-even-that">Not sick, just wrong (and maybe not even that)</h2>
<p>The problem with the metaphor of hallucinations is that it invites us to see a pathology and respond accordingly. We’re all wrong from time to time. How we assess people being wrong in the presence of a pathology is crucial to our moral understanding. Consider the legal definition of insanity, known to everyone who had to suffer through first-year criminal law as the <em>M’Naghten</em> formula: insanity is when a defect of reason results in a person either not knowing what they’re doing (being ‘wrong’ as to their actions) or not knowing right from wrong (being ‘wrong’ as to the nature of their actions). We relieve people of legal, and sometimes moral, responsibility for their actions in that situation because of the pathology. We treat it, essentially, as a moral (albeit not legal) break in the chain of responsibility.</p>
<p>The model is not an agent (in the moral-human sense) with goals or a will, nor is it a patient suffering a perceptual and/or cognitive defect. It’s a computational system following its training objective, which is to produce a set of tokens that reflect a learned conditional probability. And that’s the essence of why the terminology of pathology is wrong: we might not like its output, it might not comport with the ground truth, but on its own terms, a ‘hallucinated’ token is not intrinsically ‘wrong’ or ‘pathological’. It is a necessary feature of the architecture. There’s nothing ‘wrong’, internally, with a hallucinating model. There are no integer overflows or computational errors, you didn’t mess up your code, the model hasn’t been wrongly trained or ran on compromised hardware. Hallucinations are the price we pay for the stochasticity of a model.</p>
<p>The other time I sort of agreed with Emily Bender was when she called LLMs ‘stochastic parrots’, and her comment here is entirely accurate. A parrot has no more understanding of the meaning of the words it utters than an LLM has. The parrot utters them because of, well, reinforcement learning: repeat a sequence of tokens (which are in this case acoustic) and get a reward. That’s actually exactly how we train LLMs. They’re not trained to pursue truth, they are trained to land somewhere in truth’s vicinity at best. Notably, LLMs are not in any way ‘told’ the truth. We train LLMs on large corpora, and we expect that they contain a reflection of reality. If the corpus is relatively well selected, from a source that has its own way of ensuring its content comports with reality, then the conditional probabilities our model learns will also largely align with reality. But to the model, it’s all the same, at least absent specific measures like RLHF to weed out nonsense. If one were to contaminate basic corpora ubiquitously trusted as reliable sources,<sup>4</sup> models would replicate that. You can train models on self-contradictions, on nonsense prose, on propaganda, you name it – as long as token follows token, you have what it takes. What this ultimately means is that such models exist in a space of learned relative probabilities, no more, no less. They mathematically faithfully replicate those probabilities in generating a sequence of tokens. If the result does not comport with our consensus understanding of reality, that’s not some peculiar pathology internal to the model. It’s the model functioning as expected. It’s not sick, and at least on its own terms, not even wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Which is why the controversy around what ends up on Wikipedia is so relevant, and such an ignored undercurrent of the discourse around the values reflected – not held, reflected! – by LLMs.</p></div></div></section>
<section id="coda" class="level2">
<h2 class="anchored" data-anchor-id="coda">Coda</h2>
<p>In the end, metaphors matter. Like a lantern, they may illuminate our way, or they may blind us. The metaphor of neuropsychiatric pathology when one fundamentally means to say ‘wrong’ has created a kind of mystery theatre (devoid, largely, of science) that has perhaps been unwittingly spurred by, and in turn perpetuated, an air of mystery around AI. Some of it is just clumsy phrasing – Sam Altman was criticised for <a href="https://www.marketwatch.com/story/openais-sam-altman-tells-salesforces-marc-benioff-that-ai-hallucinations-are-more-feature-than-bug-1c035c52">a comment that seemed to imply that hallucinations are somehow intrinsically tied to a kind of creativity that we appreciate in LLMs</a>, but what he meant is I think closer to the fact that stochastic models necessarily yield those occasional samples from a little bit off center of the probability distribution that result in what we fancifully came to call hallucinations.</p>
<p>Hallucinations, bona fide hallucinations in humans of various levels of neuropsychiatric competence, induced or otherwise, have a rich cultural history. From the pythia of Delphi through the ergot-induced visions of mediaeval witches, saints and heretics to the paintings of <a href="https://en.wikipedia.org/wiki/Louis_Wain">Louis Wain</a> and the fiction of Hunter S. Thompson, they have always been a part of the human experience, albeit very much an extraordinary part (and often, one that took more than it gave – there are few happy endings in the civilisational story of perceiving things that aren’t there). But they are very much a peculiarly <em>human</em> experience. When we draw this tenuous metaphor, we do a disservice to AI, a disservice to those who live with hallucinations and the often quite tragic and marginalising pathologies that cause them, and not least a disservice to our own attempts at understanding how LLMs work, and how sometimes that results in these extraordinary phenomena.</p>
<p>And maybe, past all mystery and metaphor, we may just call them <em>occasionally wrong</em>.</p>
<hr>
<p>I am indebted to my colleagues at HCLTech for the discussions that led to this post. All errors and omissions are mine.</p>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Just Noise in the Neurons},
  date = {2025-06-27},
  url = {https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Just Noise in the Neurons.”</span>
June 27, 2025. <a href="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/">https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>agents</category>
  <category>neuroscience</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/</guid>
  <pubDate>Fri, 27 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/noise-in-the-neurons/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>After agents, part 2 – Agents and the Agora</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/agents-agora/</link>
  <description><![CDATA[ 




<p>If you spend any time on LinkedIn, it’s almost a certainty that you have come across a bevy of alleged ‘agentic AI architectures’. They all look something like this:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    %% Define the main nodes
    n1([Start])
    n2[[Manager Agent]]
    n3((End))

    %% Subgraph 1: Data Ingestion
    subgraph s1["Data Ingestion"]
        n11["Ingestion Manager"]
        n12["SQL Interactor Agent"]
        n13["RAG Agent"]
        n11 --&gt; n12
        n11 --&gt; n13
    end

    %% Subgraph 2: Analysis
    subgraph s2["Analysis"]
        n21["Analysis Manager"]
        n22["Analyst Agent"]
        n21 --&gt; n22
    end

    %% Subgraph 3: Reporting
    subgraph s3["Reporting"]
        n33["Reporting Manager"]
        n34["Report Writing Agent"]
        n33 --&gt; n34
    end

    %% Subgraph 4: Some Other Stuff
    subgraph s4["Some Other Stuff"]
        n41["Foo Manager"]
        n42["Bar Agent"]
        n43["Baz Agent"]
        n41 --&gt; n42 &amp; n43
    end

    %% Main flow
    n1 --&gt; n2
    n2 --&gt; s1 &amp; s2 &amp; s3 &amp; s4
    s3 --&gt; n3
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>All very neat, but the audience might be forgiven for asking what exactly is agentic about this, except for relabeling subprocesses in what is a run-of-the-mill RPA workflow as ‘agents’. And the audience is, this once, perfectly right. This is agentic AI in its most limited sense – and that limited sense is in many ways a result of the semantics we chose to adopt to reason about agents. For while we call it <em>agentic AI</em>, it’s not, actually, the agents that matter: it’s how they are structured. It is this governed agentic connectome, which I have come to call the Agora, that holds the power of agentic AI – and which is almost universally neglected.</p>
<section id="beyond-agents-to-the-agora" class="level1 page-columns page-full">
<h1>Beyond agents, to the Agora</h1>
<p>This narrow perspective is essentially a rebranding of what any decently designed application does – self-contained pieces of code passing information to one another – embellished with the agentic buzzword <em>du jour</em>, rendering what we all have been doing for the last few decades ever so much more VC-friendly. It misses the true key to the power of agentic systems. In a <a href="https://chrisvoncsefalvay.com/posts/after-agents/">previous post</a>, I reflected on the need to adopt an ecosystem thinking about agents, to consider their strength in creating complexity through interconnectedness. At the moment, our epistemic perspective on agents is intrinsically tied to, and defined by, the <em>what</em>, i.e.&nbsp;the agents themselves. Much of the time, it fails to take account of what matters vastly more, namely how those agents relate to each other – the <em>how</em> of agents. Just as we understand that the power of the human brain does not derive from a bunch of neurons in one place but their interconnectedness, we need to come to understand that agents are the least interesting part of agentic AI. In the end, in any complex system, it’s the connections that matter more than what is doing the connecting.</p>
<p>What should be agentic AI’s focus, then, is the space in which those agents can interact. Few of these neat hierarchical frameworks that are now touted as ‘agentic’ on LinkedIn envisage any meaningful interaction beyond manager agents bossing around single-functionality executor agents. This fits very well with existing software development paradigms, but has a hard limitation: the complexity of the resulting system will reach just as far as the developer has had time and energy to wire up various components. Even if it’s a rat’s nest of agent spaghetti, this complexity will be limited in at least two ways. It will, for one, be limited by the static, pre-defined nature of the framework: what is once defined remains set in stone. If no connections are manually made <em>a priori</em>, processes and agents live separate lives. More concerning, however, is the epistemic limitation: if we have to <em>a priori</em> define the agentic structure, we are stuck with the known knowns and perhaps the known unknowns. We are trying to tie reality to Procrustes’ bed, except it’s us who will end up a foot short in the end.</p>
<p>The alternative focus, then, should be on creating agentic frameworks that focus on a governed space where agents can engage with each other – the space I chose to call the Agora, in analogy with the city-square of Ancient Greece where merchants, artisans, philosophers, politicians and citizens got to interact and form connections. The Parthenon may have been the most glorious structure of Athens, the Pnyx might have been the seat of the Assembly’s power, the Areopagus might have been where life and death was decided upon – but it was its agora that made Athens great.<sup>1</sup> The Agora of agentic AI holds the same promise: to act as a place of free interaction, within governed bounds, for our agents, unlocking the true power of the agentic perspective: emergence.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;To the point that people enjoyed the latter enough to forego participation in the former. Eventually, a bunch of slaves would have to roam the agora every time the Assembly was in session, carrying ropes with a red dye. Staining the garments of those who preferred the agora to the Assembly, the red dye identified them and made them liable to punishment.</p></div></div></section>
<section id="constructing-the-agora" class="level1">
<h1>Constructing the Agora</h1>
<p>There is, in fact, relatively little new in the concept of tackling complexity through self-organising emergence. Consider neural networks: what lends artificial neural networks their awesome power is that instead of having to manually code stacks of filter banks, we use – typically – backpropagation to condition a large number of highly connected filters to minimise a loss function (i.e.&nbsp;to make the resulting model more accurate). Nobody would propose to manually define each filter in a neural network <em>a priori</em>: why, then, are we still talking about deterministically defined hierarchies and flows of agents instead of allowing agents to organise themselves and control that process through some outcome metric?</p>
<p>One aspect of this is that the Agora is more than a collection of agents idly milling around. Crucially, we need to provide three key elements:</p>
<ul>
<li>A <strong>discovery framework</strong>: agents must be able to discover other agents, and what they can do, so as to be able to identify other agents that they may recruit to assist them in their goal – this would typically take place using a registry where agents ‘enroll’ their profiles and which other agents can then access.</li>
<li>An <strong>interaction framework</strong>: agents must be able to communicate with each other, which requires both a message-passing standard (i.e.&nbsp;a minimum interface of how one agent may programmatically call another), and a suitable implementation (i.e.&nbsp;the message broker service that implements this standard).</li>
<li>A <strong>governance framework</strong>: the governance of the Agora relies on the fact that not all agents may register themselves to the agent registry. Who may, and who may not, participate in the Agora determines and governs the overall process. Equally, the fact that we do not want our agents’ interaction to be entirely deterministic does not mean we want it to be entirely ungoverned. Various policies can be used to condition where connections can, and cannot, be made: some agents may be barred from creating certain direct connections, for instance it should be possible to specify that no agent should be able to directly return data to the user without having to pass it through a guardrail agent. The agora was a place of free interaction, but not of lawlessness – the same goes for the Agora of our AI agents.</li>
</ul>
<p>The Agora is not an ‘enhancement’ of agentic AI – it is what agentic AI <em>is</em>, or at least ought to be. It is what allows the greatest strength, i.e.&nbsp;self-organised emergence, of AI agents to unfold in a governed, controlled domain. And perhaps quite perplexingly, it is probably going to be easier to implement than most deterministic agentic structures. Certainly it is going to be more economical to allow agents to reason through how to solve their problems and discover the resources they need within the Agora, recruiting them as needed and releasing them once done, than having to think through the process <em>a priori</em>. A solid agentic framework can accommodate the fact that the world is complex, and organise itself to cater for the unexpected (within, of course, its means – that is, within what agents are available to the Agora).</p>
</section>
<section id="the-agoric-shift" class="level1 page-columns page-full">
<h1>The Agoric Shift</h1>
<p>This, then, is where I personally see our next challenge – both from an epistemic-conceptual perspective, which will call for us to rethink agentic AI in a way that perhaps focuses less on agents and more on their interconnectedness (and ways to facilitate it) and from an engineering perspective, which will require us to implement the tools and structures it will take to make this interconnectedness happen. Neither challenge is trivial. There is a pervasive trend to attempt to simply take deterministic RPA-like processes and workflows, rename them agents and watch the money roll in. The conceptual challenge thus is to illuminate what agentic AI properly so called brings to the table – the promise of emergence.</p>
<p>From a technical perspective, there is as of yet no universal way for agents to interact. Anthropic’s <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a> is a valiant attempt at beginning to set some standards in that regard, but the reality on the ground is that most agent implementations have relied on exaptation, and for inter-process communication in the internet era, that means REST for the most part. This may support deterministic designs with modest needs for interaction, but the Agora has need for other structures, too, such as a model registry. This raises a wealth of coordination problems that need to be tackled before we can let our agents go to (the) town (square).</p>
<p>Yet this shift is where we unlock the power of modern AI. In 2023, <a href="https://chrisvoncsefalvay.com/posts/team-of-rivals/">I predicted the rise of agentic systems</a> as a way to unleash the potential of LLMs by making them interact with each other in various roles. The Agoric Shift is the consummation of this idea: agentic systems where such interactions arise not from predefined workflows and patterns but from self-organising assemblies of agents<sup>2</sup> – the point where we finally stop trying to painstakingly orchestrate every step of our agents’ interactions and instead build a vibrant Agora for them to roam, collaborate and perhaps even surprise us.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Which, by the way, may include humans. There’s no reason why we shouldn’t conceive of ‘humans in the loop’ not as a superordinate stage that comes after agentic AI has done its part, but simply another agent.</p></div></div><p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {After Agents, Part 2 -\/- {Agents} and the {Agora}},
  date = {2025-02-10},
  url = {https://chrisvoncsefalvay.com/posts/agents-agora/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“After Agents, Part 2 -- Agents and the
Agora.”</span> <a href="https://chrisvoncsefalvay.com/posts/agents-agora/">https://chrisvoncsefalvay.com/posts/agents-agora/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/agents-agora/</guid>
  <pubDate>Mon, 10 Feb 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/agents-agora/B20C419F-6819-49E1-B80E-11B95A28D6CC.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Seatbelts and straitjackets</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</link>
  <description><![CDATA[ 




<p>DeepSeek has been grabbing headlines in AI circles lately, showing up everywhere from Discord servers full of ML enthusiasts to LinkedIn posts where “thought leaders” tag each other in endless threads. <a href="https://www.cnbc.com/amp/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">CNBC even ran a piece</a> framing it as the latest challenge to American AI hegemony, and soon the story emerged painting DeepSeek as the scrappy competitor to OpenAI, with a heart-warming underdog narrative about a small quant shop in the PRC that decided, on a whim, to open source their fancy new large language model. Except, as any cynic will tell you, if a story seems too neat, it probably is.</p>
<div id="fig-hxiao" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/hxiao.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: It’s basically <em>The Astronaut Farmer</em>, but with LLMs and jackboots.
</figcaption>
</figure>
</div>
<p>Of course, I had to go and try it. And I’ve found something rather interesting – albeit unsurprising. In a dictatorship, truth can be adjusted. It is a negotiable commodity. And if the facts do not support the regime’s truth… well, then it can be suppressed. When simply asked about something inconvenient to the CCP, we get a very expected answer.</p>
<div id="fig-ba-sing-se" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/ba_sing_se.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: There’s no war in Ba Sing Se.
</figcaption>
</figure>
</div>
<p>Where it gets interesting is when you enable the search functionality, which – oddly enough – performs a search beyond the Great Firewall.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The Great Firewall actually cuts both ways. Its main intent might be to keep China’s netizens from accessing the ‘free’ internet, but a good deal of the effort is also to keep users on this side of the Wall from having access to whatever slight semblance of occasional uncontrolled discourse there is on the Chinese internet. This aspect is rather often ignored by Western commentators unfamiliar with Chinese internet culture.</p></div></div><div id="fig-tams" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/tams.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: This is where it gets interesting.
</figcaption>
</figure>
</div>
<p>Ask it a question about the PRC’s track record on, say, human rights or historical controversies, and it would start to spill the beans – then abruptly slam on the brakes, invoking what we in the AI business call a ‘guardrail’, a kind of safety mechanism that protects users from undesirable outputs. It’s as if DeepSeek half-read you a classified file, then went “actually, never mind” and pretended you never asked. Who needs a sense of free inquiry when you can enjoy curated silence?</p>
<p>And there’s the real scandal about DeepSeek. It’s not that it’s a CCP mouthpiece. It’s not even that the CCP managed to lobotomise a machine learning model then put it out into the world as ‘open source’, making an absolute and utter mockery of that idea. The real scandal is that it also managed to pervert the notion of responsible AI and guardrails in the process, abusing what is meant to be a seatbelt, turning it into a straitjacket.</p>
<section id="whose-seatbelt-is-it-anyway" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whose-seatbelt-is-it-anyway">Whose Seatbelt Is It Anyway</h2>
<p>Guardrails are meant to filter out harmful or illegal content. Sounds decent enough, right? Nobody wants a chatbot spitting out tips for building bombs or fomenting genocidal ideation. But guardrails also make a convenient muzzle when the people setting them have a vested interest in what can and cannot be said.</p>
<p>DeepSeek demonstrates this with such awkward flamboyance it almost feels like performance art. It shows that it <em>knows</em> certain inconvenient truths – only to pull the plug mid-sentence. There’s an inconvenient truth here: what can keep you safe from, say, the recipe for mustard gas (my pet test case for guardrails) can, in the wrong hands, keep you “safe” from factual history. When the powers behind an LLM use guardrails to stifle legitimate discourse, they invert the entire idea of “safety” –suddenly it’s about state or corporate safety, not user empowerment.</p>
<p>Which, of course, makes the mythology about DeepSeek being just a jolly side project even more ridiculous.<sup>2</sup> Training a massive model is not a trivial affair, financially or otherwise. You need loads of data, advanced expertise, technical infrastructure and computing power to pull this off. Doing so in the PRC, by a regulated company (which inevitably means CCP presence inside the company’s decision-making apparatus), means governmental oversight at best, direct involvement at worst, and my money is firmly on the latter.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Because one might need to be actively working in this field to understand how silly the assertion is that training a model of this size and accuracy, even with the clever RL only trick, could be anything other than a large scale industrial endeavour.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Never mind that they can run it for free, at pretty good (OpenAI-defying!) levels of performance. With the volume of press they’ve been getting, load must be incredible, yet I have encountered no performance degradation or outages, nor have any of the other researchers with whom I have had the pleasure to discuss DeepSeek these last few days. This is not running on someone’s home lab and a bunch of spare GPUs that aren’t doing whatever quants do with GPUs. Just operating this system at its current performance is an industrial level task, no matter how clever the reinforcement learning trick used to improve model efficiency might be.</p></div></div><p>I hope I have illustrated why the “quant side project” explanation is about as plausible as me spontaneously building a passenger jet in my garage. Sure, it is not impossible, but it sits ill with reality.<sup>3</sup> The moment you notice state-friendly guardrails are baked into the system, the notion that nobody official was meddling starts to crumble.</p>
</section>
<section id="souring-trust-and-fueling-polarisation" class="level2">
<h2 class="anchored" data-anchor-id="souring-trust-and-fueling-polarisation">Souring trust and fueling polarisation</h2>
<p>What really stings is how stunts like this undermine trust in AI more broadly. If a single model can appear to openly discuss a contentious issue, then suddenly lock down as if a party censor is peering over its shoulder, that sets off alarm bells. It is a short leap from there to suspecting <em>all</em> publicly released LLMs might be covert mouthpieces for whichever power sponsored them. The innocent curiosity that once framed AI as a neutral tool becomes overshadowed by paranoia.</p>
<p>This also amplifies the kind of polarisation we see in geopolitics. When big states can afford to spin up models that quietly nudge narratives in a particular direction, we lose the last remnants of hope that AI might be above the political fray. It becomes yet another field where states compete to drown one another in carefully curated content or half-truths, with users stuck in the middle.</p>
<p>DeepSeek drives home a new brand of cynicism: “If a fancy new LLM appears, can we trust it?” The official line might be “We open-sourced it! Nothing to hide!” But if the training data was curated, or if shadowy “alignment” policies are embedded, the model can still be a Trojan horse. Once that suspicion sets in, good luck convincing people to use AI tools for earnest, balanced exploration.</p>
<p>The answer, of course, is that one shouldn’t trust anything, or at least verify. Not only is that avenue generally foreclosed to the lay end user, it is not even really afforded to those with the means and knowledge. Open sourcing an LLM is not the same as open sourcing human-readable code. The sole reason DeepSeek’s internal flaws are so evident is that we know where to look. Were that not the case, or had the developers (and their minders from what likely is the PRC’s Ministry for State Security aka MSS) been any more subtle, we would not know what biases we have brought under our roof. The usual “don’t trust me, bro” disclaimers are about as useful here as the “not cleared by the FDA” notices on snake oil. It’s still, at the end of the day, making promises. It still, like snake oil, fails to meet them.</p>
</section>
<section id="the-epistemic-of-tools-vs.-information" class="level2">
<h2 class="anchored" data-anchor-id="the-epistemic-of-tools-vs.-information">The epistemic of tools vs.&nbsp;information</h2>
<p>We tend to treat large language models like glorified chat apps, forgetting that they amass vast amounts of textual knowledge. They do more than just parse grammar; they internalise cultural, historical and political contexts. When external gatekeepers meddle with the training set or impose ideological constraints, the model will reflect that in its embedded worldview, something most of us forget even exists. If a user is unaware of those hidden constraints and accept answers at face value, they might never suspect how they are being manipulated, be it as hamfisted as DeepSeek’s responses or wiser, subtler, more insidious bending of the truth that a slightly less <a href="[Stupid Evil - TV Tropes](https://tvtropes.org/pmwiki/pmwiki.php/Main/StupidEvil)">stupid evil</a> regime would have tried. We ultimately must disabuse ourselves of the notion that we’re dealing with unbiased, value-neutral tools, and consider LLMs what they are – information. And the moment guardrails come into the picture, any claim to being free of human bias goes off the table.</p>
<p>This isn’t an argument against guardrails altogether. On the contrary, we need some form of alignment to keep truly vile content at bay. But the question remains: <em>which</em> alignment and <em>whose</em> values? A possible way forward is an auditable chain-of-custody for model training, coupled with immutable model cards that detail the sources, curation processes and alignment methods. If a government or company demands specific guardrails for certain subjects, that fact should be clearly disclosed for all to see. But we have yet to see credible attempts at widespread use and popularisation of such technology. More ink was spilled on comparing DeepSeek with o1 than on the glaring issues presented by a model that one could, and should, expect to bear the fingerprints of one of the most repressive regimes on the planet.</p>
<p>DeepSeek is more than just a technological marvel or a threat to Western AI hegemony. It’s a reminder that alignment itself can be weaponized – particularly by entities with a history of suppressing information and controlling narratives. When alignment is used to conceal rather than safeguard, or to manipulate rather than protect, we’re staring down the business end of a propaganda pipeline disguised as advanced software.</p>
<p>It’s easy to argue users are responsible for checking the answers they get, but let’s be honest – propaganda works. We know it does. It works because by and large, people don’t check the information they consume. When millions of users worldwide rely on these models to explain historical or political content, subtle manipulations can shape public understanding in ways we might not even notice until it’s too late. AI might not have self-awareness, but it certainly does have the power to shape awareness in others.</p>
<p>And so, here we are, strapped in tight on the propaganda rollercoaster by a seatbelt supposedly meant to protect us from harm. In a sense, we are fortunate – because hopefully, this will spark the right kind of discussion in certain corners about the painful reality that there are some very human hands turning the knobs and levers of alignment and guardrails. And we <em>must</em> have this awkward, painful discussion, because the alternative is a world in which reality is dictated by the mightiest sponsor with a big enough GPU farm – and a bigger political agenda.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Seatbelts and Straitjackets},
  date = {2025-01-25},
  url = {https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Seatbelts and Straitjackets.”</span>
<a href="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/">https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>alignment</category>
  <category>ethics</category>
  <guid>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</guid>
  <pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Deja Vu, All Over Again.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</link>
  <description><![CDATA[ 




<p>The Department for Science, Innovation and Technology has just dropped its long-awaited <a href="https://assets.publishing.service.gov.uk/media/67851771f0528401055d2329/ai_opportunities_action_plan.pdf">AI Opportunities Action Plan</a>, a 50-page vision of how the UK government plans to guide us into an AI-powered future. The document, adorned with the obligatory glowing circuit board imagery that seems to be mandatory for any government AI publication, promises to transform Britain into an “AI superpower.” As someone who’s spent their career at the intersection of computational science and public policy, I can’t help but feel a familiar sense of déjà vu. We’ve been here before – just with different technology and different buzzwords.</p>
<section id="the-ghost-of-centralisation-past" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-ghost-of-centralisation-past">The Ghost of Centralisation Past</h2>
<p>Ah, government AI initiatives. Like watching your uncle try to explain TikTok at Christmas dinner – enthusiastic, well-meaning, but somehow missing the point entirely. The UK’s new AI Opportunities Action Plan, released this week with characteristic governmental fanfare, was supposed to inspire and plot the course for the UK’s ship on the stormy seas of AI. But really, it rather brings to mind that most delicious of historical failures: Project Cybersyn.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/D-W7mk4UcAA42hm.png" class="img-fluid figure-img"></p>
<figcaption>Swivel chairs!</figcaption>
</figure>
</div>
</div><div id="fn1"><p><sup>1</sup>&nbsp;One of history’s great tragedies is that the winds of change often sweep away lessons before they can fail to their fullest extent. The Allende regime’s fall sadly also brought with it the destruction of Cybersyn before its complete failure would come to be in full evidence.</p></div></div><p>For those who actually have a life and don’t regularly spend their evenings reading about Cold War technological follies, Cybersyn was Salvador Allende’s attempt to run Chile’s economy through the dark arts of the hype of the day – cybernetics –, a network of telex machines and a room that looked like the love child of Star Trek and Austin Powers. It had swivel chairs. It had buttons. It had everything except, well, actual functionality.<sup>1</sup> It was an example of what happens when you have a sensible idea (networking) in pursuit of a dubious goal (central planning) executed on the back of governmental fiat. Back then, the buzzword was ‘cybernetics’, and it was supposed to usher in an era of truly scientific central planning. We would soon all be waving goodbye to the inefficiencies of market economies thanks to… telexes. And big screens.</p>
<p>This pattern of centrally-planned technological innovation has repeated itself throughout history - from the Soviet Union’s OGAS project<sup>2</sup> to Japan’s Fifth Generation Computer Project. These initiatives share a common thread: they all failed spectacularly despite massive government backing. Innovation, like my attempts at Provençal cuisine, tends to work best when it’s organic and bottom-up, not when it’s directed from above by committee. You can’t will creativity or innovation into existence. Least of all can you do so by governmental fiat.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<a href="https://www.youtube.com/@Asianometry">asianometry</a> on YouTube has a <a href="https://www.youtube.com/watch?v=cLOD5f-q0as">wonderful video</a> on this.</p></div></div></section>
<section id="too-little-too-late-too-bureaucratic" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="too-little-too-late-too-bureaucratic">Too little, too late, too bureaucratic</h2>
<p>Fast forward to 2025, and we’re treated to Matt Clifford’s vision of how the UK government can shepherd us into an AI-powered future. The plan isn’t wrong, per se – it’s just adorably optimistic about the ability of government to direct technological evolution, or the beneficial effect of government getting involved in innovation. It’s like trying to herd cats, if the cats were quantum computers and the herders were Whitehall civil servants still struggling with Excel.</p>

<div class="no-row-height column-margin column-container"><div class="">
<hr>
<p><strong>Recipe</strong>: Coq au vin</p>
<ul>
<li>1 whole chicken, cut into pieces</li>
<li>750mL decent red wine</li>
<li>200g lardons</li>
<li>Pearl onions… lots of them. Two dozen or so.</li>
<li>500g mushrooms</li>
<li>2 carrots</li>
<li>Fresh thyme</li>
<li>Bay leaves</li>
</ul>
<p>Dunk the chicken in the wine and let rest overnight (patience is the name of the game). On the day, brown the lardons slowly in a Dutch oven. Remove, and reserve the fat. Brown the chicken pieces in the fat – avoid crowding your Dutch oven, work in batches if you have to. Sautee the vegetables, and return everything to the Dutch oven, including the chicken – and the wine. Cover and simmer on low-medium heat for 90 minutes. Thicken to taste with a beurre manié. Serve with a thick slab of a hearty, crusty bread.</p>
<hr>
</div></div><p>Let’s be real: The UK is already painfully late to this party. While we’re planning to plan about planning our AI infrastructure, OpenAI and Anthropic are training models that make GPT-4 look like a pocket calculator. The plan acknowledges we’re “the third largest AI market in the world” – which sounds impressive until you realize that in absolute terms, Britain’s contribution compared to the front runners (US and PRC) barely amounts to a rounding error.</p>
<p>The focus on government-directed innovation is particularly eyebrow-raising. The plan calls for establishing ‘AI Growth Zones’, expanding compute infrastructure and creating a new unit called UK Sovereign AI (because if you do not create more bureaucracy, you clearly aren’t doing this government thing well).<sup>3</sup> All potentially good stuff in theory, but premised on the notion that the government can effectively pick winners and shape the development of arguably the most dynamic technology since electricity. And, of course, the notion of ‘growth zones’ might strike one as rather odd considering that most of the work we do gets done in the cloud. I don’t need to be in a ‘growth zone’ to do my job. The extremely delocalised nature of software and AI development makes these echoes from the era of industrial parks and logistical efficiencies sound anachronistic at best, smacking of a misunderstanding of AI in practice at worst.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Interestingly, <a href="https://uksovereign.ai">uksovereign.ai</a> leads you to <a href="https://mindfoundry.ai">Mind Foundry</a>, a spin-out of my alma mater that seems to mainly cater for the defence industry.</p></div></div></section>
<section id="just-stay-out-of-the-way" class="level2">
<h2 class="anchored" data-anchor-id="just-stay-out-of-the-way">Just stay out of the way</h2>
<p>That said, there are some genuinely promising elements. The commitment to building AI infrastructure and expanding compute capacity is crucial - though one wonders if we might have thought about this back when compute was actually available to purchase. The focus on AI safety and ethics is also commendable, even if it feels a bit like installing guardrails after the car has already left the garage.</p>
<p>What’s missing is an acknowledgment that the most successful government interventions in technology have historically been indirect - creating the conditions for innovation rather than trying to micromanage it. DARPA gave us the internet by funding basic research and then getting out of the way. The UK’s own success stories - from ARM to DeepMind - emerged from an ecosystem that encouraged experimentation and risk-taking, not from government planning.</p>
<p>As someone who has spent time in both academia and industry, who has seen how innovation actually happens in the trenches of AI research, I can’t help but feel we’re missing the forest for the bureaucratically-approved, committee-selected trees. The UK has world-class AI talent, outstanding universities and a vibrant startup scene. What it needs isn’t more government steering – it needs the government to focus on removing obstacles and then getting out of the way. But hey, at least we’re not building a room full of swivel chairs. Though given some of the proposals in this plan, I wouldn’t rule it out for AI Action Plan 2.0.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Deja {Vu,} {All} {Over} {Again.}},
  date = {2025-01-16},
  url = {https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Deja Vu, All Over Again.”</span> <a href="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/">https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>governance</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</guid>
  <pubDate>Thu, 16 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>After agents</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/after-agents/</link>
  <description><![CDATA[ 




<p>When he first began his excavations at what is today Hisarlik in modern-day Türkiye, Heinrich Schliemann set out to find a single city – the city of Homer’s Iliad, a city many actually felt lay in the realms of fiction rather than any map he could lay his hands on. By the time excavations were over, Schliemann would find not one but nine cities, all built on top of each other.<sup>1</sup> In that, he found something relatively common – cities built on top of cities, each turning the last one into the foundation of the next.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Some of which he would blow up. Schliemann’s excavations were peak cowboy archaeology.</p></div></div><p>Technology is not much different. What Newton described as ‘standing on the shoulders of giants’ was an astute reflection of this fact. When I surveyed the landscape of LLMs in late 2023, I saw – and I was not alone in doing so! – that there was more to the practical application of LLMs than their most ubiquitous ‘low hanging fruit’ use case at the time – that is, chatbots and conversational interfaces. That notion, of course, became the agentic revolution that emerged as the most talked-about topic of 2024.</p>
<p>If your hot take for 2025 is that ‘agentic AI is going to dominate’, however, you have missed the train. The agentic revolution is over, done and accomplished. In my <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">wild-ass guessing of what 2025 may bring</a>, I tried to reflect as a leading theme on what I believe comes after agents – namely, systems of interaction. In this post, I hope to expand on that notion a little.</p>
<section id="agents-are-over-long-live-agents" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="agents-are-over-long-live-agents">Agents are over (long live agents!)</h2>
<p>Looking back at 2024, I have to laugh at how quickly the agentic revolution went from being seen as wild-eyed speculation to something almost embarrassingly obvious. I spent a good chunk of late 2023 explaining to sceptical audiences why autonomous AI agents were not just chatbots that could call APIs. I would spend most of the next year fielding calls from the same audiences to help develop a strategy for agentic AI. The whiplash-inducing speed of this transformation was stunning even by AI’s standards, where we seem to have moved into a 24-minute, rather than 24-hour, news cycle. What started as hacky demos and GitHub repos with more stars than working features evolved into agents casually writing production code, running research pipelines and tying all of this together into operational workflows that actually made sense. The real shift was not just in what these agents could do – it was in how they subtly changed our relationship with AI systems in the process. We went from the digital equivalent of playing 20 Questions with chatbots to having persistent virtual assistants that could actually maintain context, manage complex tasks and make reasonable decisions without needing to be guided through every minor choice. For someone who spent years working with the digital equivalent of goldfish, this was heady stuff.</p>
<p>The thing about agents is that their true power is not in what they can do alone – it is in how they work together. This is not just some hand-wavy My Little Pony-esque ‘collaboration is magic’ adage. It is fundamental to the nature of what an agent is, and why we even bother with them. An agent that can write code is useful, but an agent that can write code while collaborating with another that handles testing while yet another manages deployment and a fourth monitors performance is truly transformative. The industry’s current obsession with making individual agents (or, even worse, just individual foundation models) more powerful is like trying to make a better dish by adding more of a particular ingredient.<sup>2</sup> The magic of a perfect meal lies not in how many spoonfuls of exotic ingredients like fennel pollen and saffron threads you can dump into your pan, but in the fine balance between whatever ends up on a plate – even if it is just arugula, shaved parmesan and balsamic glaze (see recipe in sidebar). In short, the magic is in the complex web of interactions between the ingredients, the emergent phenomena that arise from their coexistence. We see this pattern play out all the time in technology. The apex of maturity always involved interaction. Consider, most ubiquitously, the web: from individual sites, we evolved to a more semantic web and eventually, a knowledge/information ecosystem driven through APIs. We are at the cusp of that third phase with AI agents, and anyone still fixated solely on individual agent capabilities is missing the plot entirely.<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;More AI folks should spend more time in the kitchen. This is <em>my</em> hot take for 2025.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;This is even more so as agents themselves are becoming meaningless. An agent, in a properly designed ecosystem, is entirely replaceable. It implements a contract or protocol. It then does not quite matter what that agent does. And if that sounds a little like the Liskov substitutability principle, then that is not quite by accident.</p></div><div class="">
<hr>
<p><strong>Recipe</strong>: The simplest salad you’ll ever love</p>
<ul>
<li>A bunch of baby arugula</li>
<li>1/4 cup of your favourite nuts</li>
<li>freshly ground salt and pepper</li>
<li>Parmigiano Reggiano, lots</li>
<li>1 clove garlic</li>
<li>1/4 cup lemon juice, freshly squeezed</li>
<li>1/2tsp honey</li>
<li>1/4 cup extra virgin olive oil</li>
<li>1tsp Dijon, the smoother the better</li>
<li>1/2tsp fresh thyme</li>
<li>optional: a good balsamic reduction</li>
</ul>
<p>Mix the lemon juice, minced garlic, Dijon and honey, and salt &amp; pepper to your heart’s content. While whisking, drizzle in the olive oil. Add the thyme and taste – add some more olive oil if it’s too acidic for your taste. In a separate bowl, add the arugula, and toss it with the vinaigrette you just made. Mix, using your hands – plastic or wooden mixing tools break the arugula leaves, which renders the whole thing bitter. Plate, then cover with the nuts. Using a coarse Microplane grater, shave enough Parmigiano Reggiano to make the whole thing a happy mixture. Enjoy on its own or as a light side.</p>
<hr>
</div></div>
<p>We are already seeing the first tentative steps from multi-agent systems towards ecosystems of agents, even if most have not recognised them as such. Enterprise agent marketplaces will be 2025’s hot commodities, drawing on past experiences with data marketplaces and exchanges. But conceptually, most seem to still treat agents as distinct pieces of software rather than collaborators in an ecosystem. The real pioneering work for the coming year(s) will be in developing the frameworks and protocols at the edges: agent orchestration systems that go beyond simple API calling, trust negotiation protocols that let agents establish their capabilities and limitations and collaborative frameworks that enable genuine multi-agent workflows. Technical implementations of these, however, are scarce. Working out an interaction protocol is not glamorous (trust me – speaking from personal experience), and there are going to be few headlines and fewer medals in working out how this sudden flood of AI agents is going to interoperate. And yet, this is the manifest destiny of agentic AI. Of all targeted agentic AI spend in 2025, marketplaces and interoperability orchestrators will without a doubt be the best dollars spent bar none.</p>
</section>
<section id="from-agents-to-ecosystems" class="level2">
<h2 class="anchored" data-anchor-id="from-agents-to-ecosystems">From agents to ecosystems</h2>
<p>What is an ecosystem? In its natural sense, we have relatively little trouble distinguishing between a species, an individual, a population and an ecosystem. In short, an ecosystem has three distinguishing features:</p>
<ol type="1">
<li>Diversity: ecosystems consist of multiple species that all play their role, quite similarly to agents in a well-architected system.</li>
<li>Interactions/rhizomality: ecosystems become what they are from the interactions between their participants, not the mere assemblage of the participants. In that sense, a good meal is an ‘ecosystem’ of sorts, where the acids balance out the fats and so on. In a more functional context, however, as we are dealing with here, what makes a bunch of agents an ecosystem is their ability to exercise a higher function in complementarity.</li>
<li>Interdependence: ecosystems produce their benefits through these interactions, which collectively amount to more than the sum of the parts.</li>
</ol>
<p>In an agentic AI system, our definition can largely be similar: an agentic AI ecosystem is a bunch of agents with different functionalities that interact and thereby unfold value. An ecosystem implies more than just the ability to pass messages between agents or chain them together in sequence. It requires the emergence of specialisation, of niches, of ways to establish trust and capabilities, and – crucially – ways to negotiate the terms of interaction. Unlike today’s relatively deterministic structures, this environmental discovery could eventually be self-governing, more like a bustling market bazaar, where agents can discover each other’s capabilities, negotiate terms of engagement, establish trust relationships, and even form longer-term collaborative partnerships. This is a fundamental reimagining of how artificial intelligence systems interact with each other.</p>
<p>The emergence of enterprise agent marketplaces will be one of the defining developments of 2025, but most organisations are still thinking about them wrong. The knee-jerk reaction is to build something akin to an app store: a catalogue of pre-built agents with rating systems and standard pricing. That is a useful (and often indispensable) first step, but it could be so much more. The real value of these marketplaces will not be in the agents themselves, but in the curation and verification mechanisms they enable. Think less ‘app store’ and more ‘commodity futures exchange’: what matters is not just what is being traded, but the rules of engagement, the verification of capabilities, the establishment of trust, and the standardisation of interfaces. We will need ways to verify that agents can actually do what they claim, that they operate within defined constraints and that they can be trusted with sensitive data or critical operations. This is where enterprise agent marketplaces will differentiate themselves from consumer platforms – through robust governance frameworks that make agent deployment actually feasible in regulated environments.</p>
<p>The real challenge – and opportunity – in building these ecosystems lies in standardising the right things while leaving room for innovation. Over-standardisation kills ecosystems as surely as no standardisation at all. The emerging protocols for agent interaction will need to thread this needle. They will have to standardise the essential patterns of trust establishment, capability discovery and resource negotiation, while remaining flexible enough to accommodate new forms of agent collaboration we have not even imagined yet. This is where the enterprise agent marketplace builders of 2025 will either make their fortunes or waste their investors’ money. The winners will be those who create the right balance of structure and flexibility – the ones who understand that they are not building an app store so much as cultivating an ecosystem.</p>
</section>
<section id="trust-falls-and-rises" class="level2">
<h2 class="anchored" data-anchor-id="trust-falls-and-rises">Trust falls (and rises)</h2>
<p>The hardest part of building these agentic ecosystems is not the technical implementation (in fact, we arguably have most of that already in place, mutatis mutandis, for data and other asset marketplaces) – but the trust architecture. In particular where agents choose and commission other agents to perform tasks as delegates or helpers, we need to find protocols that outline the powers of delegation and the flows of authority in such an architecture. The frameworks we are building now are laughably primitive compared to what we will need, mostly amounting to simple API keys and rate limits. It is like trying to build a modern financial system with nothing but paper IOUs and handshake agreements.</p>
<p>The chain of trust problem in agent delegation is fascinating precisely because it mirrors and yet fundamentally differs from how we handle human organisational hierarchies. When Agent A delegates a task to Agent B, which in turn needs to commission Agent C, we are not just passing around access tokens - we are creating a chain of responsibility that needs to be both traceable and constrained. Each link in this chain needs to carry not just the authority to act, but also the constraints and audit requirements of all previous links. An agent operating as a fourth-level delegate should still be bound by the original constraints set at the root of the delegation tree, even if it has no direct knowledge of them. This is not just about security - it is about maintaining coherent behaviour across increasingly complex chains of interaction. The financial sector learned this lesson the hard way with automated trading systems: without clear chains of responsibility and well-defined constraint propagation, you end up with cascading failures that no single participant can explain or control.</p>
<p>The next year will be defined not by breakthroughs in individual agent capabilities, but by our success or failure in building these frameworks for trusted collaboration. The winners will not be those who build the most powerful agents, but those who crack the code of helping agents work together effectively and safely. This is not just about technology – it is about understanding how to create systems of trust that can scale with the complexity of agent interactions.</p>
<p>The agentic revolution of 2024 was just the overture. The real symphony begins when we figure out how to let the players actually work together in concert. Those who are still focused solely on building better individual agents are composing for soloists in an age that demands orchestras.</p>
<p>The future belongs to those who can conduct.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {After Agents},
  date = {2025-01-04},
  url = {https://chrisvoncsefalvay.com/posts/after-agents/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“After Agents.”</span> <a href="https://chrisvoncsefalvay.com/posts/after-agents/">https://chrisvoncsefalvay.com/posts/after-agents/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/after-agents/</guid>
  <pubDate>Sat, 04 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/after-agents/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Five unconventional predictions</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</link>
  <description><![CDATA[ 




<p>As I sit here at year’s end, I’m reminded of the ancient Swedish tradition of årsgång - the ritual winter walk taken on New Year’s Eve to divine the fortunes of the coming year. The practice required one to walk alone in complete silence, visiting places of significance while remaining carefully alert to any signs or omens.<sup>1</sup> While I may be rather ill-equipped for mystical midnight wanderings, I’ve spent enough time observing the enterprise AI space to develop my own form of augury.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I gather from time to time, certain mind-altering substances were involved.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Note that I did not come up with the idea of agentic AI – in fact, a good part of what I believe is that there’s much less novelty to agentic AI than some would like to pretend. Rather, my point was that this more mature framework of agents interacting with agents will dominate over simple human-machine interactions.</p></div></div><p><a href="http://localhost:5836/posts/team-of-rivals/">In 2023, I predicted the rise of agentic systems</a>, when chatbots were still seen as the dominant form of LLM usage.<sup>2</sup> What at the time felt like a wild-ass guess is now almost received wisdom. And thus, I will throw my customary conservatism and restraint to the wind and in the spirit of årsgång, let me share five predictions for 2025.</p>
<section id="ai-governance-gets-productised" class="level2">
<h2 class="anchored" data-anchor-id="ai-governance-gets-productised">AI governance gets productised</h2>
<p>A few months ago, I shared a drink with an acquaintance whose work is in the Responsible AI field. It was pretty obvious he needed that drink a whole lot more than I needed my Diet Coke.</p>
<p>“Nobody wants or needs us,” he bemoaned.</p>
<p>“What are you talking about? Everybody is talking about Responsible AI and AI governance.” I was puzzled.</p>
<p>“No, I meant <em>us</em>,” he pointed at himself. “They want the ideas, the manuals, the guidelines… just not the people. They want governance as a service.”</p>
<p>Thinking back to several conversations I had this past year, I could see his point. There is plenty of interest in AI governance and Responsible AI, especially in the regulated sectors, where I spend most of my working (waking?) hours. This is unsurprising – even more in those sectors than any others, the success of GenAI initiatives hinges on regulatory tolerability to a fairly significant extent. But I hear much more enthusiasm for neatly packaged, productised, almost SaaS-like governance products than for Responsible AI as a function.</p>
<p>And so, while everyone’s been politely nodding along to principles and frameworks for most of 2024, the conversations all seemed to end with the same question – “great, can I have this on a SaaS model?”. What’s emerging is a clear pattern: organizations want Responsible AI practices, but they want them delivered as a service. This is at least partly due to buyers’ perceptions that much of those practices sound lofty and abstract, when their main concern is keeping the board and the regulators happy. You can’t feed a starving belly with high-minded principles.</p>
<p>We’re already seeing this in cloud providers are starting to package governance features, the rise of tools to facilitate this, and most tellingly, in how procurement departments are writing RFPs that specifically ask for such services in product-ish or service-ish terms – documents, procedures, TTPs and as-a-services. This shift signals the operationalisation of AI governance as it matures from theoretical frameworks to practical, subscription-based implementations. Because let’s face it: nothing says “we take ethics seriously” quite like a monthly fee.</p>
<p>My bet is that by the time the new regulatory flora and fauna of AI that is slowly emerging comes to bear its first fruits (say, late 2025-early 2027), we’ll have entire platforms dedicated to automated governance, continuous monitoring and “RAI middleware” that sits between models and applications. The real winners will be those who can package the complex requirements of Responsible AI into digestible, subscription-based services that make compliance and governance feel as natural as running a CI/CD pipeline. And, of course, we’ll get a new buzzword out of it. I’ll go and grab the domains for “AIGovOps” after this, but I’m sure someone will come up with something even more cringe-worthy.</p>
</section>
<section id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models">Small is beautiful (at the very least when it comes to language models)</h2>
<p>In a <a href="https://arxiv.org/abs/2402.10951">paper I published earlier in 2024</a>, I hijacked my audience for a few paragraphs’ worth of musings about the ethical, environmental, pragmatic and financial cases for small language models. At this time, one must recall, fine-tuning GPT models became feasible and, as it happens with AI hype, turned instantly into a status symbol. You know an industry has jumped the shark when you buy a night light, and it comes with a companion app with its own fine-tuned GPT. Just no.</p>
<p>Fortunately, the arc of AI development has largely bent the other way – towards small language models (which I believe is the correct direction). Turns out not every task needs a model that’s read all of Wikipedia and can write Shakespearean sonnets about your cat. Instead, we’re seeing enterprises discover that they can develop small domain-specific models for even very specific sets of terminologies, with better results than the large generalist models, which can then be relegated to act as dispatchers over these specialists. LLMs will be the general practitioners, while SLMs will increasingly take the specialist’s role.</p>
<p>It’s perhaps worth noting at this juncture that in AI, quantity has a quality all of its own. I’m somewhat reminded of a realisation that came to me this year when putting together a training plan. I hold multiple adaptive world records in a fairly esoteric sport called the SkiErg,<sup>3</sup> at distances ranging from 100m (the shortest distance eligible for a record) to marathons and half-marathons (the longest record-eligible distances). My heart is mostly with longer distances, so I had to put quite a bit of thought into figuring out how to train for shorter, explosive sprints. A 100m sprint is not just a ‘shorter 2k’. You need to approach it as a distance of its own, with its own challenges and merits. I was amused to see the same in SLMs – these aren’t pared-down LLMs. Successful SLMs are created as SLMs to begin with, not as reduced afterthoughts of larger models.<sup>4</sup> And so, just like I had to create a completely new training plan for sprint distances, SLM developers have to keep in mind that they’re not building LLMs writ small, but a different type of model with different desiderata.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Think of it as an indoor rower rotated by 90 degrees around its coronal plane, replicating not the ‘draw’ of rowing but the ‘pull’ of… something, I guess, having to do with skiing? <a href="https://www.concept2.com/ergs/skierg">Here’s a video.</a></p></div><div id="fn4"><p><sup>4</sup>&nbsp;That approach and attitude may be appropriate for quantisation, however, which is an entirely different story.</p></div></div><p>I expect we’ll see a proliferation of specialised SLMs in 2025, each trained on narrow domains but designed to work in concert. The art will be in the orchestration – how to route queries to the right specialist model, how to combine their outputs and how to maintain this constellation of smaller models efficiently. This parallels the evolution of microservices in software architecture, and we’re about to rediscover all the same lessons about service discovery, orchestration, and system design, but this time with AI models. The emergence of “ModelOps” is inevitable, and the best we can hope for is that we haven’t forgotten too much from last time.</p>
</section>
<section id="plot-twist-the-new-ai-kingmakers" class="level2">
<h2 class="anchored" data-anchor-id="plot-twist-the-new-ai-kingmakers">Plot twist: the new AI kingmakers</h2>
<p>Here’s a plot twist for 2025: your next AI project will likely live or die based on a decision made by some actuary who’s never written a line of code in their life. The insurance industry – that most conservative of institutions – may well become the de facto regulator of enterprise AI deployment. We’re already seeing the early signs in how cyber insurance policies are evolving to cover AI incidents, and how underwriters are starting to ask increasingly sophisticated questions about model governance and deployment practices. <a href="https://www.munichre.com/en/solutions/for-industry-clients/insure-ai/ai-self.html">Munich Re now offers AI insurance not only for commercial providers but also, essentially, in-house AI work</a>, and apparently, <a href="https://www.computerweekly.com/news/366586014/Munich-Re-sees-strong-growth-in-AI-insurance">business is booming</a>. Meanwhile, other insurers remain rather less sanguine. <a href="https://assets.lloyds.com/media/439566f8-e042-4f98-83e5-b430d358f297/Lloyds_Futureset_GenAI_Transforming_the_cyber_landscape.pdf">Lloyd’s commissioned a report last March</a> that is noticeably less upbeat. At least they didn’t compare it to asbestos.</p>
<p>But AI insurance is coming, and it may well become the kingmaker of AI solutions. This will particularly affect the startup ecosystem, who often do not have the funds to pursue certification before pitching to clients. Alas, the cost of enterprise executives’ comfort that is bought by the availability of an insurance and indemnity policy is that market access will be rather less feasible for those who cannot obtain such insurance – who, paradoxically enough, are the ones who would need it most.</p>
<p>And since that leaves insurers with a ‘shadow governance’ function, I can well imagine the insurance industry creating <em>de facto</em> standards for AI governance faster than any standards body or government regulator could dream of. And let’s be honest – the actuaries will probably do a better job than most regulators could anyway.</p>
</section>
<section id="cross-estate-ai-xeai-is-the-future" class="level2">
<h2 class="anchored" data-anchor-id="cross-estate-ai-xeai-is-the-future">Cross Estate AI (XEAI) is the future</h2>
<p>Agentic AI was an interesting development, but to me, that’s primarily about the <em>how</em>. The really interesting story is of the <em>what</em> – that is, of what we can do with agents that we couldn’t do without them. And by far one of the most interesting such applications is what I call Cross Estate AI (XEAI). In XEAI, information crosses boundaries between enterprises as AI agents from different places are composed together to build an agentic structure. Think of API calling, but for AI agents – and with way more sophistication. A company’s agentic model may reach out to multiple other companies that offer their own agents, and collaborate with them. We are now relatively comfortable with notions like our AI agents calling an external API – after all, REST API calling has been around for a long, long time. But my company’s AI agent ‘calling up’ a specialised AI agent from another company to, say, engage in a discussion to refine the wording in a regulatory submission is something that enterprise stakeholders might need some time to get their head around.</p>
<p>The technical challenges here are fascinating – we need dynamic trust scoring, cryptographic proofs of model lineage and secure compute enclaves. But the real challenge is the social architecture: a very complex dance of trust, verification and governance that will have to be mediated. We’re essentially speed-running the development of diplomatic protocols that took human societies centuries to develop. And just like real diplomacy, it’s all about managing relationships between different systems with different organizational cultures and governance models.</p>
<p>GenAI does not yet have a <em>lingua franca</em> akin to REST to allow systems to talk to each other, least of all one that allows not just a query-response format to be conveyed but also various conventions on trust and governance to be exchanged. The real potential of agentic AI lies in how various agents can interact and together create emergent structures that are ultimately more than their parts. Function calling and an AI agent being able to look up something on Google are neat, but not really anything spectacular that a simple RPA actor or a 10-line Python script starting with <code>import requests</code> couldn’t do just as well. To paraphrase Kipling: the strength of the agent is the ecosystem. The big challenge, then, is to figure out how we can allow such ecosystems to exist across the boundaries of corporate networks.</p>
</section>
<section id="model-metabolomics-takes-over" class="level2">
<h2 class="anchored" data-anchor-id="model-metabolomics-takes-over">Model metabolomics takes over</h2>
<p>Energy efficiency is about to become the dominant factor in AI deployment economics. We’re not just talking about cost savings – we’re talking about a fundamental shift in how we think about AI system design. Just as biological systems optimize for metabolic efficiency rather than raw performance, we’re about to see AI architecture undergo a similar evolution.</p>
<p>This shift will upend the current obsession with inference speed and model size. Organizations will start optimising models for what I call “computational metabolomics” – the total energy cost of getting useful work done. We’ll see the emergence of new architectures that might be slower in raw terms but dramatically more efficient in their use of resources.</p>
<p>The really controversial part? Many organizations will opt for slower but more energy-efficient approaches, fundamentally challenging the industry’s obsession with real-time everything. I expect to see in 2025 more and more architectures that prioritize efficiency over speed, and a new set of metrics focused on energy consumption per useful output. The hype cycles of 2023-24 were all about who could build the biggest model - 2025 will be about who can build the most efficient one. Especially with the rise of SLMs, we’re slowly approaching the point where increasing the parameter size of generalist LLMs is going to yield any useful business benefits, or get developers any more free drinks. Such diminishing returns are, of course, part and parcel of every evolving system, and while it will be conceptualised as the beginning of a new AI winter by the usual suspects (it is not!), this is a good thing. For as we now have slowly reached a model size we’re comfortable with, we can start on making it more metabolically efficient. This is, of course, not limited to language models – in fact, this development has been going on in the computer vision world, where a good deal of processing occurs on edge devices, for a long time. Much of the lacking enthusiasm for wholesale replacement of computer vision models with vision-language models like LLaVA, CogVLM or DeepSeek boils down to the simple economics of the matter: the vast majority of computer vision challenges, in practice, can be solved quite well with a 30-year-old algorithm that can be implemented in fifty lines of C. The same, incidentally, is true for LLMs. I wince when I see GPT-based approaches to problems that a bag-of-words classifier can solve with comparable accuracy for essentially no cost and in a fraction of time. And once the power of cool wears off, I expect a renaissance of many of those solutions.</p>
<hr>
<p>As I complete this digital årsgång, what strikes me most is how 2025 looks to be the year when enterprise AI grows up. The trends all point toward practicality over pizzazz: smaller models over larger ones, efficiency over raw power, productized governance over philosophical frameworks, and insurance actuaries over innovation evangelists. Perhaps that’s not as exciting as the breathless predictions of AI singularities and digital transformations that dominated 2023, and maybe closer to the relatively sober atmosphere of 2024. But then again, maybe that’s exactly the point – real progress tends to be more about making things work than making headlines. And personally, I find that far more interesting.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {Five Unconventional Predictions},
  date = {2024-12-31},
  url = {https://chrisvoncsefalvay.com/posts/five-wild-guesses/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“Five Unconventional
Predictions.”</span> <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">https://chrisvoncsefalvay.com/posts/five-wild-guesses/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>tech predictions</category>
  <guid>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</guid>
  <pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/five-wild-guesses/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>LAIR - Language As Intermediate Representation</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/lair/</link>
  <description><![CDATA[ 




<p>The awesome thing about language is that, well, we all mostly speak it, to some extent or another. This gives us an immensely powerful tool to manipulate transformational tasks. For the purposes of this post, I consider a transformational task to be essentially anything that takes an input and is largel intended to return some version of the same thing. This is not a very precise definition, but it will have to do for now.</p>
<p>Such models are nothing new. Perhaps the most eye-catching and ubiquitous of such models are <a href="https://www.tensorflow.org/tutorials/generative/style_transfer">neural style transfer</a> models that take an image and return a version of the same image in a different style that let you turn a picture of your dog into a Van Gogh painting <span class="citation" data-cites="gatys2015neural">(Gatys, Ecker, and Bethge 2015)</span>. Quite simply put, these models are your typical generative model, with the difference that it takes two separate loss definitions: content loss, which is loss of the generated image vis-a-vis the content reference, and style loss, which is the loss vis-a-vis the style reference image. A “good” image then is one that minimises total loss, i.e.&nbsp;it’s just as close to your dog as it is to Van Gogh. Figure&nbsp;1 outlines this logic.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gatys2015neural" class="csl-entry">
Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. <span>‘A Neural Algorithm of Artistic Style’</span>. <em>arXiv Preprint arXiv:1508.06576</em>.
</div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nst" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-nst">flowchart TD
    C["Content image"]
    S["Style image"]

    G["Generated image"]

    G --&gt; L["Loss network"]
    L --&gt; G
    
    C -- "Content loss" --&gt; L
    S -- "Style loss" --&gt; L
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A rough outline of NST.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Given a content source image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bc%7D"> and a style reference image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D">, we define the total loss of our generated image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bg%7D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Btotal%7D%20=%20%5Calpha%20%5Cmathcal%7BL%7D_%7Bcontent%7D%20+%20%5Cbeta%20%5Cmathcal%7BL%7D_%7Bstyle%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are hyperparameters that control the relative importance of the content and style losses. The content loss for layer <img src="https://latex.codecogs.com/png.latex?l"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bcontent%7D(%5Cvec%7Bc%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi,j%7D%20(%5Cvec%7Bc%7D_%7Bij%7D%5El%20-%20%5Cvec%7Bg%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>which is basically a simple squared error loss between the feature vector of the content image and the generated image at layer <img src="https://latex.codecogs.com/png.latex?l">. The style loss is a bit more complicated, and is these days typically defined as the Maximum Mean Discrepancy, which <span class="citation" data-cites="li2017demystifying">Li et al. (2017)</span> have shown is essentially equivalent to the Gram matrix loss, defined as</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017demystifying" class="csl-entry">
Li, Yanghao, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. 2017. <span>‘Demystifying Neural Style Transfer’</span>. <a href="https://arxiv.org/abs/1701.01036">https://arxiv.org/abs/1701.01036</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bstyle%7D(%5Cvec%7Bs%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B4N_l%5E2M_l%5E2%7D%20%5Csum_%7Bi,j%7D%20(%5Cmathbf%7BG%7D_%7Bij%7D%5El%20-%20%5Cmathbf%7BS%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BG%7D_%7Bij%7D%5El"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BS%7D_%7Bij%7D%5El"> are the Gram matrices of the generated reference image and the style image at layer <img src="https://latex.codecogs.com/png.latex?l">, respectively. <img src="https://latex.codecogs.com/png.latex?N%5El"> is the number of feature maps in layer <img src="https://latex.codecogs.com/png.latex?l"> and <img src="https://latex.codecogs.com/png.latex?M_l"> is the dimensionality (height times width) of the feature map of layer <img src="https://latex.codecogs.com/png.latex?l">. There are two fundamental problems with this.</p>
<ol type="1">
<li>This works much less well for things that aren’t images or at least sufficiently similar to images.</li>
<li>The bigger problem is that the style reference is pretty much exhaustive. By that, I mean that there isn’t much we can convey to the model about the style that isn’t encapsulated in the style reference images. Depending on how semantically apt your model is, it may or may not be able to pick up some higher level ideas. It may be able to pick up the brush strokes of Van Gogh or the colours of a Turner, but it may not be able to paint your characters in the semantic context of Van Gogh’s time and place.</li>
</ol>
<p>This is where language comes in. Language is a very powerful tool for conveying information, and it turns out that if we use language as an intermediate representation, we can use language models to manipulate this intermediate representation to our heart’s content, using a relatively informal and rather forgiving language. This is the idea behind LAIR.</p>
<section id="a-toy-example" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-toy-example">A toy example</h2>
<p>I’m a firm believer in silly toy examples. The sillier, the better. So, we’ll start with the following proposition: can we create a model that will look at a photo from, say, the front page of our favourite newspaper, and transpose it into the Warhammer 40k universe? In case you’re unfamiliar, Warhammer 40k is set – as the name suggests – in the 40th millennium, but is a weird mixture of medieval and futuristic technology. The whole atmosphere is taking the ‘Dark Ages’ part of the Middle Ages,<sup>1</sup> adding spaceships and laser guns, and turning the whole thing into an absolutely depressing dystopia. It’s a lot of fun.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Which may or may not ever have actually existed.</p></div></div><p>There are a few things we want here: I don’t merely want the <em>visual</em> style of the Warhammer 40k universe, I also want the <em>semantics</em> – that is, I want characters to be transposed into the Warhammer 40k universe. I want the model to understand that the people in the photo are now Space Marines, and that the buildings are now Gothic cathedrals. I can’t get neural transfer to that for me, because it does not understand, or care, about semantics, and does not do semantic transformation. More importantly, I cannot interact with the ‘guts’ of neural style transfer beyond setting the hyperparameters and the source images.</p>
<p>What I can, however, do is to use the language I am mostly most proficient in – that is, human language – to manipulate an intermediate representation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lair" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-lair">flowchart LR
    subgraph Description
        direction TB
        S["Source image"] --&gt; D("Descriptor\ne.g. GPT-4 vision") --&gt; d["Description"]
    end

    subgraph Transformation
        direction TB
        t("Transformer\ne.g. GPT-4") --&gt; td["Transformed\ndescription"]
    end

    subgraph Rendering
        direction TB
        r("Renderer\ne.g. DALL-E") --&gt; I["Output\nimage"]
    end

    Description --&gt; Transformation --&gt; Rendering
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A rough outline of LAIR as applied to the toy example.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure&nbsp;2 outlines the basic idea for images. We use a descriptor model to describe the image, and then use a text-to-text transformer to transform the description. The transformed description is then rendered into an image. The descriptor model is typically a vision model, and the transformer is typically a language model. The renderer would then be a generative model.</p>
<p>Consider the following source image I downloaded from Pexels:</p>
<div id="fig-source" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/pexels-canva-studio-3153198.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Our source image.
</figcaption>
</figure>
</div>
<p>Here’s the intermediate representation (IR) description created by GPT-4 vision:</p>
<blockquote class="blockquote">
<p>This is an image of four individuals gathered around a laptop, suggesting a collaborative work environment. The setting has an ample amount of natural light, evident by the bright illumination on the subjects and the interior. They appear to be located in a modern indoor environment with a large curved window in the background, contributing to the light-flooded ambiance.</p>
<p>Starting from the left, the first individual is a man with short, dark hair and a light beard, wearing a black t-shirt with a white graphic and text. He appears focused, resting his hands on the laptop while looking at the screen. To his right is a woman with shoulder-length curly hair who seems engaged and smiling, leaning slightly towards the laptop. She’s wearing a light grey short-sleeved top.</p>
<p>Behind these two and slightly out of focus, two more individuals are present. The person in the middle of the composition is a man with shoulder-length wavy hair, wearing a black shirt, gazing at the laptop screen. To the far right, partially cropped out of the frame, there is a woman with glasses, wearing a dark business casual blazer. She seems to be observing the laptop’s screen with interest, standing slightly behind the seated individuals.</p>
<p>The group exhibits a sense of teamwork and focus, with everyone’s attention directed toward the laptop, situated on what appears to be a white table or desk. The body language and expressions suggest a positive, collaborative atmosphere, typical of a professional or academic team setting.</p>
</blockquote>
<p>This is a pretty decent description. Let’s grimdark it up by passing it to our transformer model, which was initialised with the following prompt:</p>
<blockquote class="blockquote">
<p>Your job is to analyze descriptions of images and change them to transpose the image into the dark, futuristic aesthetic of the Warhammer 40k universe. You are given a description of a scene, and your job is to change that scene according to the instructions, and give a detailed description of the changed scene that would allow a generative model to create an image.</p>
<ol type="1">
<li>Replace clothing items with a mixture of futuristic and mediaeval looking armour.</li>
<li>Replace every-day objects held by characters with power swords, chainswords,futuristic looking guns, staffs or magic items.</li>
<li>Replace architecture with the monumental, dark architecture common to the Warhammer 40k universe.</li>
<li>Include things like monumental gothic spaceships in the image description.</li>
</ol>
</blockquote>
<p>Here’s the output:</p>
<blockquote class="blockquote">
<p>This is an image of four battle-hardened individuals gathered around a hololith, suggesting a strategic war council. The setting is bathed in the cold, harsh light of flickering lumens, casting long, grim shadows on the figures and the interior. They appear to be situated in a cathedral-like command room of an Imperial Battleship, with a monumental, gothic archway in the background, contributing to the oppressive ambiance.</p>
<p>Starting from the left, the first individual is an Adeptus Astartes, his short, dark hair and light beard visible beneath the rim of his power armor helmet. He is clad in a black power armor with white Imperial insignia. He appears focused, his gauntleted hands resting on the hololith while studying the battlefield projection. To his right, a Sororitas, her shoulder-length, curly hair peeking out from beneath her wimple, leans towards the holographic display. She’s clad in a light grey power armor, her face engaging and determined.</p>
<p>Behind these two and slightly out of focus, two more figures loom. The figure in the middle of the composition is a man, his shoulder-length wavy hair barely visible beneath his Inquisitorial hat, wearing a black, ornate, medieval-looking armor, his eyes fixed on the hololith. To the far right, partially cropped out of the frame, there is a Tech-Priestess, her bionic glasses glinting in the lumen-light, her dark, mechanicus robes adding to her awe-inspiring presence. She seems to be observing the hololith with keen interest, standing slightly behind the armored individuals.</p>
<p>The group gives off a sense of unity and focus, with everyone’s attention directed toward the hololith, situated on what appears to be an aged, white marble table. The body language and expressions suggest a tense, collaborative atmosphere, typical of a war council in the grim darkness of the far future.</p>
</blockquote>
<p>By the Emperor, that’s some good stuff. Let’s see what DALL-E makes of it:</p>
<div id="fig-output" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/img-eTqdxb5CYGT2vY5FqNkRJdva.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Our output image.
</figcaption>
</figure>
</div>
<p>It’s interesting to note what remains and what has changed. In particular, the relationship of the figures to each other, both spatially and semantically, as well as their posture, their number (mostly!) and the general subject matter at a highly semanticised level was preserved. Instead of a laptop, they’re looking at a big holographic sand table. Note that unlike in NST, we actually have control over what is preserved and what is not – that’s what our transformer prompt accomplishes. We’ve told it to change dress, for instance, but we haven’t told it to make any changes to the overall relationship between the figures (note how a “collaborative atmosphere”, for instance, was retained word for word).</p>
</section>
<section id="whats-the-point" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-the-point">What’s the point?</h2>
<p>Okay, this was quite fun, but what’s the point? Well, the point is that we can use language as an intermediate representation to manipulate things relatively easily. This is powerful because language is such an accessible and forgiving intermediate representation. More importantly, however, <em>anything</em> that can be represented in language can be manipulated this way.</p>
<ul>
<li>In the space of language-to-language, this includes adapting text to the needs of special audiences,<span class="citation" data-cites="steinmetz2023developing verma2023bridging">(Steinmetz 2023; Verma, Boland, and Miesenberger 2023)</span> conveying information to lay audiences <span class="citation" data-cites="lee2023ai">(Lee, Goldberg, and Kohane 2023)</span> and creating secondary explanatory materials e.g.&nbsp;statutory explanations <span class="citation" data-cites="blair2023can">(Blair-Stanek, Holzenberger, and Van Durme 2023)</span>.</li>
<li>For image-to-image transformations, retaining semanticity while simpifying visuals and removing clutter may often be useful, e.g.&nbsp;for creating procedural visual guidance for medical procedures <span class="citation" data-cites="chen2023endoscopic">(Chen 2023)</span>. Often, such images are created by hand, but this is a time-consuming process that could be automated.</li>
<li>For code-to-code, language as an intermediate representation allows the interjection of desired features into code, e.g.&nbsp;for the purposes of code refactoring. Beyond simple code rewriting, this allows a kind of opinionated transformation. Often, a target language is not only idiomatically different but also has certain other characteristics, and this is <em>a fortiori</em> the case for DSLs <span class="citation" data-cites="magalhaes2023c2taco">(Magalhães et al. 2023)</span>.</li>
<li>For code-to-text, this allows the creation of documentation from code, which is a perennial problem in software engineering. The textual intermediate representation allows fine control over the resulting documentation.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-steinmetz2023developing" class="csl-entry">
Steinmetz, Ina. 2023. <span>‘Developing <span>“EasyTalk”</span>–a Writing System Utilizing Natural Language Processing for Interactive Generation of <span>“Leichte Sprache”</span>(easy-to-Read German) to Assist Low-Literate Users with Intellectual or Developmental Disabilities and/or Complex Communication Needs in Writing’</span>. <a href="https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf">https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf</a>.
</div><div id="ref-verma2023bridging" class="csl-entry">
Verma, A Kumar, S Gavra Boland, and Klaus Miesenberger. 2023. <span>‘Bridging the Digital Divide for Persons with Intellectual Disabilities: Assessing the Role of ChatGPT in Enabling Access, Evaluation, Integration, Management, and Creation of Digital Content’</span>. In <em>ICERI2023 Proceedings</em>, 3767–76. IATED.
</div><div id="ref-lee2023ai" class="csl-entry">
Lee, Peter, Carey Goldberg, and Isaac Kohane. 2023. <em>The AI Revolution in Medicine: GPT-4 and Beyond</em>. Pearson.
</div><div id="ref-blair2023can" class="csl-entry">
Blair-Stanek, Andrew, Nils Holzenberger, and Benjamin Van Durme. 2023. <span>‘Can GPT-3 Perform Statutory Reasoning?’</span> <em>arXiv Preprint arXiv:2302.06100</em>.
</div><div id="ref-chen2023endoscopic" class="csl-entry">
Chen, Hao-Wen. 2023. <span>‘Endoscopic Endonasal Skull Base Surgery for Pituitary Lesions: An AI-Assisted Creative Workflow to Develop an Animated Educational Resource for Patients and Physicians’</span>. PhD thesis, Johns Hopkins University.
</div><div id="ref-magalhaes2023c2taco" class="csl-entry">
Magalhães, José Wesley de Souza, Jackson Woodruff, Elizabeth Polgreen, and Michael FP O’Boyle. 2023. <span>‘C2TACO: Lifting Tensor Code to TACO’</span>. In <em>Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences</em>, 42–56.
</div></div><p>The possibilities for using language as an intermediate representation are endless. LAIR is a powerful paradigm for transformational tasks that allows us to use language as an intermediate representation to manipulate things in a way that is both accessible and powerful, and that allows us to pick and choose <em>what</em> part of semanticity we want to manipulate <em>versus</em> what we want to preserve. It’s hard to reason about LAIR’s relative performance given that it is not a technique but a paradigm, and that its focus is not simple style transfer but finely controlled stylistic and contextual transformation, but even in the current absence of benchmarks, it is clear that models benefit from using language as an easily workable and malleable intermediate representation.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>The code for the toy example is available <a href="https://gist.github.com/chrisvoncsefalvay/f3c6e7547350cfae50be0078ad99435f">here</a>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {LAIR - {Language} {As} {Intermediate} {Representation}},
  date = {2024-01-06},
  url = {https://chrisvoncsefalvay.com/posts/lair/},
  doi = {10.59350/qg7b3-crs97},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“LAIR - Language As Intermediate
Representation.”</span> <a href="https://doi.org/10.59350/qg7b3-crs97">https://doi.org/10.59350/qg7b3-crs97</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>style transfer</category>
  <guid>https://chrisvoncsefalvay.com/posts/lair/</guid>
  <pubDate>Sat, 06 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/lair/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>The 95% myth</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/95-percent-myth/</link>
  <description><![CDATA[ 




<p>The year is 1959. Eisenhower is on his second term, Castro just kicked Batista out of the country and Ray Charles’s Let the Good Times Roll is topping the charts. And, in slightly nerdier pursuits, a psychiatrist named Albert “Mickey” Stunkard and a dietician colleague of his, Mavis McLaren-Hume, published a paper in the <em>AMA Archives of Internal Medicine</em> titled <em>The Results of Treatment for Obesity</em> <span class="citation" data-cites="smh">(Stunkard 1959)</span>. Written in a somewhat endearingly old school way by authors not quite initiated to the sacred mysteries of, y’know, <img src="https://latex.codecogs.com/png.latex?p">-values and proper statistical methodology, it reports on “the literature” on dietary and pharmacologically assisted treatments (which is an extremely generous term for 8 studies, none of which are randomised, one of which is entirely retrospective, and the total <img src="https://latex.codecogs.com/png.latex?N"> is 1,368), and on a sample of 100 patients (!) from the Nutrition Clinic at New York Hospital (the “NY sample”). And from that, they draw wide-ranging conclusions on the ‘harm’ and ‘ineffectiveness’ of ‘weight reduction’.</p>
<div class="no-row-height column-margin column-container"><div id="ref-smh" class="csl-entry">
Stunkard, A. J. 1959. <span>‘The Results of Treatment for Obesity’</span>. <em>A.M.A. Archives of Internal Medicine</em> 103: 79. <a href="https://doi.org/10.1001/archinte.1959.00270010085011">https://doi.org/10.1001/archinte.1959.00270010085011</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Fat acceptance activists love to point out the size of the US ‘diet industry’, which is somewhere around US$ 70bn. For comparison, the US QSR (quick service restaurants – read: fast food) industry had a 2022 market size of US$ 380bn.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;If you are wondering by the end of this whether Stunkard got his MD by sending in some coupons from cereal boxes, you’re not alone, but that would be quite unfair. From what I’ve read of the man, he was both a great human being and a very accomplished clinician. It’s a pity that his most influential piece was not only uncharacteristically bad work for him, but was also grievously misinterpreted.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Don’t worry, it gets worse. The <img src="https://latex.codecogs.com/png.latex?N"> for maintenance is 12. That’s not a typo.</p></div></div><p>In this post, I’ll dissect the Stunkard-McLaren-Hume (SMH) paper, and how it became an uncritically repeated figure and later the guiding mantra of ‘fat acceptance’, a movement I consider to be organised science denialism fuelled by self-delusion and corporate interests.<sup>1</sup> I try to do so with some generosity and understanding for the vagaries of the age, but one shouldn’t be under any illusions that the quality of the SMH paper would pass muster as an undergraduate research paper today. Standards have evolved, but the SMH paper is, well, not to mince words, bad even for the time.<sup>2</sup> It is conclusory to the extreme over a pathetically small sample, and I’m mostly convinced that the reason the paper is never cited is because anyone making major life decisions about their health and well-being based on a 65-year-old study of 100 people would look, well, quite silly.<sup>3</sup></p>
<section id="sm-damn-h" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sm-damn-h">SM-damn-H</h2>
<p>The SMH paper consists of two parts. One is a rudimentary ‘mini-meta’ of 8 studies from 1931 to 1958, of a little over 1,300 patients. The other is a retrospective study of 100 patients from the Nutrition Clinic at New York Hospital. Neither is great, but the mini-meta is so bad, I won’t belabour it in detail. It looks at very few patients overall on a range of ‘diets’, ranging from a 600kcal VLCD to diets assisted by dexedrine (classy!) and thyroid supplementation (unmonitored, of course), and reports the percentage of patients by lbs of weight lost. This is confusing to the extreme, because of course the absolute weight loss on a diet is highly contextual. A 200lbs person losing 50lbs is an entirely different story from a morbidly obese individual, say 350lbs, losing back to 300lbs. The mini-meta is, in short, a mess, and I’m not going to waste time on it.</p>
<p>The second part, the retrospective study, is also very bad, but at least informative. Here, 100 patients were interviewed at admission to the Nutrition Clinic, and prescribed a diet. What diet, you might ask? “Balanced weight-reduction diets from 800 to 1,500 Cal” (sic). That’s right, the diets were not standardised, nor do we see the estimated BMR<sup>4</sup> or the shortfall vis-a-vis the BMR reported anywhere. No mention of exercise or control for activity is in evidence anywhere. The clinic itself gives a bit of a flying by the seat of one’s pants impression, and the study doesn’t get better from here on.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Basal Metabolic Rate.</p></div></div><p>The retrospective study was then performed 2.5 years after initial admission, as a chart review. Of the 100 patients, only 12 were considered to have successfully lost weight, with 20lbs lost being the cut-off point (once again, entirely insensitive to starting body weight, thus likely privileging the more obese initial participants). A flowchart of this is laid out in Figure&nbsp;1.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-flowchart">flowchart TD
    I["Initial intake:&lt;br&gt;100"] --&gt; J["Follow-up&lt;br&gt;61 (61%)"]
    I --&gt; L["Lost to follow-up&lt;br&gt;39 (39%)"]
    J --&gt; S["Successful (more than 20lbs lost)&lt;br&gt;12 (19.7%)"]
    J --&gt; U["Unsuccessful (20lbs or less lost)&lt;br&gt;49 (80.3%)"]
    S1["Maintained at 1y&lt;br&gt;6 (50%)"]
    S2["Maintained at 2y&lt;br&gt;2 (16.6%)"]
    LFU1["Lost to follow-up&lt;br&gt;1 (8.3%)"]
    F1["Failed at 1y&lt;br&gt;5 (41.7%)"]
    F2["Failed at 2y&lt;br&gt;4 (33.3%)"]
    S --&gt; S1
    S --&gt; F1
    S --&gt; LFU1
    S1 --&gt; S2
    S1 --&gt; F2
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Patient flow of the SMH study. Numerators of percentages are noted in bold. Based on Table 2 of the SMH paper, except that I have broken out the maintenance group into two subgroups.
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are a few things worth noting here. One is that this is a study of an <img src="https://latex.codecogs.com/png.latex?N"> barely in the three figures, and has a 39% loss to follow-up. One would likely not want to publish that. That’s just quite simply not publishable data. Worse, however, is the confusion of numerators and denominators.</p>
<p>If we consider maintenance failure to be failure of the diet after 1 or 2 years of finishing treatment, then it is true that only 6 out of 100 initial patients (6%) maintained their weight loss for 1 year, and only 2 (2%) for 2 years. Except that’s altogether the wrong way of calculating these figures, especially in the face of losing <em>over a third</em> of the initial cohort to follow-up. For what it’s worth, each of those 39 lost to follow-up patients could have maintained for two years and simply didn’t bother to go back to the clinic. We don’t know. We can’t know. We can’t even make a reasonable guess. If your figures permit a conclusion that success might have been anything between 2% and 41%, you don’t have a study. You have a mess.</p>
<p>One highly suggestive feature here (which, to their credit, SMH point out) is that 28 of the 39 lost to follow-up (71.8%) never attended any <em>other</em> clinic at the hospital. SMH note that</p>
<blockquote class="blockquote">
<p>[s]ince admission to the Nutrition Clinic occurs entirely by referral from other clinics, this represents the rupture of at least two therapeutic relationships.</p>
</blockquote>
<p>Is that an inescapable conclusion? Or could one conceive that at least a good part of the patients lost to follow-up either met their goals and thus never reported back, or at the very least, their weight loss was actually enough of a <em>resounding</em> success that it alleviated the primary issue for which they presented prior to referral to the Nutrition Clinic? Once again, we don’t know. We can’t know. We can’t even make a reasonable guess.<sup>5</sup> What we can say is that concluding from this data that long-term weight loss is 6% or 2% effective is an incredibly, unreasonably strong interpretation of very, very weak data indeed.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Or can we? The QUOVADIS study had this issue: following up with the dropouts (7%) indicated many were just really satisfied with the results.</p></div></div></section>
<section id="pulling-the-thread-misinterpreting-smh" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="pulling-the-thread-misinterpreting-smh">Pulling the thread: misinterpreting SMH</h2>
<p>Of course, scientific writing can easily become like a runaway horse. Once one puts their thoughts out in the world, it’s open for the audience to misinterpret it and draw every single wrong conclusion from it. As grievously bad as I think SMH is, it is nowhere near the juggernaut of bad science that uncritical repetition has turned it into over the years.</p>
<p>Consider a randomly picked example <span class="citation" data-cites="Carmichael1999">(Carmichael 1999)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="ref-Carmichael1999" class="csl-entry">
Carmichael, A. R. 1999. <span>‘Treatment for Morbid Obesity’</span>. <em>Postgraduate Medical Journal</em> 75: 7–12. <a href="https://doi.org/10.1136/pgmj.75.879.7">https://doi.org/10.1136/pgmj.75.879.7</a>.
</div></div><blockquote class="blockquote">
<p>Results on the role of diet in the treatment of obesity reported more than 30 years ago do not vary from those reported more recently, because as many as 95% of dieters tend to regain their lost weight over a relatively short period.</p>
</blockquote>
<p>SMH is, of course, the authority for that assertion. Yet that misunderstands, quite fundamentally, what SMH actually does say, which is that a good percentage of diets fail, not that a good percentage of dieters do. This point is far from being so subtle as to justify someone actually publishing this misunderstanding past a peer reviewer.</p>
<p>It is, for instance, widely acknowledged that one by one, antidepressants are effective only a relatively small percentage of the time (something between 15-30%). Yet antidepressant therapy by and large is vastly more successful. That is because just like diets, antidepressants are not a monolithic treatment, but a class of treatments: if one fails, one is free to try another, or a combination of others. Like obesity and human metabolism, depression is a multifaceted disorder with many possible aetiological processes, and it might take a few tries to find the right diet. Not that someone who has already concluded that ‘diets don’t work’ and convinced themselves that a study of all of 100 patients on all of one type of diet would prove that fact would come to that conclusion, of course. Sadly, that’s the cost of sloppy science of the kind the SMH paper perpetrates: you blind yourself to fundamental truths in a mad pursuit of confirmatory evidence.</p>
<p>The second pernicious misinterpretation is that weight regain indicates a diet ‘failure’. Once again, I’m puzzled by how Stunkard and McLaren-Hale missed this point, for their study did not control for maintenance:</p>
<blockquote class="blockquote">
<p>Our results are summarized in Table 2. In this Table any person who maintained a weight loss of 20 lb. or more is classified as a “success”; any person whose weight was within 19 lb. of the starting weight is a “failure.</p>
</blockquote>
<p>What is of course entirely omitted is what these patients did over the 1- and 2-year follow-up period. We know from studies on VLE/VLCDs<sup>6</sup> that adherence is crucial <span class="citation" data-cites="10.7243/2052-5966-1-4">(Wright et al. 2013)</span>. So, if the assertion is that a diet does not immunise to weight regain once it is abandoned, the SMH paper’s finding is trivial to the point of banality. Proper sciencin’ would require us to control for calorie intake in that maintenance period. No diet will magically mean that reverting to an intake above expenditure will not result in significant weight gain once again. This is reflected in a modern understanding of diets, which considers the best diets to be those that can be sustained indefinitely (with the exception of some acute ‘crash’ diets used to rapidly reduce weight before surgical or other interventions). In short, the SMH paper’s alleged finding of long-term ‘diet failure’ is either trivially true (yes, a diet, once abandoned, will not confer continuing benefits) or at the very best unproven. While the entire process was largely unmonitored (it’s admittedly difficult to monitor dietary intake in an outpatient setting, especially in the pre-Nutrition Facts Labeling era<sup>7</sup>), the lack of even the slightest semblance of monitoring or tracking adherence, even self-reported, is fatal to the study as evidence for maintenance of weight loss. Whatever its merits (and there aren’t many), SMH is epistemically incapable of supporting the argument for which it is most famously and extensively used.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Very Low Energy/Very Low Calorie Diets.</p></div><div id="ref-10.7243/2052-5966-1-4" class="csl-entry">
Wright, T. G., B. Dawson, G. Jalleh, and K. J. Guelfi. 2013. <span>‘Program Compliance, Weight Loss and Health Profile Changes in Who Obesity Classes After Very Low Energy Dietary Intervention’</span>. <em>Global Epidemic Obesity</em> 1: 4. <a href="https://doi.org/10.7243/2052-5966-1-4">https://doi.org/10.7243/2052-5966-1-4</a>.
</div><div id="fn7"><p><sup>7</sup>&nbsp;Nutrition Facts labels were implemented by the Nutrition Labeling and Education Act 1990, which entered into force in 1994.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;But that’s a misinterpretation of the science for another day.</p></div></div><p>Together, these two misinterpretations gave rise to a dangerous myth that all but ‘proves’ the inevitability of one’s own weight, which in turn morphed into the ‘set point theory’.<sup>8</sup> I’d like to acquit Stunkard and McLaren-Hale from responsibility for these, for fairness demands I do so, but I cannot wholeheartedly feel that they haven’t, through sloppy science and bad writing, opened Pandora’s box. On their head, then, must some of the resulting blame land.</p>
</section>
<section id="why-this-matters" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-this-matters">Why this matters</h2>
<p>It wouldn’t behoove to fire cheap shots at a paper old enough to collect retirement benefits in most civilised countries if it weren’t for the fact that its sloppy distillation into the 95% myth had become a uniquely harmful mantra. There are very few interventions that justify even relatively small risks in return for a 5% effectiveness, so if that figure is accurate, even the modest risks of adequate, well-executed, medically supervised diets<sup>9</sup> might appear excessive. But, of course, that figure is almost definitely wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Which are generally transitory. It’s probably worth pointing out at this point again that the diets examined in the SMH paper included dexedrine, thyroid supplementation without much proper monitoring and 900kcal low calorie diets. It’s perhaps not unreasonable to assume that these aren’t on the safer side. Their risks do not compare to those of a properly monitored modern diet aimed at a few hundred kcals of deficit.</p></div><div id="ref-thomas2014weight" class="csl-entry">
Thomas, J Graham, Dale S Bond, Suzanne Phelan, James O Hill, and Rena R Wing. 2014. <span>‘Weight-Loss Maintenance for 10 Years in the National Weight Control Registry’</span>. <em>American Journal of Preventive Medicine</em> 46 (1): 17–23.
</div><div id="ref-kraschnewski2010long" class="csl-entry">
Kraschnewski, JL, Jarol Boan, Joseph Esposito, Nancy E Sherwood, Erik B Lehman, Donna K Kephart, and Christopher N Sciamanna. 2010. <span>‘Long-Term Weight Loss Maintenance in the United States’</span>. <em>International Journal of Obesity</em> 34 (11): 1644–54.
</div><div id="ref-perri2008extended" class="csl-entry">
Perri, Michael G, Marian C Limacher, Patricia E Durning, David M Janicke, Lesley D Lutes, Linda B Bobroff, Martha Sue Dale, Michael J Daniels, Tiffany A Radcliff, and A Daniel Martin. 2008. <span>‘Extended-Care Programs for Weight Management in Rural Communities: The Treatment of Obesity in Underserved Rural Settings (TOURS) Randomized Trial’</span>. <em>Archives of Internal Medicine</em> 168 (21): 2347–54.
</div><div id="ref-10.1016/j.amjmed.2012.10.010" class="csl-entry">
Unick, J., D. Beavers, D. Bond, J. Clark, J. Jakicic, A. Kitabchi, W. Knowler, T. Wadden, L. Wagenknecht, and R. Wing. 2013. <span>‘The Long-Term Effectiveness of a Lifestyle Intervention in Severely Obese Individuals’</span>. <em>The American Journal of Medicine</em> 126: 236–242.e2. <a href="https://doi.org/10.1016/j.amjmed.2012.10.010">https://doi.org/10.1016/j.amjmed.2012.10.010</a>.
</div><div id="ref-diabetes200910" class="csl-entry">
Group, Diabetes Prevention Program Research et al. 2009. <span>‘10-Year Follow-up of Diabetes Incidence and Weight Loss in the Diabetes Prevention Program Outcomes Study’</span>. <em>The Lancet</em> 374 (9702): 1677–86.
</div></div><p>Thus, we know that especially when used as part of a comprehensive treatment plan, diets are effective. <span class="citation" data-cites="thomas2014weight">Thomas et al. (2014)</span>, reporting on the National Weight Control Registry study (<img src="https://latex.codecogs.com/png.latex?N%20="> 2,886), found that 88.4% of participants maintained a weight loss of at least 10% of their initial body weight for at least 5 years and 86.6% still maintained a 10% weight loss at 10 years. The retrospective study based on NHANES by <span class="citation" data-cites="kraschnewski2010long">Kraschnewski et al. (2010)</span> is even more encouraging: not only did they find that more than one in six overweight adults have lost and maintained a 10% weight loss for over a year, they also found that over a third (36.6%) of those who lost at least 5% of their body weight were able to maintain this – note that this is entirely retrospective, i.e.&nbsp;no specific intervention was administered to these individuals. A smaller study by <span class="citation" data-cites="perri2008extended">Perri et al. (2008)</span> found that following a 10.0kg mean weight loss over a 6-month treatment period, weight regain ranged from 1.2 to 3.7 kg (extended-care <em>vs.</em> education control), resulting in a sustained weight loss of 6.3 to 8.8 kg – nothing to sneer at, especially considering that this was specifically in an underserved rural setting. It turns out that when we look at studies that have been performed in the last 20 years, we find that the 95% figure is not only wrong, but the exact opposite of the truth. Sustained weight loss is the rule, not the exception, especially in the context of a comprehensive treatment plan, and few studies bear out the idea that those that lose weight will regain <em>more</em> than what was lost. Even where weight is regained, there are often lasting benefits. <span class="citation" data-cites="10.1016/j.amjmed.2012.10.010">Unick et al. (2013)</span> examined 5,145 individuals with Type 2 diabetes and a 4-year follow-up, and found that not only did they still have on average a 4% or so weight loss after 4 years but also significant improvements in their HbA1c, blood pressure and lipid profile. We also know, from <span class="citation" data-cites="diabetes200910">Group et al. (2009)</span>, that weight loss itself may prove protective against Type 2 diabetes even if some of the cohort regains the lost weight.</p>
<p>So, we know the 95% figure is wrong. Why, then, does it persist? I think there are two reasons for this. One is that it’s a convenient excuse for people who don’t want to put in the work. The other is that it’s a convenient excuse for people who want to sell you something. The former is a matter of personal responsibility, and I’m not going to tell anyone how to live their lives. The latter, however, is a matter of public health, which is being actively harmed by a motley assortment of ignorami, grifters and – worst of all – the occasional medical professional who has not done their research and/or slept through their biostats classes who all repeat uncritically the 95% myth (while politely eliding any mention of its dubious parentage). Social media has responded to harmful medical misinformation during the COVID-19 pandemic with the zeal of the Spanish Inquisition on steroids. It is, then, rather incongruent that the same platforms are happy to let the 95% myth run rampant, despite the fact that it is demonstrably false and demonstrably harmful.<sup>10</sup></p>


<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;I’m not generally a fan of censorship, but I like public health and I like consistency. I’d like every video repeating this myth to have a massive big red sticker on it saying “this video contains medical misinformation”.</p></div></div>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The 95\% Myth},
  date = {2023-12-27},
  url = {https://chrisvoncsefalvay.com/posts/95-percent-myth/},
  doi = {10.59350/xmf8m-t1d22},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The 95% Myth.”</span> <a href="https://doi.org/10.59350/xmf8m-t1d22">https://doi.org/10.59350/xmf8m-t1d22</a>.
</div></div></section></div> ]]></description>
  <category>fitness</category>
  <category>science</category>
  <category>bad science</category>
  <guid>https://chrisvoncsefalvay.com/posts/95-percent-myth/</guid>
  <pubDate>Wed, 27 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/95-percent-myth/93e3df55-79ec-44b0-87c1-03bbca32ee46.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>What I learned from getting bodied by a robot.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</link>
  <description><![CDATA[ 




<p>Say you’re busing tables and you’re trying to pass someone in a wheelchair. What do you do? Do you say “excuse me” and wait for them to move? Do you say “excuse me” and then try to pass them? Do you just try to pass them? Do you say nothing and just try to pass them? All of these are, actually, pretty legitimate answers.</p>
<p>Now, say you’re a robot. What do you do? The robots that are currently deployed at the United Airlines lounge at SFO (one of <a href="https://www.reddit.com/r/unitedairlines/comments/10pdr0u/the_robots_arrived_today_at_the_new_ord_c_club/">these</a>, courtesy of <a href="https://www.bearrobotics.ai">Bear Robotics</a>) thought beeping, then bodying me with full military power was the right answer. I’m not sure I agree.</p>
<p>(I’m fine. The robot’s fine. This post isn’t about that. I play a sport that involves crazy strong people in wheelchairs that look straight out of Mad Max ram into each other at full speed to let their violence out. It takes a bit to dislodge me.)<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;A few people have asked me for details on the incident. It really wasn’t even big enough a deal to report to the lounge stewards – there was no injury to me, no damage to the robot and overall no harm done. The robot kept bumping into my chair and pushing against me as I was trying to get out of the way, which of course made everything harder. I was a little concerned that its frantic efforts might result in the dishes it was carrying getting dislodged and falling on me, but thankfully that didn’t happen. It did, however, shine a light on the lack of modern robotics’ lack of understanding the needs of customers with disabilities, and I am more than a little concerned by that – not everyone in a wheelchair is a 6’2 180lbs adaptive athlete. We can do better than this. We <em>have</em> to do better than this.</p></div></div><p>The point is about this <em>modus vivendi</em> between humans and artificial intelligences that we really haven’t worked out adequately.</p>
<section id="mental-models" class="level2">
<h2 class="anchored" data-anchor-id="mental-models">Mental models</h2>
<p>Humans aren’t mind-readers… but they really, really want to be. We’re constantly trying to figure out what other people are thinking. Our survival as a species has depended on it. The cost of this is worrying what people think about us every time we enter a room. But the benefit, oh, the benefit of it: we can create models of other people’s minds, and we can use those models to predict what they’re going to do next. This is a very useful skill to have when you’re, say, hunting a woolly mammoth. As you do.</p>
<p>This means that in trying to determine what to do next, we don’t just reason by some goal-directed reinforcement learning framework sitting on top of some observations of reality. We create a model not just of reality but of other minds, too.</p>
<p>Part of that is to understand what other minds do: their capabilities, but also their limitations.</p>
</section>
<section id="your-fear-in-a-handful-of-dust" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="your-fear-in-a-handful-of-dust">Your fear in a handful of dust</h2>
<p>Consider, for instance, fear. A robot has about as little need, <em>ex facie</em>, to understand that humans are afraid as it has for empathy to understand that some people use a wheelchair and need some time to get out of the way. A machine isn’t mortal in the conventional sense. It has had no need to develop the complex neurological-psychological responses that, in excess, give us, say, a fear of heights (because <em>some</em> fear of heights is definitely evolutionarily useful!).</p>
<p>Fear is not only a useful emotion to have, it’s also something humans <em>have</em>, like it or not. The consequence is that anyone and anything that seeks to interact with humans has to understand that fact. If you don’t, you’re collectively going to have a bad time.</p>
<p>And so, if a human is working on, say, a roof, they will reason from the place the poet called “the unstill tremors of the fearful heart”.<sup>2</sup> A machine working on its own on a roof can ignore fear as much as it desires. A machine that seeks to interact with humans and live in human society, however, cannot. And there’s the rub. It’s easy to create a machine that does surgery. It’s near impossible to create one to <em>assist</em> in surgery. Interacting with humans is a tough call, and it’s not just because we’re a bunch of weirdos (though that definitely contributes).</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Dyneley Hussey, who deserves to be known way more than he is.</p></div></div></section>
<section id="the-problem-of-other-minds" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-of-other-minds">The problem of other minds</h2>
<p>A robot, then, doesn’t have to understand that it has, or rather <em>is</em>, a mind of a sort. But it absolutely has to understand that <em>other humans</em> have minds of their own, and that those minds do and think stuff.</p>
<p>The problem of developing a theory of mind is one of those watersheds of artificial intelligence that will have a clear before and after. There isn’t much room for gradualism here. A machine that can understand that other humans have minds of their own is going to be one that will be able to interact with other humans and live in some level of comity, and one that doesn’t, won’t. This is the next big thing in AI, and it’s going to be a big thing indeed.</p>
<p>Or maybe I’m wrong. Who knows. I did just get bodied by a robot, after all.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {What {I} Learned from Getting Bodied by a Robot.},
  date = {2023-12-12},
  url = {https://chrisvoncsefalvay.com/posts/ai-human-interaction/},
  doi = {10.59350/r8k9q-zdm06},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“What I Learned from Getting Bodied by
a Robot.”</span> <a href="https://doi.org/10.59350/r8k9q-zdm06">https://doi.org/10.59350/r8k9q-zdm06</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>disability</category>
  <category>human-computer interaction</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</guid>
  <pubDate>Tue, 12 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-human-interaction/cd091fdb-e9e7-49d9-9b11-2b1eb92e2f86.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
