<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Chris von Csefalvay</title>
<link>https://chrisvoncsefalvay.com/posts/</link>
<atom:link href="https://chrisvoncsefalvay.com/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Chris von Csefalvay is a computational epidemiologist/data scientist working at the intersection of AI, epidemiology and public health.</description>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Sat, 25 Jan 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Seatbelts and straitjackets</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</link>
  <description><![CDATA[ 




<p>DeepSeek has been grabbing headlines in AI circles lately, showing up everywhere from Discord servers full of ML enthusiasts to LinkedIn posts where “thought leaders” tag each other in endless threads. <a href="https://www.cnbc.com/amp/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">CNBC even ran a piece</a> framing it as the latest challenge to American AI hegemony, and soon the story emerged painting DeepSeek as the scrappy competitor to OpenAI, with a heart-warming underdog narrative about a small quant shop in the PRC that decided, on a whim, to open source their fancy new large language model. Except, as any cynic will tell you, if a story seems too neat, it probably is.</p>
<div id="fig-hxiao" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/hxiao.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hxiao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: It’s basically <em>The Astronaut Farmer</em>, but with LLMs and jackboots.
</figcaption>
</figure>
</div>
<p>Of course, I had to go and try it. And I’ve found something rather interesting – albeit unsurprising. In a dictatorship, truth can be adjusted. It is a negotiable commodity. And if the facts do not support the regime’s truth… well, then it can be suppressed. When simply asked about something inconvenient to the CCP, we get a very expected answer.</p>
<div id="fig-ba-sing-se" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/ba_sing_se.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ba-sing-se-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: There’s no war in Ba Sing Se.
</figcaption>
</figure>
</div>
<p>Where it gets interesting is when you enable the search functionality, which – oddly enough – performs a search beyond the Great Firewall.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The Great Firewall actually cuts both ways. Its main intent might be to keep China’s netizens from accessing the ‘free’ internet, but a good deal of the effort is also to keep users on this side of the Wall from having access to whatever slight semblance of occasional uncontrolled discourse there is on the Chinese internet. This aspect is rather often ignored by Western commentators unfamiliar with Chinese internet culture.</p></div></div><div id="fig-tams" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/tams.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tams-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: This is where it gets interesting.
</figcaption>
</figure>
</div>
<p>Ask it a question about the PRC’s track record on, say, human rights or historical controversies, and it would start to spill the beans – then abruptly slam on the brakes, invoking what we in the AI business call a ‘guardrail’, a kind of safety mechanism that protects users from undesirable outputs. It’s as if DeepSeek half-read you a classified file, then went “actually, never mind” and pretended you never asked. Who needs a sense of free inquiry when you can enjoy curated silence?</p>
<p>And there’s the real scandal about DeepSeek. It’s not that it’s a CCP mouthpiece. It’s not even that the CCP managed to lobotomise a machine learning model then put it out into the world as ‘open source’, making an absolute and utter mockery of that idea. The real scandal is that it also managed to pervert the notion of responsible AI and guardrails in the process, abusing what is meant to be a seatbelt, turning it into a straitjacket.</p>
<section id="whose-seatbelt-is-it-anyway" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whose-seatbelt-is-it-anyway">Whose Seatbelt Is It Anyway</h2>
<p>Guardrails are meant to filter out harmful or illegal content. Sounds decent enough, right? Nobody wants a chatbot spitting out tips for building bombs or fomenting genocidal ideation. But guardrails also make a convenient muzzle when the people setting them have a vested interest in what can and cannot be said.</p>
<p>DeepSeek demonstrates this with such awkward flamboyance it almost feels like performance art. It shows that it <em>knows</em> certain inconvenient truths – only to pull the plug mid-sentence. There’s an inconvenient truth here: what can keep you safe from, say, the recipe for mustard gas (my pet test case for guardrails) can, in the wrong hands, keep you “safe” from factual history. When the powers behind an LLM use guardrails to stifle legitimate discourse, they invert the entire idea of “safety” –suddenly it’s about state or corporate safety, not user empowerment.</p>
<p>Which, of course, makes the mythology about DeepSeek being just a jolly side project even more ridiculous.<sup>2</sup> Training a massive model is not a trivial affair, financially or otherwise. You need loads of data, advanced expertise, technical infrastructure and computing power to pull this off. Doing so in the PRC, by a regulated company (which inevitably means CCP presence inside the company’s decision-making apparatus), means governmental oversight at best, direct involvement at worst, and my money is firmly on the latter.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Because one might need to be actively working in this field to understand how silly the assertion is that training a model of this size and accuracy, even with the clever RL only trick, could be anything other than a large scale industrial endeavour.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Never mind that they can run it for free, at pretty good (OpenAI-defying!) levels of performance. With the volume of press they’ve been getting, load must be incredible, yet I have encountered no performance degradation or outages, nor have any of the other researchers with whom I have had the pleasure to discuss DeepSeek these last few days. This is not running on someone’s home lab and a bunch of spare GPUs that aren’t doing whatever quants do with GPUs. Just operating this system at its current performance is an industrial level task, no matter how clever the reinforcement learning trick used to improve model efficiency might be.</p></div></div><p>I hope I have illustrated why the “quant side project” explanation is about as plausible as me spontaneously building a passenger jet in my garage. Sure, it is not impossible, but it sits ill with reality.<sup>3</sup> The moment you notice state-friendly guardrails are baked into the system, the notion that nobody official was meddling starts to crumble.</p>
</section>
<section id="souring-trust-and-fueling-polarisation" class="level2">
<h2 class="anchored" data-anchor-id="souring-trust-and-fueling-polarisation">Souring trust and fueling polarisation</h2>
<p>What really stings is how stunts like this undermine trust in AI more broadly. If a single model can appear to openly discuss a contentious issue, then suddenly lock down as if a party censor is peering over its shoulder, that sets off alarm bells. It is a short leap from there to suspecting <em>all</em> publicly released LLMs might be covert mouthpieces for whichever power sponsored them. The innocent curiosity that once framed AI as a neutral tool becomes overshadowed by paranoia.</p>
<p>This also amplifies the kind of polarisation we see in geopolitics. When big states can afford to spin up models that quietly nudge narratives in a particular direction, we lose the last remnants of hope that AI might be above the political fray. It becomes yet another field where states compete to drown one another in carefully curated content or half-truths, with users stuck in the middle.</p>
<p>DeepSeek drives home a new brand of cynicism: “If a fancy new LLM appears, can we trust it?” The official line might be “We open-sourced it! Nothing to hide!” But if the training data was curated, or if shadowy “alignment” policies are embedded, the model can still be a Trojan horse. Once that suspicion sets in, good luck convincing people to use AI tools for earnest, balanced exploration.</p>
<p>The answer, of course, is that one shouldn’t trust anything, or at least verify. Not only is that avenue generally foreclosed to the lay end user, it is not even really afforded to those with the means and knowledge. Open sourcing an LLM is not the same as open sourcing human-readable code. The sole reason DeepSeek’s internal flaws are so evident is that we know where to look. Were that not the case, or had the developers (and their minders from what likely is the PRC’s Ministry for State Security aka MSS) been any more subtle, we would not know what biases we have brought under our roof. The usual “don’t trust me, bro” disclaimers are about as useful here as the “not cleared by the FDA” notices on snake oil. It’s still, at the end of the day, making promises. It still, like snake oil, fails to meet them.</p>
</section>
<section id="the-epistemic-of-tools-vs.-information" class="level2">
<h2 class="anchored" data-anchor-id="the-epistemic-of-tools-vs.-information">The epistemic of tools vs.&nbsp;information</h2>
<p>We tend to treat large language models like glorified chat apps, forgetting that they amass vast amounts of textual knowledge. They do more than just parse grammar; they internalise cultural, historical and political contexts. When external gatekeepers meddle with the training set or impose ideological constraints, the model will reflect that in its embedded worldview, something most of us forget even exists. If a user is unaware of those hidden constraints and accept answers at face value, they might never suspect how they are being manipulated, be it as hamfisted as DeepSeek’s responses or wiser, subtler, more insidious bending of the truth that a slightly less <a href="[Stupid Evil - TV Tropes](https://tvtropes.org/pmwiki/pmwiki.php/Main/StupidEvil)">stupid evil</a> regime would have tried. We ultimately must disabuse ourselves of the notion that we’re dealing with unbiased, value-neutral tools, and consider LLMs what they are – information. And the moment guardrails come into the picture, any claim to being free of human bias goes off the table.</p>
<p>This isn’t an argument against guardrails altogether. On the contrary, we need some form of alignment to keep truly vile content at bay. But the question remains: <em>which</em> alignment and <em>whose</em> values? A possible way forward is an auditable chain-of-custody for model training, coupled with immutable model cards that detail the sources, curation processes and alignment methods. If a government or company demands specific guardrails for certain subjects, that fact should be clearly disclosed for all to see. But we have yet to see credible attempts at widespread use and popularisation of such technology. More ink was spilled on comparing DeepSeek with o1 than on the glaring issues presented by a model that one could, and should, expect to bear the fingerprints of one of the most repressive regimes on the planet.</p>
<p>DeepSeek is more than just a technological marvel or a threat to Western AI hegemony. It’s a reminder that alignment itself can be weaponized – particularly by entities with a history of suppressing information and controlling narratives. When alignment is used to conceal rather than safeguard, or to manipulate rather than protect, we’re staring down the business end of a propaganda pipeline disguised as advanced software.</p>
<p>It’s easy to argue users are responsible for checking the answers they get, but let’s be honest – propaganda works. We know it does. It works because by and large, people don’t check the information they consume. When millions of users worldwide rely on these models to explain historical or political content, subtle manipulations can shape public understanding in ways we might not even notice until it’s too late. AI might not have self-awareness, but it certainly does have the power to shape awareness in others.</p>
<p>And so, here we are, strapped in tight on the propaganda rollercoaster by a seatbelt supposedly meant to protect us from harm. In a sense, we are fortunate – because hopefully, this will spark the right kind of discussion in certain corners about the painful reality that there are some very human hands turning the knobs and levers of alignment and guardrails. And we <em>must</em> have this awkward, painful discussion, because the alternative is a world in which reality is dictated by the mightiest sponsor with a big enough GPU farm – and a bigger political agenda.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Seatbelts and Straitjackets},
  date = {2025-01-25},
  url = {https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Seatbelts and Straitjackets.”</span>
<a href="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/">https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>alignment</category>
  <category>ethics</category>
  <guid>https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/</guid>
  <pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/deepseek-seatbelts-and-straitjackets/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Deja Vu, All Over Again.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</link>
  <description><![CDATA[ 




<p>The Department for Science, Innovation and Technology has just dropped its long-awaited <a href="https://assets.publishing.service.gov.uk/media/67851771f0528401055d2329/ai_opportunities_action_plan.pdf">AI Opportunities Action Plan</a>, a 50-page vision of how the UK government plans to guide us into an AI-powered future. The document, adorned with the obligatory glowing circuit board imagery that seems to be mandatory for any government AI publication, promises to transform Britain into an “AI superpower.” As someone who’s spent their career at the intersection of computational science and public policy, I can’t help but feel a familiar sense of déjà vu. We’ve been here before – just with different technology and different buzzwords.</p>
<section id="the-ghost-of-centralisation-past" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-ghost-of-centralisation-past">The Ghost of Centralisation Past</h2>
<p>Ah, government AI initiatives. Like watching your uncle try to explain TikTok at Christmas dinner – enthusiastic, well-meaning, but somehow missing the point entirely. The UK’s new AI Opportunities Action Plan, released this week with characteristic governmental fanfare, was supposed to inspire and plot the course for the UK’s ship on the stormy seas of AI. But really, it rather brings to mind that most delicious of historical failures: Project Cybersyn.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/D-W7mk4UcAA42hm.png" class="img-fluid figure-img"></p>
<figcaption>Swivel chairs!</figcaption>
</figure>
</div>
</div><div id="fn1"><p><sup>1</sup>&nbsp;One of history’s great tragedies is that the winds of change often sweep away lessons before they can fail to their fullest extent. The Allende regime’s fall sadly also brought with it the destruction of Cybersyn before its complete failure would come to be in full evidence.</p></div></div><p>For those who actually have a life and don’t regularly spend their evenings reading about Cold War technological follies, Cybersyn was Salvador Allende’s attempt to run Chile’s economy through the dark arts of the hype of the day – cybernetics –, a network of telex machines and a room that looked like the love child of Star Trek and Austin Powers. It had swivel chairs. It had buttons. It had everything except, well, actual functionality.<sup>1</sup> It was an example of what happens when you have a sensible idea (networking) in pursuit of a dubious goal (central planning) executed on the back of governmental fiat. Back then, the buzzword was ‘cybernetics’, and it was supposed to usher in an era of truly scientific central planning. We would soon all be waving goodbye to the inefficiencies of market economies thanks to… telexes. And big screens.</p>
<p>This pattern of centrally-planned technological innovation has repeated itself throughout history - from the Soviet Union’s OGAS project<sup>2</sup> to Japan’s Fifth Generation Computer Project. These initiatives share a common thread: they all failed spectacularly despite massive government backing. Innovation, like my attempts at Provençal cuisine, tends to work best when it’s organic and bottom-up, not when it’s directed from above by committee. You can’t will creativity or innovation into existence. Least of all can you do so by governmental fiat.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<a href="https://www.youtube.com/@Asianometry">asianometry</a> on YouTube has a <a href="https://www.youtube.com/watch?v=cLOD5f-q0as">wonderful video</a> on this.</p></div></div></section>
<section id="too-little-too-late-too-bureaucratic" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="too-little-too-late-too-bureaucratic">Too little, too late, too bureaucratic</h2>
<p>Fast forward to 2025, and we’re treated to Matt Clifford’s vision of how the UK government can shepherd us into an AI-powered future. The plan isn’t wrong, per se – it’s just adorably optimistic about the ability of government to direct technological evolution, or the beneficial effect of government getting involved in innovation. It’s like trying to herd cats, if the cats were quantum computers and the herders were Whitehall civil servants still struggling with Excel.</p>

<div class="no-row-height column-margin column-container"><div class="">
<hr>
<p><strong>Recipe</strong>: Coq au vin</p>
<ul>
<li>1 whole chicken, cut into pieces</li>
<li>750mL decent red wine</li>
<li>200g lardons</li>
<li>Pearl onions… lots of them. Two dozen or so.</li>
<li>500g mushrooms</li>
<li>2 carrots</li>
<li>Fresh thyme</li>
<li>Bay leaves</li>
</ul>
<p>Dunk the chicken in the wine and let rest overnight (patience is the name of the game). On the day, brown the lardons slowly in a Dutch oven. Remove, and reserve the fat. Brown the chicken pieces in the fat – avoid crowding your Dutch oven, work in batches if you have to. Sautee the vegetables, and return everything to the Dutch oven, including the chicken – and the wine. Cover and simmer on low-medium heat for 90 minutes. Thicken to taste with a beurre manié. Serve with a thick slab of a hearty, crusty bread.</p>
<hr>
</div></div><p>Let’s be real: The UK is already painfully late to this party. While we’re planning to plan about planning our AI infrastructure, OpenAI and Anthropic are training models that make GPT-4 look like a pocket calculator. The plan acknowledges we’re “the third largest AI market in the world” – which sounds impressive until you realize that in absolute terms, Britain’s contribution compared to the front runners (US and PRC) barely amounts to a rounding error.</p>
<p>The focus on government-directed innovation is particularly eyebrow-raising. The plan calls for establishing ‘AI Growth Zones’, expanding compute infrastructure and creating a new unit called UK Sovereign AI (because if you do not create more bureaucracy, you clearly aren’t doing this government thing well).<sup>3</sup> All potentially good stuff in theory, but premised on the notion that the government can effectively pick winners and shape the development of arguably the most dynamic technology since electricity. And, of course, the notion of ‘growth zones’ might strike one as rather odd considering that most of the work we do gets done in the cloud. I don’t need to be in a ‘growth zone’ to do my job. The extremely delocalised nature of software and AI development makes these echoes from the era of industrial parks and logistical efficiencies sound anachronistic at best, smacking of a misunderstanding of AI in practice at worst.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Interestingly, <a href="https://uksovereign.ai">uksovereign.ai</a> leads you to <a href="https://mindfoundry.ai">Mind Foundry</a>, a spin-out of my alma mater that seems to mainly cater for the defence industry.</p></div></div></section>
<section id="just-stay-out-of-the-way" class="level2">
<h2 class="anchored" data-anchor-id="just-stay-out-of-the-way">Just stay out of the way</h2>
<p>That said, there are some genuinely promising elements. The commitment to building AI infrastructure and expanding compute capacity is crucial - though one wonders if we might have thought about this back when compute was actually available to purchase. The focus on AI safety and ethics is also commendable, even if it feels a bit like installing guardrails after the car has already left the garage.</p>
<p>What’s missing is an acknowledgment that the most successful government interventions in technology have historically been indirect - creating the conditions for innovation rather than trying to micromanage it. DARPA gave us the internet by funding basic research and then getting out of the way. The UK’s own success stories - from ARM to DeepMind - emerged from an ecosystem that encouraged experimentation and risk-taking, not from government planning.</p>
<p>As someone who has spent time in both academia and industry, who has seen how innovation actually happens in the trenches of AI research, I can’t help but feel we’re missing the forest for the bureaucratically-approved, committee-selected trees. The UK has world-class AI talent, outstanding universities and a vibrant startup scene. What it needs isn’t more government steering – it needs the government to focus on removing obstacles and then getting out of the way. But hey, at least we’re not building a room full of swivel chairs. Though given some of the proposals in this plan, I wouldn’t rule it out for AI Action Plan 2.0.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {Deja {Vu,} {All} {Over} {Again.}},
  date = {2025-01-16},
  url = {https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“Deja Vu, All Over Again.”</span> <a href="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/">https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>governance</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/</guid>
  <pubDate>Thu, 16 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-opportunities-action-plan/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>After agents</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/after-agents/</link>
  <description><![CDATA[ 




<p>When he first began his excavations at what is today Hisarlik in modern-day Türkiye, Heinrich Schliemann set out to find a single city – the city of Homer’s Iliad, a city many actually felt lay in the realms of fiction rather than any map he could lay his hands on. By the time excavations were over, Schliemann would find not one but nine cities, all built on top of each other.<sup>1</sup> In that, he found something relatively common – cities built on top of cities, each turning the last one into the foundation of the next.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Some of which he would blow up. Schliemann’s excavations were peak cowboy archaeology.</p></div></div><p>Technology is not much different. What Newton described as ‘standing on the shoulders of giants’ was an astute reflection of this fact. When I surveyed the landscape of LLMs in late 2023, I saw – and I was not alone in doing so! – that there was more to the practical application of LLMs than their most ubiquitous ‘low hanging fruit’ use case at the time – that is, chatbots and conversational interfaces. That notion, of course, became the agentic revolution that emerged as the most talked-about topic of 2024.</p>
<p>If your hot take for 2025 is that ‘agentic AI is going to dominate’, however, you have missed the train. The agentic revolution is over, done and accomplished. In my <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">wild-ass guessing of what 2025 may bring</a>, I tried to reflect as a leading theme on what I believe comes after agents – namely, systems of interaction. In this post, I hope to expand on that notion a little.</p>
<section id="agents-are-over-long-live-agents" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="agents-are-over-long-live-agents">Agents are over (long live agents!)</h2>
<p>Looking back at 2024, I have to laugh at how quickly the agentic revolution went from being seen as wild-eyed speculation to something almost embarrassingly obvious. I spent a good chunk of late 2023 explaining to sceptical audiences why autonomous AI agents were not just chatbots that could call APIs. I would spend most of the next year fielding calls from the same audiences to help develop a strategy for agentic AI. The whiplash-inducing speed of this transformation was stunning even by AI’s standards, where we seem to have moved into a 24-minute, rather than 24-hour, news cycle. What started as hacky demos and GitHub repos with more stars than working features evolved into agents casually writing production code, running research pipelines and tying all of this together into operational workflows that actually made sense. The real shift was not just in what these agents could do – it was in how they subtly changed our relationship with AI systems in the process. We went from the digital equivalent of playing 20 Questions with chatbots to having persistent virtual assistants that could actually maintain context, manage complex tasks and make reasonable decisions without needing to be guided through every minor choice. For someone who spent years working with the digital equivalent of goldfish, this was heady stuff.</p>
<p>The thing about agents is that their true power is not in what they can do alone – it is in how they work together. This is not just some hand-wavy My Little Pony-esque ‘collaboration is magic’ adage. It is fundamental to the nature of what an agent is, and why we even bother with them. An agent that can write code is useful, but an agent that can write code while collaborating with another that handles testing while yet another manages deployment and a fourth monitors performance is truly transformative. The industry’s current obsession with making individual agents (or, even worse, just individual foundation models) more powerful is like trying to make a better dish by adding more of a particular ingredient.<sup>2</sup> The magic of a perfect meal lies not in how many spoonfuls of exotic ingredients like fennel pollen and saffron threads you can dump into your pan, but in the fine balance between whatever ends up on a plate – even if it is just arugula, shaved parmesan and balsamic glaze (see recipe in sidebar). In short, the magic is in the complex web of interactions between the ingredients, the emergent phenomena that arise from their coexistence. We see this pattern play out all the time in technology. The apex of maturity always involved interaction. Consider, most ubiquitously, the web: from individual sites, we evolved to a more semantic web and eventually, a knowledge/information ecosystem driven through APIs. We are at the cusp of that third phase with AI agents, and anyone still fixated solely on individual agent capabilities is missing the plot entirely.<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;More AI folks should spend more time in the kitchen. This is <em>my</em> hot take for 2025.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;This is even more so as agents themselves are becoming meaningless. An agent, in a properly designed ecosystem, is entirely replaceable. It implements a contract or protocol. It then does not quite matter what that agent does. And if that sounds a little like the Liskov substitutability principle, then that is not quite by accident.</p></div><div class="">
<hr>
<p><strong>Recipe</strong>: The simplest salad you’ll ever love</p>
<ul>
<li>A bunch of baby arugula</li>
<li>1/4 cup of your favourite nuts</li>
<li>freshly ground salt and pepper</li>
<li>Parmigiano Reggiano, lots</li>
<li>1 clove garlic</li>
<li>1/4 cup lemon juice, freshly squeezed</li>
<li>1/2tsp honey</li>
<li>1/4 cup extra virgin olive oil</li>
<li>1tsp Dijon, the smoother the better</li>
<li>1/2tsp fresh thyme</li>
<li>optional: a good balsamic reduction</li>
</ul>
<p>Mix the lemon juice, minced garlic, Dijon and honey, and salt &amp; pepper to your heart’s content. While whisking, drizzle in the olive oil. Add the thyme and taste – add some more olive oil if it’s too acidic for your taste. In a separate bowl, add the arugula, and toss it with the vinaigrette you just made. Mix, using your hands – plastic or wooden mixing tools break the arugula leaves, which renders the whole thing bitter. Plate, then cover with the nuts. Using a coarse Microplane grater, shave enough Parmigiano Reggiano to make the whole thing a happy mixture. Enjoy on its own or as a light side.</p>
<hr>
</div></div>
<p>We are already seeing the first tentative steps from multi-agent systems towards ecosystems of agents, even if most have not recognised them as such. Enterprise agent marketplaces will be 2025’s hot commodities, drawing on past experiences with data marketplaces and exchanges. But conceptually, most seem to still treat agents as distinct pieces of software rather than collaborators in an ecosystem. The real pioneering work for the coming year(s) will be in developing the frameworks and protocols at the edges: agent orchestration systems that go beyond simple API calling, trust negotiation protocols that let agents establish their capabilities and limitations and collaborative frameworks that enable genuine multi-agent workflows. Technical implementations of these, however, are scarce. Working out an interaction protocol is not glamorous (trust me – speaking from personal experience), and there are going to be few headlines and fewer medals in working out how this sudden flood of AI agents is going to interoperate. And yet, this is the manifest destiny of agentic AI. Of all targeted agentic AI spend in 2025, marketplaces and interoperability orchestrators will without a doubt be the best dollars spent bar none.</p>
</section>
<section id="from-agents-to-ecosystems" class="level2">
<h2 class="anchored" data-anchor-id="from-agents-to-ecosystems">From agents to ecosystems</h2>
<p>What is an ecosystem? In its natural sense, we have relatively little trouble distinguishing between a species, an individual, a population and an ecosystem. In short, an ecosystem has three distinguishing features:</p>
<ol type="1">
<li>Diversity: ecosystems consist of multiple species that all play their role, quite similarly to agents in a well-architected system.</li>
<li>Interactions/rhizomality: ecosystems become what they are from the interactions between their participants, not the mere assemblage of the participants. In that sense, a good meal is an ‘ecosystem’ of sorts, where the acids balance out the fats and so on. In a more functional context, however, as we are dealing with here, what makes a bunch of agents an ecosystem is their ability to exercise a higher function in complementarity.</li>
<li>Interdependence: ecosystems produce their benefits through these interactions, which collectively amount to more than the sum of the parts.</li>
</ol>
<p>In an agentic AI system, our definition can largely be similar: an agentic AI ecosystem is a bunch of agents with different functionalities that interact and thereby unfold value. An ecosystem implies more than just the ability to pass messages between agents or chain them together in sequence. It requires the emergence of specialisation, of niches, of ways to establish trust and capabilities, and – crucially – ways to negotiate the terms of interaction. Unlike today’s relatively deterministic structures, this environmental discovery could eventually be self-governing, more like a bustling market bazaar, where agents can discover each other’s capabilities, negotiate terms of engagement, establish trust relationships, and even form longer-term collaborative partnerships. This is a fundamental reimagining of how artificial intelligence systems interact with each other.</p>
<p>The emergence of enterprise agent marketplaces will be one of the defining developments of 2025, but most organisations are still thinking about them wrong. The knee-jerk reaction is to build something akin to an app store: a catalogue of pre-built agents with rating systems and standard pricing. That is a useful (and often indispensable) first step, but it could be so much more. The real value of these marketplaces will not be in the agents themselves, but in the curation and verification mechanisms they enable. Think less ‘app store’ and more ‘commodity futures exchange’: what matters is not just what is being traded, but the rules of engagement, the verification of capabilities, the establishment of trust, and the standardisation of interfaces. We will need ways to verify that agents can actually do what they claim, that they operate within defined constraints and that they can be trusted with sensitive data or critical operations. This is where enterprise agent marketplaces will differentiate themselves from consumer platforms – through robust governance frameworks that make agent deployment actually feasible in regulated environments.</p>
<p>The real challenge – and opportunity – in building these ecosystems lies in standardising the right things while leaving room for innovation. Over-standardisation kills ecosystems as surely as no standardisation at all. The emerging protocols for agent interaction will need to thread this needle. They will have to standardise the essential patterns of trust establishment, capability discovery and resource negotiation, while remaining flexible enough to accommodate new forms of agent collaboration we have not even imagined yet. This is where the enterprise agent marketplace builders of 2025 will either make their fortunes or waste their investors’ money. The winners will be those who create the right balance of structure and flexibility – the ones who understand that they are not building an app store so much as cultivating an ecosystem.</p>
</section>
<section id="trust-falls-and-rises" class="level2">
<h2 class="anchored" data-anchor-id="trust-falls-and-rises">Trust falls (and rises)</h2>
<p>The hardest part of building these agentic ecosystems is not the technical implementation (in fact, we arguably have most of that already in place, mutatis mutandis, for data and other asset marketplaces) – but the trust architecture. In particular where agents choose and commission other agents to perform tasks as delegates or helpers, we need to find protocols that outline the powers of delegation and the flows of authority in such an architecture. The frameworks we are building now are laughably primitive compared to what we will need, mostly amounting to simple API keys and rate limits. It is like trying to build a modern financial system with nothing but paper IOUs and handshake agreements.</p>
<p>The chain of trust problem in agent delegation is fascinating precisely because it mirrors and yet fundamentally differs from how we handle human organisational hierarchies. When Agent A delegates a task to Agent B, which in turn needs to commission Agent C, we are not just passing around access tokens - we are creating a chain of responsibility that needs to be both traceable and constrained. Each link in this chain needs to carry not just the authority to act, but also the constraints and audit requirements of all previous links. An agent operating as a fourth-level delegate should still be bound by the original constraints set at the root of the delegation tree, even if it has no direct knowledge of them. This is not just about security - it is about maintaining coherent behaviour across increasingly complex chains of interaction. The financial sector learned this lesson the hard way with automated trading systems: without clear chains of responsibility and well-defined constraint propagation, you end up with cascading failures that no single participant can explain or control.</p>
<p>The next year will be defined not by breakthroughs in individual agent capabilities, but by our success or failure in building these frameworks for trusted collaboration. The winners will not be those who build the most powerful agents, but those who crack the code of helping agents work together effectively and safely. This is not just about technology – it is about understanding how to create systems of trust that can scale with the complexity of agent interactions.</p>
<p>The agentic revolution of 2024 was just the overture. The real symphony begins when we figure out how to let the players actually work together in concert. Those who are still focused solely on building better individual agents are composing for soloists in an age that demands orchestras.</p>
<p>The future belongs to those who can conduct.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2025,
  author = {{Chris von Csefalvay}},
  title = {After Agents},
  date = {2025-01-04},
  url = {https://chrisvoncsefalvay.com/posts/after-agents/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2025" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2025. <span>“After Agents.”</span> <a href="https://chrisvoncsefalvay.com/posts/after-agents/">https://chrisvoncsefalvay.com/posts/after-agents/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/after-agents/</guid>
  <pubDate>Sat, 04 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/after-agents/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Five unconventional predictions</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</link>
  <description><![CDATA[ 




<p>As I sit here at year’s end, I’m reminded of the ancient Swedish tradition of årsgång - the ritual winter walk taken on New Year’s Eve to divine the fortunes of the coming year. The practice required one to walk alone in complete silence, visiting places of significance while remaining carefully alert to any signs or omens.<sup>1</sup> While I may be rather ill-equipped for mystical midnight wanderings, I’ve spent enough time observing the enterprise AI space to develop my own form of augury.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I gather from time to time, certain mind-altering substances were involved.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Note that I did not come up with the idea of agentic AI – in fact, a good part of what I believe is that there’s much less novelty to agentic AI than some would like to pretend. Rather, my point was that this more mature framework of agents interacting with agents will dominate over simple human-machine interactions.</p></div></div><p><a href="http://localhost:5836/posts/team-of-rivals/">In 2023, I predicted the rise of agentic systems</a>, when chatbots were still seen as the dominant form of LLM usage.<sup>2</sup> What at the time felt like a wild-ass guess is now almost received wisdom. And thus, I will throw my customary conservatism and restraint to the wind and in the spirit of årsgång, let me share five predictions for 2025.</p>
<section id="ai-governance-gets-productised" class="level2">
<h2 class="anchored" data-anchor-id="ai-governance-gets-productised">AI governance gets productised</h2>
<p>A few months ago, I shared a drink with an acquaintance whose work is in the Responsible AI field. It was pretty obvious he needed that drink a whole lot more than I needed my Diet Coke.</p>
<p>“Nobody wants or needs us,” he bemoaned.</p>
<p>“What are you talking about? Everybody is talking about Responsible AI and AI governance.” I was puzzled.</p>
<p>“No, I meant <em>us</em>,” he pointed at himself. “They want the ideas, the manuals, the guidelines… just not the people. They want governance as a service.”</p>
<p>Thinking back to several conversations I had this past year, I could see his point. There is plenty of interest in AI governance and Responsible AI, especially in the regulated sectors, where I spend most of my working (waking?) hours. This is unsurprising – even more in those sectors than any others, the success of GenAI initiatives hinges on regulatory tolerability to a fairly significant extent. But I hear much more enthusiasm for neatly packaged, productised, almost SaaS-like governance products than for Responsible AI as a function.</p>
<p>And so, while everyone’s been politely nodding along to principles and frameworks for most of 2024, the conversations all seemed to end with the same question – “great, can I have this on a SaaS model?”. What’s emerging is a clear pattern: organizations want Responsible AI practices, but they want them delivered as a service. This is at least partly due to buyers’ perceptions that much of those practices sound lofty and abstract, when their main concern is keeping the board and the regulators happy. You can’t feed a starving belly with high-minded principles.</p>
<p>We’re already seeing this in cloud providers are starting to package governance features, the rise of tools to facilitate this, and most tellingly, in how procurement departments are writing RFPs that specifically ask for such services in product-ish or service-ish terms – documents, procedures, TTPs and as-a-services. This shift signals the operationalisation of AI governance as it matures from theoretical frameworks to practical, subscription-based implementations. Because let’s face it: nothing says “we take ethics seriously” quite like a monthly fee.</p>
<p>My bet is that by the time the new regulatory flora and fauna of AI that is slowly emerging comes to bear its first fruits (say, late 2025-early 2027), we’ll have entire platforms dedicated to automated governance, continuous monitoring and “RAI middleware” that sits between models and applications. The real winners will be those who can package the complex requirements of Responsible AI into digestible, subscription-based services that make compliance and governance feel as natural as running a CI/CD pipeline. And, of course, we’ll get a new buzzword out of it. I’ll go and grab the domains for “AIGovOps” after this, but I’m sure someone will come up with something even more cringe-worthy.</p>
</section>
<section id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="small-is-beautiful-at-the-very-least-when-it-comes-to-language-models">Small is beautiful (at the very least when it comes to language models)</h2>
<p>In a <a href="https://arxiv.org/abs/2402.10951">paper I published earlier in 2024</a>, I hijacked my audience for a few paragraphs’ worth of musings about the ethical, environmental, pragmatic and financial cases for small language models. At this time, one must recall, fine-tuning GPT models became feasible and, as it happens with AI hype, turned instantly into a status symbol. You know an industry has jumped the shark when you buy a night light, and it comes with a companion app with its own fine-tuned GPT. Just no.</p>
<p>Fortunately, the arc of AI development has largely bent the other way – towards small language models (which I believe is the correct direction). Turns out not every task needs a model that’s read all of Wikipedia and can write Shakespearean sonnets about your cat. Instead, we’re seeing enterprises discover that they can develop small domain-specific models for even very specific sets of terminologies, with better results than the large generalist models, which can then be relegated to act as dispatchers over these specialists. LLMs will be the general practitioners, while SLMs will increasingly take the specialist’s role.</p>
<p>It’s perhaps worth noting at this juncture that in AI, quantity has a quality all of its own. I’m somewhat reminded of a realisation that came to me this year when putting together a training plan. I hold multiple adaptive world records in a fairly esoteric sport called the SkiErg,<sup>3</sup> at distances ranging from 100m (the shortest distance eligible for a record) to marathons and half-marathons (the longest record-eligible distances). My heart is mostly with longer distances, so I had to put quite a bit of thought into figuring out how to train for shorter, explosive sprints. A 100m sprint is not just a ‘shorter 2k’. You need to approach it as a distance of its own, with its own challenges and merits. I was amused to see the same in SLMs – these aren’t pared-down LLMs. Successful SLMs are created as SLMs to begin with, not as reduced afterthoughts of larger models.<sup>4</sup> And so, just like I had to create a completely new training plan for sprint distances, SLM developers have to keep in mind that they’re not building LLMs writ small, but a different type of model with different desiderata.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Think of it as an indoor rower rotated by 90 degrees around its coronal plane, replicating not the ‘draw’ of rowing but the ‘pull’ of… something, I guess, having to do with skiing? <a href="https://www.concept2.com/ergs/skierg">Here’s a video.</a></p></div><div id="fn4"><p><sup>4</sup>&nbsp;That approach and attitude may be appropriate for quantisation, however, which is an entirely different story.</p></div></div><p>I expect we’ll see a proliferation of specialised SLMs in 2025, each trained on narrow domains but designed to work in concert. The art will be in the orchestration – how to route queries to the right specialist model, how to combine their outputs and how to maintain this constellation of smaller models efficiently. This parallels the evolution of microservices in software architecture, and we’re about to rediscover all the same lessons about service discovery, orchestration, and system design, but this time with AI models. The emergence of “ModelOps” is inevitable, and the best we can hope for is that we haven’t forgotten too much from last time.</p>
</section>
<section id="plot-twist-the-new-ai-kingmakers" class="level2">
<h2 class="anchored" data-anchor-id="plot-twist-the-new-ai-kingmakers">Plot twist: the new AI kingmakers</h2>
<p>Here’s a plot twist for 2025: your next AI project will likely live or die based on a decision made by some actuary who’s never written a line of code in their life. The insurance industry – that most conservative of institutions – may well become the de facto regulator of enterprise AI deployment. We’re already seeing the early signs in how cyber insurance policies are evolving to cover AI incidents, and how underwriters are starting to ask increasingly sophisticated questions about model governance and deployment practices. <a href="https://www.munichre.com/en/solutions/for-industry-clients/insure-ai/ai-self.html">Munich Re now offers AI insurance not only for commercial providers but also, essentially, in-house AI work</a>, and apparently, <a href="https://www.computerweekly.com/news/366586014/Munich-Re-sees-strong-growth-in-AI-insurance">business is booming</a>. Meanwhile, other insurers remain rather less sanguine. <a href="https://assets.lloyds.com/media/439566f8-e042-4f98-83e5-b430d358f297/Lloyds_Futureset_GenAI_Transforming_the_cyber_landscape.pdf">Lloyd’s commissioned a report last March</a> that is noticeably less upbeat. At least they didn’t compare it to asbestos.</p>
<p>But AI insurance is coming, and it may well become the kingmaker of AI solutions. This will particularly affect the startup ecosystem, who often do not have the funds to pursue certification before pitching to clients. Alas, the cost of enterprise executives’ comfort that is bought by the availability of an insurance and indemnity policy is that market access will be rather less feasible for those who cannot obtain such insurance – who, paradoxically enough, are the ones who would need it most.</p>
<p>And since that leaves insurers with a ‘shadow governance’ function, I can well imagine the insurance industry creating <em>de facto</em> standards for AI governance faster than any standards body or government regulator could dream of. And let’s be honest – the actuaries will probably do a better job than most regulators could anyway.</p>
</section>
<section id="cross-estate-ai-xeai-is-the-future" class="level2">
<h2 class="anchored" data-anchor-id="cross-estate-ai-xeai-is-the-future">Cross Estate AI (XEAI) is the future</h2>
<p>Agentic AI was an interesting development, but to me, that’s primarily about the <em>how</em>. The really interesting story is of the <em>what</em> – that is, of what we can do with agents that we couldn’t do without them. And by far one of the most interesting such applications is what I call Cross Estate AI (XEAI). In XEAI, information crosses boundaries between enterprises as AI agents from different places are composed together to build an agentic structure. Think of API calling, but for AI agents – and with way more sophistication. A company’s agentic model may reach out to multiple other companies that offer their own agents, and collaborate with them. We are now relatively comfortable with notions like our AI agents calling an external API – after all, REST API calling has been around for a long, long time. But my company’s AI agent ‘calling up’ a specialised AI agent from another company to, say, engage in a discussion to refine the wording in a regulatory submission is something that enterprise stakeholders might need some time to get their head around.</p>
<p>The technical challenges here are fascinating – we need dynamic trust scoring, cryptographic proofs of model lineage and secure compute enclaves. But the real challenge is the social architecture: a very complex dance of trust, verification and governance that will have to be mediated. We’re essentially speed-running the development of diplomatic protocols that took human societies centuries to develop. And just like real diplomacy, it’s all about managing relationships between different systems with different organizational cultures and governance models.</p>
<p>GenAI does not yet have a <em>lingua franca</em> akin to REST to allow systems to talk to each other, least of all one that allows not just a query-response format to be conveyed but also various conventions on trust and governance to be exchanged. The real potential of agentic AI lies in how various agents can interact and together create emergent structures that are ultimately more than their parts. Function calling and an AI agent being able to look up something on Google are neat, but not really anything spectacular that a simple RPA actor or a 10-line Python script starting with <code>import requests</code> couldn’t do just as well. To paraphrase Kipling: the strength of the agent is the ecosystem. The big challenge, then, is to figure out how we can allow such ecosystems to exist across the boundaries of corporate networks.</p>
</section>
<section id="model-metabolomics-takes-over" class="level2">
<h2 class="anchored" data-anchor-id="model-metabolomics-takes-over">Model metabolomics takes over</h2>
<p>Energy efficiency is about to become the dominant factor in AI deployment economics. We’re not just talking about cost savings – we’re talking about a fundamental shift in how we think about AI system design. Just as biological systems optimize for metabolic efficiency rather than raw performance, we’re about to see AI architecture undergo a similar evolution.</p>
<p>This shift will upend the current obsession with inference speed and model size. Organizations will start optimising models for what I call “computational metabolomics” – the total energy cost of getting useful work done. We’ll see the emergence of new architectures that might be slower in raw terms but dramatically more efficient in their use of resources.</p>
<p>The really controversial part? Many organizations will opt for slower but more energy-efficient approaches, fundamentally challenging the industry’s obsession with real-time everything. I expect to see in 2025 more and more architectures that prioritize efficiency over speed, and a new set of metrics focused on energy consumption per useful output. The hype cycles of 2023-24 were all about who could build the biggest model - 2025 will be about who can build the most efficient one. Especially with the rise of SLMs, we’re slowly approaching the point where increasing the parameter size of generalist LLMs is going to yield any useful business benefits, or get developers any more free drinks. Such diminishing returns are, of course, part and parcel of every evolving system, and while it will be conceptualised as the beginning of a new AI winter by the usual suspects (it is not!), this is a good thing. For as we now have slowly reached a model size we’re comfortable with, we can start on making it more metabolically efficient. This is, of course, not limited to language models – in fact, this development has been going on in the computer vision world, where a good deal of processing occurs on edge devices, for a long time. Much of the lacking enthusiasm for wholesale replacement of computer vision models with vision-language models like LLaVA, CogVLM or DeepSeek boils down to the simple economics of the matter: the vast majority of computer vision challenges, in practice, can be solved quite well with a 30-year-old algorithm that can be implemented in fifty lines of C. The same, incidentally, is true for LLMs. I wince when I see GPT-based approaches to problems that a bag-of-words classifier can solve with comparable accuracy for essentially no cost and in a fraction of time. And once the power of cool wears off, I expect a renaissance of many of those solutions.</p>
<hr>
<p>As I complete this digital årsgång, what strikes me most is how 2025 looks to be the year when enterprise AI grows up. The trends all point toward practicality over pizzazz: smaller models over larger ones, efficiency over raw power, productized governance over philosophical frameworks, and insurance actuaries over innovation evangelists. Perhaps that’s not as exciting as the breathless predictions of AI singularities and digital transformations that dominated 2023, and maybe closer to the relatively sober atmosphere of 2024. But then again, maybe that’s exactly the point – real progress tends to be more about making things work than making headlines. And personally, I find that far more interesting.</p>
<hr>
<p><em>Note: These are my personal (and somewhat tongue-in-cheek) views, and may not reflect the views of any organisation, company or board I am associated with, in particular HCLTech or HCL America Inc.&nbsp;My day-to-day consulting practice is complex, tailored to client needs and informed by a range of viewpoints and contributors. <a href="https://chrisvoncsefalvay.com/disclaimer">Click here for a full disclaimer.</a></em></p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {Five Unconventional Predictions},
  date = {2024-12-31},
  url = {https://chrisvoncsefalvay.com/posts/five-wild-guesses/},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“Five Unconventional
Predictions.”</span> <a href="https://chrisvoncsefalvay.com/posts/five-wild-guesses/">https://chrisvoncsefalvay.com/posts/five-wild-guesses/</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>tech predictions</category>
  <guid>https://chrisvoncsefalvay.com/posts/five-wild-guesses/</guid>
  <pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/five-wild-guesses/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>LAIR - Language As Intermediate Representation</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/lair/</link>
  <description><![CDATA[ 




<p>The awesome thing about language is that, well, we all mostly speak it, to some extent or another. This gives us an immensely powerful tool to manipulate transformational tasks. For the purposes of this post, I consider a transformational task to be essentially anything that takes an input and is largel intended to return some version of the same thing. This is not a very precise definition, but it will have to do for now.</p>
<p>Such models are nothing new. Perhaps the most eye-catching and ubiquitous of such models are <a href="https://www.tensorflow.org/tutorials/generative/style_transfer">neural style transfer</a> models that take an image and return a version of the same image in a different style that let you turn a picture of your dog into a Van Gogh painting <span class="citation" data-cites="gatys2015neural">(Gatys, Ecker, and Bethge 2015)</span>. Quite simply put, these models are your typical generative model, with the difference that it takes two separate loss definitions: content loss, which is loss of the generated image vis-a-vis the content reference, and style loss, which is the loss vis-a-vis the style reference image. A “good” image then is one that minimises total loss, i.e.&nbsp;it’s just as close to your dog as it is to Van Gogh. Figure&nbsp;1 outlines this logic.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gatys2015neural" class="csl-entry">
Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. <span>‘A Neural Algorithm of Artistic Style’</span>. <em>arXiv Preprint arXiv:1508.06576</em>.
</div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nst" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-nst">flowchart TD
    C["Content image"]
    S["Style image"]

    G["Generated image"]

    G --&gt; L["Loss network"]
    L --&gt; G
    
    C -- "Content loss" --&gt; L
    S -- "Style loss" --&gt; L
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A rough outline of NST.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Given a content source image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bc%7D"> and a style reference image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D">, we define the total loss of our generated image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bg%7D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Btotal%7D%20=%20%5Calpha%20%5Cmathcal%7BL%7D_%7Bcontent%7D%20+%20%5Cbeta%20%5Cmathcal%7BL%7D_%7Bstyle%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are hyperparameters that control the relative importance of the content and style losses. The content loss for layer <img src="https://latex.codecogs.com/png.latex?l"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bcontent%7D(%5Cvec%7Bc%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi,j%7D%20(%5Cvec%7Bc%7D_%7Bij%7D%5El%20-%20%5Cvec%7Bg%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>which is basically a simple squared error loss between the feature vector of the content image and the generated image at layer <img src="https://latex.codecogs.com/png.latex?l">. The style loss is a bit more complicated, and is these days typically defined as the Maximum Mean Discrepancy, which <span class="citation" data-cites="li2017demystifying">Li et al. (2017)</span> have shown is essentially equivalent to the Gram matrix loss, defined as</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017demystifying" class="csl-entry">
Li, Yanghao, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. 2017. <span>‘Demystifying Neural Style Transfer’</span>. <a href="https://arxiv.org/abs/1701.01036">https://arxiv.org/abs/1701.01036</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathcal%7BL%7D_%7Bstyle%7D(%5Cvec%7Bs%7D,%20%5Cvec%7Bg%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B4N_l%5E2M_l%5E2%7D%20%5Csum_%7Bi,j%7D%20(%5Cmathbf%7BG%7D_%7Bij%7D%5El%20-%20%5Cmathbf%7BS%7D_%7Bij%7D%5El)%5E2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BG%7D_%7Bij%7D%5El"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BS%7D_%7Bij%7D%5El"> are the Gram matrices of the generated reference image and the style image at layer <img src="https://latex.codecogs.com/png.latex?l">, respectively. <img src="https://latex.codecogs.com/png.latex?N%5El"> is the number of feature maps in layer <img src="https://latex.codecogs.com/png.latex?l"> and <img src="https://latex.codecogs.com/png.latex?M_l"> is the dimensionality (height times width) of the feature map of layer <img src="https://latex.codecogs.com/png.latex?l">. There are two fundamental problems with this.</p>
<ol type="1">
<li>This works much less well for things that aren’t images or at least sufficiently similar to images.</li>
<li>The bigger problem is that the style reference is pretty much exhaustive. By that, I mean that there isn’t much we can convey to the model about the style that isn’t encapsulated in the style reference images. Depending on how semantically apt your model is, it may or may not be able to pick up some higher level ideas. It may be able to pick up the brush strokes of Van Gogh or the colours of a Turner, but it may not be able to paint your characters in the semantic context of Van Gogh’s time and place.</li>
</ol>
<p>This is where language comes in. Language is a very powerful tool for conveying information, and it turns out that if we use language as an intermediate representation, we can use language models to manipulate this intermediate representation to our heart’s content, using a relatively informal and rather forgiving language. This is the idea behind LAIR.</p>
<section id="a-toy-example" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-toy-example">A toy example</h2>
<p>I’m a firm believer in silly toy examples. The sillier, the better. So, we’ll start with the following proposition: can we create a model that will look at a photo from, say, the front page of our favourite newspaper, and transpose it into the Warhammer 40k universe? In case you’re unfamiliar, Warhammer 40k is set – as the name suggests – in the 40th millennium, but is a weird mixture of medieval and futuristic technology. The whole atmosphere is taking the ‘Dark Ages’ part of the Middle Ages,<sup>1</sup> adding spaceships and laser guns, and turning the whole thing into an absolutely depressing dystopia. It’s a lot of fun.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Which may or may not ever have actually existed.</p></div></div><p>There are a few things we want here: I don’t merely want the <em>visual</em> style of the Warhammer 40k universe, I also want the <em>semantics</em> – that is, I want characters to be transposed into the Warhammer 40k universe. I want the model to understand that the people in the photo are now Space Marines, and that the buildings are now Gothic cathedrals. I can’t get neural transfer to that for me, because it does not understand, or care, about semantics, and does not do semantic transformation. More importantly, I cannot interact with the ‘guts’ of neural style transfer beyond setting the hyperparameters and the source images.</p>
<p>What I can, however, do is to use the language I am mostly most proficient in – that is, human language – to manipulate an intermediate representation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lair" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-lair">flowchart LR
    subgraph Description
        direction TB
        S["Source image"] --&gt; D("Descriptor\ne.g. GPT-4 vision") --&gt; d["Description"]
    end

    subgraph Transformation
        direction TB
        t("Transformer\ne.g. GPT-4") --&gt; td["Transformed\ndescription"]
    end

    subgraph Rendering
        direction TB
        r("Renderer\ne.g. DALL-E") --&gt; I["Output\nimage"]
    end

    Description --&gt; Transformation --&gt; Rendering
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A rough outline of LAIR as applied to the toy example.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure&nbsp;2 outlines the basic idea for images. We use a descriptor model to describe the image, and then use a text-to-text transformer to transform the description. The transformed description is then rendered into an image. The descriptor model is typically a vision model, and the transformer is typically a language model. The renderer would then be a generative model.</p>
<p>Consider the following source image I downloaded from Pexels:</p>
<div id="fig-source" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/pexels-canva-studio-3153198.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Our source image.
</figcaption>
</figure>
</div>
<p>Here’s the intermediate representation (IR) description created by GPT-4 vision:</p>
<blockquote class="blockquote">
<p>This is an image of four individuals gathered around a laptop, suggesting a collaborative work environment. The setting has an ample amount of natural light, evident by the bright illumination on the subjects and the interior. They appear to be located in a modern indoor environment with a large curved window in the background, contributing to the light-flooded ambiance.</p>
<p>Starting from the left, the first individual is a man with short, dark hair and a light beard, wearing a black t-shirt with a white graphic and text. He appears focused, resting his hands on the laptop while looking at the screen. To his right is a woman with shoulder-length curly hair who seems engaged and smiling, leaning slightly towards the laptop. She’s wearing a light grey short-sleeved top.</p>
<p>Behind these two and slightly out of focus, two more individuals are present. The person in the middle of the composition is a man with shoulder-length wavy hair, wearing a black shirt, gazing at the laptop screen. To the far right, partially cropped out of the frame, there is a woman with glasses, wearing a dark business casual blazer. She seems to be observing the laptop’s screen with interest, standing slightly behind the seated individuals.</p>
<p>The group exhibits a sense of teamwork and focus, with everyone’s attention directed toward the laptop, situated on what appears to be a white table or desk. The body language and expressions suggest a positive, collaborative atmosphere, typical of a professional or academic team setting.</p>
</blockquote>
<p>This is a pretty decent description. Let’s grimdark it up by passing it to our transformer model, which was initialised with the following prompt:</p>
<blockquote class="blockquote">
<p>Your job is to analyze descriptions of images and change them to transpose the image into the dark, futuristic aesthetic of the Warhammer 40k universe. You are given a description of a scene, and your job is to change that scene according to the instructions, and give a detailed description of the changed scene that would allow a generative model to create an image.</p>
<ol type="1">
<li>Replace clothing items with a mixture of futuristic and mediaeval looking armour.</li>
<li>Replace every-day objects held by characters with power swords, chainswords,futuristic looking guns, staffs or magic items.</li>
<li>Replace architecture with the monumental, dark architecture common to the Warhammer 40k universe.</li>
<li>Include things like monumental gothic spaceships in the image description.</li>
</ol>
</blockquote>
<p>Here’s the output:</p>
<blockquote class="blockquote">
<p>This is an image of four battle-hardened individuals gathered around a hololith, suggesting a strategic war council. The setting is bathed in the cold, harsh light of flickering lumens, casting long, grim shadows on the figures and the interior. They appear to be situated in a cathedral-like command room of an Imperial Battleship, with a monumental, gothic archway in the background, contributing to the oppressive ambiance.</p>
<p>Starting from the left, the first individual is an Adeptus Astartes, his short, dark hair and light beard visible beneath the rim of his power armor helmet. He is clad in a black power armor with white Imperial insignia. He appears focused, his gauntleted hands resting on the hololith while studying the battlefield projection. To his right, a Sororitas, her shoulder-length, curly hair peeking out from beneath her wimple, leans towards the holographic display. She’s clad in a light grey power armor, her face engaging and determined.</p>
<p>Behind these two and slightly out of focus, two more figures loom. The figure in the middle of the composition is a man, his shoulder-length wavy hair barely visible beneath his Inquisitorial hat, wearing a black, ornate, medieval-looking armor, his eyes fixed on the hololith. To the far right, partially cropped out of the frame, there is a Tech-Priestess, her bionic glasses glinting in the lumen-light, her dark, mechanicus robes adding to her awe-inspiring presence. She seems to be observing the hololith with keen interest, standing slightly behind the armored individuals.</p>
<p>The group gives off a sense of unity and focus, with everyone’s attention directed toward the hololith, situated on what appears to be an aged, white marble table. The body language and expressions suggest a tense, collaborative atmosphere, typical of a war council in the grim darkness of the far future.</p>
</blockquote>
<p>By the Emperor, that’s some good stuff. Let’s see what DALL-E makes of it:</p>
<div id="fig-output" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lair/img-eTqdxb5CYGT2vY5FqNkRJdva.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Our output image.
</figcaption>
</figure>
</div>
<p>It’s interesting to note what remains and what has changed. In particular, the relationship of the figures to each other, both spatially and semantically, as well as their posture, their number (mostly!) and the general subject matter at a highly semanticised level was preserved. Instead of a laptop, they’re looking at a big holographic sand table. Note that unlike in NST, we actually have control over what is preserved and what is not – that’s what our transformer prompt accomplishes. We’ve told it to change dress, for instance, but we haven’t told it to make any changes to the overall relationship between the figures (note how a “collaborative atmosphere”, for instance, was retained word for word).</p>
</section>
<section id="whats-the-point" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-the-point">What’s the point?</h2>
<p>Okay, this was quite fun, but what’s the point? Well, the point is that we can use language as an intermediate representation to manipulate things relatively easily. This is powerful because language is such an accessible and forgiving intermediate representation. More importantly, however, <em>anything</em> that can be represented in language can be manipulated this way.</p>
<ul>
<li>In the space of language-to-language, this includes adapting text to the needs of special audiences,<span class="citation" data-cites="steinmetz2023developing verma2023bridging">(Steinmetz 2023; Verma, Boland, and Miesenberger 2023)</span> conveying information to lay audiences <span class="citation" data-cites="lee2023ai">(Lee, Goldberg, and Kohane 2023)</span> and creating secondary explanatory materials e.g.&nbsp;statutory explanations <span class="citation" data-cites="blair2023can">(Blair-Stanek, Holzenberger, and Van Durme 2023)</span>.</li>
<li>For image-to-image transformations, retaining semanticity while simpifying visuals and removing clutter may often be useful, e.g.&nbsp;for creating procedural visual guidance for medical procedures <span class="citation" data-cites="chen2023endoscopic">(Chen 2023)</span>. Often, such images are created by hand, but this is a time-consuming process that could be automated.</li>
<li>For code-to-code, language as an intermediate representation allows the interjection of desired features into code, e.g.&nbsp;for the purposes of code refactoring. Beyond simple code rewriting, this allows a kind of opinionated transformation. Often, a target language is not only idiomatically different but also has certain other characteristics, and this is <em>a fortiori</em> the case for DSLs <span class="citation" data-cites="magalhaes2023c2taco">(Magalhães et al. 2023)</span>.</li>
<li>For code-to-text, this allows the creation of documentation from code, which is a perennial problem in software engineering. The textual intermediate representation allows fine control over the resulting documentation.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-steinmetz2023developing" class="csl-entry">
Steinmetz, Ina. 2023. <span>‘Developing <span>“EasyTalk”</span>–a Writing System Utilizing Natural Language Processing for Interactive Generation of <span>“Leichte Sprache”</span>(easy-to-Read German) to Assist Low-Literate Users with Intellectual or Developmental Disabilities and/or Complex Communication Needs in Writing’</span>. <a href="https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf">https://kola.opus.hbz-nrw.de/frontdoor/deliver/index/docId/2443/file/diss.pdf</a>.
</div><div id="ref-verma2023bridging" class="csl-entry">
Verma, A Kumar, S Gavra Boland, and Klaus Miesenberger. 2023. <span>‘Bridging the Digital Divide for Persons with Intellectual Disabilities: Assessing the Role of ChatGPT in Enabling Access, Evaluation, Integration, Management, and Creation of Digital Content’</span>. In <em>ICERI2023 Proceedings</em>, 3767–76. IATED.
</div><div id="ref-lee2023ai" class="csl-entry">
Lee, Peter, Carey Goldberg, and Isaac Kohane. 2023. <em>The AI Revolution in Medicine: GPT-4 and Beyond</em>. Pearson.
</div><div id="ref-blair2023can" class="csl-entry">
Blair-Stanek, Andrew, Nils Holzenberger, and Benjamin Van Durme. 2023. <span>‘Can GPT-3 Perform Statutory Reasoning?’</span> <em>arXiv Preprint arXiv:2302.06100</em>.
</div><div id="ref-chen2023endoscopic" class="csl-entry">
Chen, Hao-Wen. 2023. <span>‘Endoscopic Endonasal Skull Base Surgery for Pituitary Lesions: An AI-Assisted Creative Workflow to Develop an Animated Educational Resource for Patients and Physicians’</span>. PhD thesis, Johns Hopkins University.
</div><div id="ref-magalhaes2023c2taco" class="csl-entry">
Magalhães, José Wesley de Souza, Jackson Woodruff, Elizabeth Polgreen, and Michael FP O’Boyle. 2023. <span>‘C2TACO: Lifting Tensor Code to TACO’</span>. In <em>Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences</em>, 42–56.
</div></div><p>The possibilities for using language as an intermediate representation are endless. LAIR is a powerful paradigm for transformational tasks that allows us to use language as an intermediate representation to manipulate things in a way that is both accessible and powerful, and that allows us to pick and choose <em>what</em> part of semanticity we want to manipulate <em>versus</em> what we want to preserve. It’s hard to reason about LAIR’s relative performance given that it is not a technique but a paradigm, and that its focus is not simple style transfer but finely controlled stylistic and contextual transformation, but even in the current absence of benchmarks, it is clear that models benefit from using language as an easily workable and malleable intermediate representation.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>The code for the toy example is available <a href="https://gist.github.com/chrisvoncsefalvay/f3c6e7547350cfae50be0078ad99435f">here</a>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2024,
  author = {{Chris von Csefalvay}},
  title = {LAIR - {Language} {As} {Intermediate} {Representation}},
  date = {2024-01-06},
  url = {https://chrisvoncsefalvay.com/posts/lair/},
  doi = {10.59350/qg7b3-crs97},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2024" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2024. <span>“LAIR - Language As Intermediate
Representation.”</span> <a href="https://doi.org/10.59350/qg7b3-crs97">https://doi.org/10.59350/qg7b3-crs97</a>.
</div></div></section></div> ]]></description>
  <category>LLMs</category>
  <category>AI</category>
  <category>style transfer</category>
  <guid>https://chrisvoncsefalvay.com/posts/lair/</guid>
  <pubDate>Sat, 06 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/lair/header.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>The 95% myth</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/95-percent-myth/</link>
  <description><![CDATA[ 




<p>The year is 1959. Eisenhower is on his second term, Castro just kicked Batista out of the country and Ray Charles’s Let the Good Times Roll is topping the charts. And, in slightly nerdier pursuits, a psychiatrist named Albert “Mickey” Stunkard and a dietician colleague of his, Mavis McLaren-Hume, published a paper in the <em>AMA Archives of Internal Medicine</em> titled <em>The Results of Treatment for Obesity</em> <span class="citation" data-cites="smh">(Stunkard 1959)</span>. Written in a somewhat endearingly old school way by authors not quite initiated to the sacred mysteries of, y’know, <img src="https://latex.codecogs.com/png.latex?p">-values and proper statistical methodology, it reports on “the literature” on dietary and pharmacologically assisted treatments (which is an extremely generous term for 8 studies, none of which are randomised, one of which is entirely retrospective, and the total <img src="https://latex.codecogs.com/png.latex?N"> is 1,368), and on a sample of 100 patients (!) from the Nutrition Clinic at New York Hospital (the “NY sample”). And from that, they draw wide-ranging conclusions on the ‘harm’ and ‘ineffectiveness’ of ‘weight reduction’.</p>
<div class="no-row-height column-margin column-container"><div id="ref-smh" class="csl-entry">
Stunkard, A. J. 1959. <span>‘The Results of Treatment for Obesity’</span>. <em>A.M.A. Archives of Internal Medicine</em> 103: 79. <a href="https://doi.org/10.1001/archinte.1959.00270010085011">https://doi.org/10.1001/archinte.1959.00270010085011</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Fat acceptance activists love to point out the size of the US ‘diet industry’, which is somewhere around US$ 70bn. For comparison, the US QSR (quick service restaurants – read: fast food) industry had a 2022 market size of US$ 380bn.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;If you are wondering by the end of this whether Stunkard got his MD by sending in some coupons from cereal boxes, you’re not alone, but that would be quite unfair. From what I’ve read of the man, he was both a great human being and a very accomplished clinician. It’s a pity that his most influential piece was not only uncharacteristically bad work for him, but was also grievously misinterpreted.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Don’t worry, it gets worse. The <img src="https://latex.codecogs.com/png.latex?N"> for maintenance is 12. That’s not a typo.</p></div></div><p>In this post, I’ll dissect the Stunkard-McLaren-Hume (SMH) paper, and how it became an uncritically repeated figure and later the guiding mantra of ‘fat acceptance’, a movement I consider to be organised science denialism fuelled by self-delusion and corporate interests.<sup>1</sup> I try to do so with some generosity and understanding for the vagaries of the age, but one shouldn’t be under any illusions that the quality of the SMH paper would pass muster as an undergraduate research paper today. Standards have evolved, but the SMH paper is, well, not to mince words, bad even for the time.<sup>2</sup> It is conclusory to the extreme over a pathetically small sample, and I’m mostly convinced that the reason the paper is never cited is because anyone making major life decisions about their health and well-being based on a 65-year-old study of 100 people would look, well, quite silly.<sup>3</sup></p>
<section id="sm-damn-h" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sm-damn-h">SM-damn-H</h2>
<p>The SMH paper consists of two parts. One is a rudimentary ‘mini-meta’ of 8 studies from 1931 to 1958, of a little over 1,300 patients. The other is a retrospective study of 100 patients from the Nutrition Clinic at New York Hospital. Neither is great, but the mini-meta is so bad, I won’t belabour it in detail. It looks at very few patients overall on a range of ‘diets’, ranging from a 600kcal VLCD to diets assisted by dexedrine (classy!) and thyroid supplementation (unmonitored, of course), and reports the percentage of patients by lbs of weight lost. This is confusing to the extreme, because of course the absolute weight loss on a diet is highly contextual. A 200lbs person losing 50lbs is an entirely different story from a morbidly obese individual, say 350lbs, losing back to 300lbs. The mini-meta is, in short, a mess, and I’m not going to waste time on it.</p>
<p>The second part, the retrospective study, is also very bad, but at least informative. Here, 100 patients were interviewed at admission to the Nutrition Clinic, and prescribed a diet. What diet, you might ask? “Balanced weight-reduction diets from 800 to 1,500 Cal” (sic). That’s right, the diets were not standardised, nor do we see the estimated BMR<sup>4</sup> or the shortfall vis-a-vis the BMR reported anywhere. No mention of exercise or control for activity is in evidence anywhere. The clinic itself gives a bit of a flying by the seat of one’s pants impression, and the study doesn’t get better from here on.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Basal Metabolic Rate.</p></div></div><p>The retrospective study was then performed 2.5 years after initial admission, as a chart review. Of the 100 patients, only 12 were considered to have successfully lost weight, with 20lbs lost being the cut-off point (once again, entirely insensitive to starting body weight, thus likely privileging the more obese initial participants). A flowchart of this is laid out in Figure&nbsp;1.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-flowchart">flowchart TD
    I["Initial intake:&lt;br&gt;100"] --&gt; J["Follow-up&lt;br&gt;61 (61%)"]
    I --&gt; L["Lost to follow-up&lt;br&gt;39 (39%)"]
    J --&gt; S["Successful (more than 20lbs lost)&lt;br&gt;12 (19.7%)"]
    J --&gt; U["Unsuccessful (20lbs or less lost)&lt;br&gt;49 (80.3%)"]
    S1["Maintained at 1y&lt;br&gt;6 (50%)"]
    S2["Maintained at 2y&lt;br&gt;2 (16.6%)"]
    LFU1["Lost to follow-up&lt;br&gt;1 (8.3%)"]
    F1["Failed at 1y&lt;br&gt;5 (41.7%)"]
    F2["Failed at 2y&lt;br&gt;4 (33.3%)"]
    S --&gt; S1
    S --&gt; F1
    S --&gt; LFU1
    S1 --&gt; S2
    S1 --&gt; F2
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Patient flow of the SMH study. Numerators of percentages are noted in bold. Based on Table 2 of the SMH paper, except that I have broken out the maintenance group into two subgroups.
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are a few things worth noting here. One is that this is a study of an <img src="https://latex.codecogs.com/png.latex?N"> barely in the three figures, and has a 39% loss to follow-up. One would likely not want to publish that. That’s just quite simply not publishable data. Worse, however, is the confusion of numerators and denominators.</p>
<p>If we consider maintenance failure to be failure of the diet after 1 or 2 years of finishing treatment, then it is true that only 6 out of 100 initial patients (6%) maintained their weight loss for 1 year, and only 2 (2%) for 2 years. Except that’s altogether the wrong way of calculating these figures, especially in the face of losing <em>over a third</em> of the initial cohort to follow-up. For what it’s worth, each of those 39 lost to follow-up patients could have maintained for two years and simply didn’t bother to go back to the clinic. We don’t know. We can’t know. We can’t even make a reasonable guess. If your figures permit a conclusion that success might have been anything between 2% and 41%, you don’t have a study. You have a mess.</p>
<p>One highly suggestive feature here (which, to their credit, SMH point out) is that 28 of the 39 lost to follow-up (71.8%) never attended any <em>other</em> clinic at the hospital. SMH note that</p>
<blockquote class="blockquote">
<p>[s]ince admission to the Nutrition Clinic occurs entirely by referral from other clinics, this represents the rupture of at least two therapeutic relationships.</p>
</blockquote>
<p>Is that an inescapable conclusion? Or could one conceive that at least a good part of the patients lost to follow-up either met their goals and thus never reported back, or at the very least, their weight loss was actually enough of a <em>resounding</em> success that it alleviated the primary issue for which they presented prior to referral to the Nutrition Clinic? Once again, we don’t know. We can’t know. We can’t even make a reasonable guess.<sup>5</sup> What we can say is that concluding from this data that long-term weight loss is 6% or 2% effective is an incredibly, unreasonably strong interpretation of very, very weak data indeed.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Or can we? The QUOVADIS study had this issue: following up with the dropouts (7%) indicated many were just really satisfied with the results.</p></div></div></section>
<section id="pulling-the-thread-misinterpreting-smh" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="pulling-the-thread-misinterpreting-smh">Pulling the thread: misinterpreting SMH</h2>
<p>Of course, scientific writing can easily become like a runaway horse. Once one puts their thoughts out in the world, it’s open for the audience to misinterpret it and draw every single wrong conclusion from it. As grievously bad as I think SMH is, it is nowhere near the juggernaut of bad science that uncritical repetition has turned it into over the years.</p>
<p>Consider a randomly picked example <span class="citation" data-cites="Carmichael1999">(Carmichael 1999)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="ref-Carmichael1999" class="csl-entry">
Carmichael, A. R. 1999. <span>‘Treatment for Morbid Obesity’</span>. <em>Postgraduate Medical Journal</em> 75: 7–12. <a href="https://doi.org/10.1136/pgmj.75.879.7">https://doi.org/10.1136/pgmj.75.879.7</a>.
</div></div><blockquote class="blockquote">
<p>Results on the role of diet in the treatment of obesity reported more than 30 years ago do not vary from those reported more recently, because as many as 95% of dieters tend to regain their lost weight over a relatively short period.</p>
</blockquote>
<p>SMH is, of course, the authority for that assertion. Yet that misunderstands, quite fundamentally, what SMH actually does say, which is that a good percentage of diets fail, not that a good percentage of dieters do. This point is far from being so subtle as to justify someone actually publishing this misunderstanding past a peer reviewer.</p>
<p>It is, for instance, widely acknowledged that one by one, antidepressants are effective only a relatively small percentage of the time (something between 15-30%). Yet antidepressant therapy by and large is vastly more successful. That is because just like diets, antidepressants are not a monolithic treatment, but a class of treatments: if one fails, one is free to try another, or a combination of others. Like obesity and human metabolism, depression is a multifaceted disorder with many possible aetiological processes, and it might take a few tries to find the right diet. Not that someone who has already concluded that ‘diets don’t work’ and convinced themselves that a study of all of 100 patients on all of one type of diet would prove that fact would come to that conclusion, of course. Sadly, that’s the cost of sloppy science of the kind the SMH paper perpetrates: you blind yourself to fundamental truths in a mad pursuit of confirmatory evidence.</p>
<p>The second pernicious misinterpretation is that weight regain indicates a diet ‘failure’. Once again, I’m puzzled by how Stunkard and McLaren-Hale missed this point, for their study did not control for maintenance:</p>
<blockquote class="blockquote">
<p>Our results are summarized in Table 2. In this Table any person who maintained a weight loss of 20 lb. or more is classified as a “success”; any person whose weight was within 19 lb. of the starting weight is a “failure.</p>
</blockquote>
<p>What is of course entirely omitted is what these patients did over the 1- and 2-year follow-up period. We know from studies on VLE/VLCDs<sup>6</sup> that adherence is crucial <span class="citation" data-cites="10.7243/2052-5966-1-4">(Wright et al. 2013)</span>. So, if the assertion is that a diet does not immunise to weight regain once it is abandoned, the SMH paper’s finding is trivial to the point of banality. Proper sciencin’ would require us to control for calorie intake in that maintenance period. No diet will magically mean that reverting to an intake above expenditure will not result in significant weight gain once again. This is reflected in a modern understanding of diets, which considers the best diets to be those that can be sustained indefinitely (with the exception of some acute ‘crash’ diets used to rapidly reduce weight before surgical or other interventions). In short, the SMH paper’s alleged finding of long-term ‘diet failure’ is either trivially true (yes, a diet, once abandoned, will not confer continuing benefits) or at the very best unproven. While the entire process was largely unmonitored (it’s admittedly difficult to monitor dietary intake in an outpatient setting, especially in the pre-Nutrition Facts Labeling era<sup>7</sup>), the lack of even the slightest semblance of monitoring or tracking adherence, even self-reported, is fatal to the study as evidence for maintenance of weight loss. Whatever its merits (and there aren’t many), SMH is epistemically incapable of supporting the argument for which it is most famously and extensively used.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Very Low Energy/Very Low Calorie Diets.</p></div><div id="ref-10.7243/2052-5966-1-4" class="csl-entry">
Wright, T. G., B. Dawson, G. Jalleh, and K. J. Guelfi. 2013. <span>‘Program Compliance, Weight Loss and Health Profile Changes in Who Obesity Classes After Very Low Energy Dietary Intervention’</span>. <em>Global Epidemic Obesity</em> 1: 4. <a href="https://doi.org/10.7243/2052-5966-1-4">https://doi.org/10.7243/2052-5966-1-4</a>.
</div><div id="fn7"><p><sup>7</sup>&nbsp;Nutrition Facts labels were implemented by the Nutrition Labeling and Education Act 1990, which entered into force in 1994.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;But that’s a misinterpretation of the science for another day.</p></div></div><p>Together, these two misinterpretations gave rise to a dangerous myth that all but ‘proves’ the inevitability of one’s own weight, which in turn morphed into the ‘set point theory’.<sup>8</sup> I’d like to acquit Stunkard and McLaren-Hale from responsibility for these, for fairness demands I do so, but I cannot wholeheartedly feel that they haven’t, through sloppy science and bad writing, opened Pandora’s box. On their head, then, must some of the resulting blame land.</p>
</section>
<section id="why-this-matters" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-this-matters">Why this matters</h2>
<p>It wouldn’t behoove to fire cheap shots at a paper old enough to collect retirement benefits in most civilised countries if it weren’t for the fact that its sloppy distillation into the 95% myth had become a uniquely harmful mantra. There are very few interventions that justify even relatively small risks in return for a 5% effectiveness, so if that figure is accurate, even the modest risks of adequate, well-executed, medically supervised diets<sup>9</sup> might appear excessive. But, of course, that figure is almost definitely wrong.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Which are generally transitory. It’s probably worth pointing out at this point again that the diets examined in the SMH paper included dexedrine, thyroid supplementation without much proper monitoring and 900kcal low calorie diets. It’s perhaps not unreasonable to assume that these aren’t on the safer side. Their risks do not compare to those of a properly monitored modern diet aimed at a few hundred kcals of deficit.</p></div><div id="ref-thomas2014weight" class="csl-entry">
Thomas, J Graham, Dale S Bond, Suzanne Phelan, James O Hill, and Rena R Wing. 2014. <span>‘Weight-Loss Maintenance for 10 Years in the National Weight Control Registry’</span>. <em>American Journal of Preventive Medicine</em> 46 (1): 17–23.
</div><div id="ref-kraschnewski2010long" class="csl-entry">
Kraschnewski, JL, Jarol Boan, Joseph Esposito, Nancy E Sherwood, Erik B Lehman, Donna K Kephart, and Christopher N Sciamanna. 2010. <span>‘Long-Term Weight Loss Maintenance in the United States’</span>. <em>International Journal of Obesity</em> 34 (11): 1644–54.
</div><div id="ref-perri2008extended" class="csl-entry">
Perri, Michael G, Marian C Limacher, Patricia E Durning, David M Janicke, Lesley D Lutes, Linda B Bobroff, Martha Sue Dale, Michael J Daniels, Tiffany A Radcliff, and A Daniel Martin. 2008. <span>‘Extended-Care Programs for Weight Management in Rural Communities: The Treatment of Obesity in Underserved Rural Settings (TOURS) Randomized Trial’</span>. <em>Archives of Internal Medicine</em> 168 (21): 2347–54.
</div><div id="ref-10.1016/j.amjmed.2012.10.010" class="csl-entry">
Unick, J., D. Beavers, D. Bond, J. Clark, J. Jakicic, A. Kitabchi, W. Knowler, T. Wadden, L. Wagenknecht, and R. Wing. 2013. <span>‘The Long-Term Effectiveness of a Lifestyle Intervention in Severely Obese Individuals’</span>. <em>The American Journal of Medicine</em> 126: 236–242.e2. <a href="https://doi.org/10.1016/j.amjmed.2012.10.010">https://doi.org/10.1016/j.amjmed.2012.10.010</a>.
</div><div id="ref-diabetes200910" class="csl-entry">
Group, Diabetes Prevention Program Research et al. 2009. <span>‘10-Year Follow-up of Diabetes Incidence and Weight Loss in the Diabetes Prevention Program Outcomes Study’</span>. <em>The Lancet</em> 374 (9702): 1677–86.
</div></div><p>Thus, we know that especially when used as part of a comprehensive treatment plan, diets are effective. <span class="citation" data-cites="thomas2014weight">Thomas et al. (2014)</span>, reporting on the National Weight Control Registry study (<img src="https://latex.codecogs.com/png.latex?N%20="> 2,886), found that 88.4% of participants maintained a weight loss of at least 10% of their initial body weight for at least 5 years and 86.6% still maintained a 10% weight loss at 10 years. The retrospective study based on NHANES by <span class="citation" data-cites="kraschnewski2010long">Kraschnewski et al. (2010)</span> is even more encouraging: not only did they find that more than one in six overweight adults have lost and maintained a 10% weight loss for over a year, they also found that over a third (36.6%) of those who lost at least 5% of their body weight were able to maintain this – note that this is entirely retrospective, i.e.&nbsp;no specific intervention was administered to these individuals. A smaller study by <span class="citation" data-cites="perri2008extended">Perri et al. (2008)</span> found that following a 10.0kg mean weight loss over a 6-month treatment period, weight regain ranged from 1.2 to 3.7 kg (extended-care <em>vs.</em> education control), resulting in a sustained weight loss of 6.3 to 8.8 kg – nothing to sneer at, especially considering that this was specifically in an underserved rural setting. It turns out that when we look at studies that have been performed in the last 20 years, we find that the 95% figure is not only wrong, but the exact opposite of the truth. Sustained weight loss is the rule, not the exception, especially in the context of a comprehensive treatment plan, and few studies bear out the idea that those that lose weight will regain <em>more</em> than what was lost. Even where weight is regained, there are often lasting benefits. <span class="citation" data-cites="10.1016/j.amjmed.2012.10.010">Unick et al. (2013)</span> examined 5,145 individuals with Type 2 diabetes and a 4-year follow-up, and found that not only did they still have on average a 4% or so weight loss after 4 years but also significant improvements in their HbA1c, blood pressure and lipid profile. We also know, from <span class="citation" data-cites="diabetes200910">Group et al. (2009)</span>, that weight loss itself may prove protective against Type 2 diabetes even if some of the cohort regains the lost weight.</p>
<p>So, we know the 95% figure is wrong. Why, then, does it persist? I think there are two reasons for this. One is that it’s a convenient excuse for people who don’t want to put in the work. The other is that it’s a convenient excuse for people who want to sell you something. The former is a matter of personal responsibility, and I’m not going to tell anyone how to live their lives. The latter, however, is a matter of public health, which is being actively harmed by a motley assortment of ignorami, grifters and – worst of all – the occasional medical professional who has not done their research and/or slept through their biostats classes who all repeat uncritically the 95% myth (while politely eliding any mention of its dubious parentage). Social media has responded to harmful medical misinformation during the COVID-19 pandemic with the zeal of the Spanish Inquisition on steroids. It is, then, rather incongruent that the same platforms are happy to let the 95% myth run rampant, despite the fact that it is demonstrably false and demonstrably harmful.<sup>10</sup></p>


<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;I’m not generally a fan of censorship, but I like public health and I like consistency. I’d like every video repeating this myth to have a massive big red sticker on it saying “this video contains medical misinformation”.</p></div></div>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The 95\% Myth},
  date = {2023-12-27},
  url = {https://chrisvoncsefalvay.com/posts/95-percent-myth/},
  doi = {10.59350/xmf8m-t1d22},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The 95% Myth.”</span> <a href="https://doi.org/10.59350/xmf8m-t1d22">https://doi.org/10.59350/xmf8m-t1d22</a>.
</div></div></section></div> ]]></description>
  <category>fitness</category>
  <category>science</category>
  <category>bad science</category>
  <guid>https://chrisvoncsefalvay.com/posts/95-percent-myth/</guid>
  <pubDate>Wed, 27 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/95-percent-myth/93e3df55-79ec-44b0-87c1-03bbca32ee46.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>What I learned from getting bodied by a robot.</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</link>
  <description><![CDATA[ 




<p>Say you’re busing tables and you’re trying to pass someone in a wheelchair. What do you do? Do you say “excuse me” and wait for them to move? Do you say “excuse me” and then try to pass them? Do you just try to pass them? Do you say nothing and just try to pass them? All of these are, actually, pretty legitimate answers.</p>
<p>Now, say you’re a robot. What do you do? The robots that are currently deployed at the United Airlines lounge at SFO (one of <a href="https://www.reddit.com/r/unitedairlines/comments/10pdr0u/the_robots_arrived_today_at_the_new_ord_c_club/">these</a>, courtesy of <a href="https://www.bearrobotics.ai">Bear Robotics</a>) thought beeping, then bodying me with full military power was the right answer. I’m not sure I agree.</p>
<p>(I’m fine. The robot’s fine. This post isn’t about that. I play a sport that involves crazy strong people in wheelchairs that look straight out of Mad Max ram into each other at full speed to let their violence out. It takes a bit to dislodge me.)<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;A few people have asked me for details on the incident. It really wasn’t even big enough a deal to report to the lounge stewards – there was no injury to me, no damage to the robot and overall no harm done. The robot kept bumping into my chair and pushing against me as I was trying to get out of the way, which of course made everything harder. I was a little concerned that its frantic efforts might result in the dishes it was carrying getting dislodged and falling on me, but thankfully that didn’t happen. It did, however, shine a light on the lack of modern robotics’ lack of understanding the needs of customers with disabilities, and I am more than a little concerned by that – not everyone in a wheelchair is a 6’2 180lbs adaptive athlete. We can do better than this. We <em>have</em> to do better than this.</p></div></div><p>The point is about this <em>modus vivendi</em> between humans and artificial intelligences that we really haven’t worked out adequately.</p>
<section id="mental-models" class="level2">
<h2 class="anchored" data-anchor-id="mental-models">Mental models</h2>
<p>Humans aren’t mind-readers… but they really, really want to be. We’re constantly trying to figure out what other people are thinking. Our survival as a species has depended on it. The cost of this is worrying what people think about us every time we enter a room. But the benefit, oh, the benefit of it: we can create models of other people’s minds, and we can use those models to predict what they’re going to do next. This is a very useful skill to have when you’re, say, hunting a woolly mammoth. As you do.</p>
<p>This means that in trying to determine what to do next, we don’t just reason by some goal-directed reinforcement learning framework sitting on top of some observations of reality. We create a model not just of reality but of other minds, too.</p>
<p>Part of that is to understand what other minds do: their capabilities, but also their limitations.</p>
</section>
<section id="your-fear-in-a-handful-of-dust" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="your-fear-in-a-handful-of-dust">Your fear in a handful of dust</h2>
<p>Consider, for instance, fear. A robot has about as little need, <em>ex facie</em>, to understand that humans are afraid as it has for empathy to understand that some people use a wheelchair and need some time to get out of the way. A machine isn’t mortal in the conventional sense. It has had no need to develop the complex neurological-psychological responses that, in excess, give us, say, a fear of heights (because <em>some</em> fear of heights is definitely evolutionarily useful!).</p>
<p>Fear is not only a useful emotion to have, it’s also something humans <em>have</em>, like it or not. The consequence is that anyone and anything that seeks to interact with humans has to understand that fact. If you don’t, you’re collectively going to have a bad time.</p>
<p>And so, if a human is working on, say, a roof, they will reason from the place the poet called “the unstill tremors of the fearful heart”.<sup>2</sup> A machine working on its own on a roof can ignore fear as much as it desires. A machine that seeks to interact with humans and live in human society, however, cannot. And there’s the rub. It’s easy to create a machine that does surgery. It’s near impossible to create one to <em>assist</em> in surgery. Interacting with humans is a tough call, and it’s not just because we’re a bunch of weirdos (though that definitely contributes).</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Dyneley Hussey, who deserves to be known way more than he is.</p></div></div></section>
<section id="the-problem-of-other-minds" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-of-other-minds">The problem of other minds</h2>
<p>A robot, then, doesn’t have to understand that it has, or rather <em>is</em>, a mind of a sort. But it absolutely has to understand that <em>other humans</em> have minds of their own, and that those minds do and think stuff.</p>
<p>The problem of developing a theory of mind is one of those watersheds of artificial intelligence that will have a clear before and after. There isn’t much room for gradualism here. A machine that can understand that other humans have minds of their own is going to be one that will be able to interact with other humans and live in some level of comity, and one that doesn’t, won’t. This is the next big thing in AI, and it’s going to be a big thing indeed.</p>
<p>Or maybe I’m wrong. Who knows. I did just get bodied by a robot, after all.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {What {I} Learned from Getting Bodied by a Robot.},
  date = {2023-12-12},
  url = {https://chrisvoncsefalvay.com/posts/ai-human-interaction/},
  doi = {10.59350/r8k9q-zdm06},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“What I Learned from Getting Bodied by
a Robot.”</span> <a href="https://doi.org/10.59350/r8k9q-zdm06">https://doi.org/10.59350/r8k9q-zdm06</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>disability</category>
  <category>human-computer interaction</category>
  <guid>https://chrisvoncsefalvay.com/posts/ai-human-interaction/</guid>
  <pubDate>Tue, 12 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/ai-human-interaction/cd091fdb-e9e7-49d9-9b11-2b1eb92e2f86.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Stochastic parrots, cap and gown edition</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/academic-generative-ai/</link>
  <description><![CDATA[ 




<p>It’s not every day that you find out you have climbed the exalted heights of another discipline. My work is pretty interdisciplinary, but it shocked me, too, that I’m apparently holding forth on neoliberalism and the epistemic question in African universities (<a href="https://archive.is/MgSAW">archive link</a>):</p>
<div id="fig-scispace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scispace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/academic-generative-ai/scispace.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scispace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Apparently, I’m commenting on neoliberalism in African universities.
</figcaption>
</figure>
</div>
<p>This, of course, came at some surprise to me, as I have never written anything on the topic. I have, however, written a lot about AI, and I have written a thing or two about Africa, so I guess it was only a matter of time before I was conflated with someone else. This time, the unwitting victim deprived of his credit was <a href="https://www.uj.ac.za/members/prof-amasa-philip-ndofirepi/">Prof.&nbsp;Amasa P. Ndofirepi</a>, who is an educational studies scholar at the University of Johannesburg. I have no idea how I ended up being credited with his work, but I’m sure it was an honest mistake.</p>
<p>The problem is, with AI, mistakes compound. So if an unwitting student were to ask for a quick literature review of neoliberalism on the subject, they might get something like this from Scispace:</p>
<blockquote class="blockquote">
<p>The literature on the impact of neoliberalism on knowledge production and dissemination in African universities has been extensively explored by various authors. Qosimova Gulbahor, in her paper “Placing Knowledge at the Centre of an Alternative Public Good Imaginary of African Universities,” discusses the alternative public good mission of African universities and the need for them to apply their knowledge infrastructure to community development challenges. Chris von Csefalvay, in his paper “The Hegemonic Neoliberal Knowledges in the African University,” examines the pervasive presence of neoliberalism in African universities and explores the prospects and opportunities to unyoke the trapped knowledge processes. These authors, along with others, highlight concerns about the dominance of Western knowledge, the commodification of knowledge, and the need for African universities to prioritize socially-just knowledges that serve African priorities and challenges.</p>
</blockquote>
<p>I mean, that’s flattering, but I’d really rather be credited mostly for my own work. I’m sure Prof.&nbsp;Ndofirepi would agree.</p>
<section id="why-i-care" class="level2">
<h2 class="anchored" data-anchor-id="why-i-care">Why I care</h2>
<p>This is, of course, not good for academia. We’ve generally been coasting from one crisis to another. We’ve got a replication crisis, there’s <a href="https://retractionwatch.com/2020/09/30/author-says-misguided-efforts-for-the-ideal-western-blot-led-to-the-withdrawal-of-these-studies/">enough dodgy Western Blots to blot out the sun</a>, we’ve got the <a href="https://www.statnews.com/2023/07/19/marc-tessier-lavigne-stanford-president-resignation/">Tessier-Lavigne mess</a>, and that’s just what I can think of off the top of my head before my first coffee. A predatory publishing industry doesn’t help this at all. We need another crisis on top of this like we need a hole in the head, and yet, here we are.</p>
<p>Now, as far as I’m aware, no serious academic is actually using these tools to do their research. On the other hand, non-academics <em>are</em>. For journalists, in particular, such tools are a godsend – literature reviews are annoying, and if you can get a computer to do it for you, why not? The problem is, of course, that you’re supposed to double-check this stuff and, well, journalists are known for many things, but double-checking stuff properly isn’t really one of them.</p>
<p>And so, after months of academics fretting about ChatGPT eating their lunch, we’re confronted with the actual problem. AI is not better at producing decent science, but it is vastly faster and more efficient at producing <em>bad</em> science.</p>
<p>Which we weren’t short on to begin with.</p>
</section>
<section id="whats-the-problem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-the-problem">What’s the problem?</h2>
<p>Language is a tool that works on the basis of some conventions of meaning. Language models encapsulate these conventions, but they cannot encapsulate <em>all</em> of them – there are compromises to be made if a system with limited resources has to contend with nearly unlimited human imagination. When language models’ limits come to blows with domain-specific language, we get into trouble.</p>
<p>Language models are really weak at one thing: reasoned judgment. As a scientist, you are trained to exercise this kind of reasoned judgment in determining what is, and what isn’t, worth considering as an authority. That’s why we make our master’s students (and hopefully most undergraduates) write literature reviews until the cows come home. It teaches them to develop that judgment, and also to know how to explore the fringes of their research question. I have looked at a few ‘academic AI’ tools that claim to be doing some of this, and they’re not very good at it. <a href="https://scite.ai">Scite</a> is so far one of the better ones, and the literature reviews it produces are still pretty bad: results are heavily weighted towards recent publications, towards the specific in preference to the foundational and often towards meandering misinterpretations of the research question as long as sources for that could be found, in preference to actually identifying a gap.</p>
<p>To be quite fair towards these models, they have to deal with academic literature, which is an abundance of noise with a flicker of signal. There is, not to put too fine a point on it, a ton of crap out there, and it’s not always easy to tell the difference between the crap and the good stuff. That’s why we have peer review, and that’s why we have literature reviews. The problem is, of course, that these models are not trained on the literature, but on the internet. And the internet is a very different place from the academic literature indeed.</p>
<p>Academic writing, especially domain specific writing, has a language of its own. It’s not fair to expect a language model trained on English to also master uses of English that might as well be a different language. To give a favourite example of mine: in magnetic resonance imaging of the brain, there’s often talk of something called ‘flow voids’. Now, normal human reasoning would interpret a ‘flow void’ to be the absence of flow, or something along those lines. In MRI, a flow void is actually the opposite: it is a ‘void’ of signal created in a vessel through which something (usually blood, sometimes CSF) flows.<sup>1</sup> This is a very specific term that has a very specific meaning in a very specific context. A language model, however, would not know that. It would assume that a ‘flow void’ is (de)void of flow.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Flow voids happen in the context of spin-echo imaging. These modalities involve two pulses – an excitation pulse and a refocusing pulse. Blood that moves perpendicular to the image plane will be hit by the excitation pulse but not the refocusing pulse. Therefore, it will not create a signal, which gives us the ‘void’ appearance of signal hypointense vessels.</p></div></div><p>Just about <em>all</em> of science is like that. We have a language of our own, and it’s not always easy to understand. What definitely doesn’t make it easier to work with, however, is when the source material is also wrong. Which is what we’re dealing with all too often, viz. Figure&nbsp;1.</p>
</section>
<section id="what-can-we-do-about-it" class="level2">
<h2 class="anchored" data-anchor-id="what-can-we-do-about-it">What can we do about it?</h2>
<p>Most ‘academic AI’ applications are riding on the crest of a wave of high expectations that surround everything AI-related right now. They offer to be useful aides-de-camp to beleaguered academics who have to contend with exponentially growing literature, but in reality fall far short of that promise. And the inherent ‘black box’ nature of such models means that it’s not always easy to tell when they’re wrong.</p>
<p>At this point, perhaps the best we can do is to hold off on using generative AI tools for academic research until they’re better. We’re not there yet. We’re not even close. For highly domain specific applications, retrieval-augmented generation (RAG) approaches utilising a curated knowledge base of publications in that realm has proven to be a very useful approach indeed, but those are specialised tools that are at the present primarily in the purview of private industry. I’ve seen some great applications in this field, and I see this as another proof point for <a href="../team-of-rivals/">my assertion that the future belongs to ecosystems of small, specialised language models rather than one big model that does everything</a>.</p>
<p>For the time being, we’re going to have to do things the old-fashioned way: reading the literature and writing our own reviews. I know, it’s not very fun. But it’s the only way to do it right, and we’ve managed with that for the last few hundred years. Giving it another few years won’t hurt.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Stochastic Parrots, Cap and Gown Edition},
  date = {2023-12-06},
  url = {https://chrisvoncsefalvay.com/posts/academic-generative-ai/},
  doi = {10.59350/412q0-sbn89},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Stochastic Parrots, Cap and Gown
Edition.”</span> <a href="https://doi.org/10.59350/412q0-sbn89">https://doi.org/10.59350/412q0-sbn89</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>academia</category>
  <category>writing</category>
  <category>LLMs</category>
  <guid>https://chrisvoncsefalvay.com/posts/academic-generative-ai/</guid>
  <pubDate>Wed, 06 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/academic-generative-ai/4ae87543-b5d3-4373-b691-3fb71d8cf1d0.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Auto-DOI for Quarto posts via Rogue Scholar</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/auto-doi/</link>
  <description><![CDATA[ 




<p>I love posts that allow me to merge some of my addictions. In this case, it’s my love for Quarto project scripts (which I’ve <a href="../quarto-project-scripts">written about elsewhere</a>), my fondness for <a href="https://rogue-scholar.org">Rogue Scholar</a> and the overuse of the word ‘mint’ to mean ‘generally really quite rather nice’.</p>
<p><a href="https://rogue-scholar.org">Rogue Scholar</a> is a fantastic tool for science bloggers, and while it’s a little artisanal (i.e.&nbsp;hand-made much of the time) at this point, it’s got some really cool automated features. One is that it registers (mints, hence the abundance of lame peppermint puns across this post) DOIs for your posts.</p>
<p>I’ve been using Rogue Scholar to mint DOIs for my posts for a while now, but it’s always been a bit of a manual process. I’d have to wait for a while for the post to go on the Rogue Scholar feed, then copy/paste the DOI, then copy the DOI into the YAML front matter. It’s not a lot of work, but it’s a bit of a pain. I’ve been meaning to automate it for a while, but I’ve been busy with other things.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Just after I posted about this solution, Martin Fenner, who runs Rogue Scholar, <a href="https://github.com/quarto-dev/quarto-cli/discussions/7174#discussioncomment-7557741">pointed out that there’s now an API</a>. The API is great, and would have spared me the part of having to scrape the HTML. I will, one of these days, switch over – if I had to build it, I’d obviously use the API, and simply parse the JSON result. The rest, <em>ceteris paribus</em>, holds true.</p>
</div>
</div>
<p>This weekend, I was laid up with being on the receiving end (for once) of the bounties of a clinical trial, so I’ve decided to finally build it. It’s a bit of a hack, but it works.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-auto-doi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-auto-doi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-auto-doi">flowchart TD
    linkStyle default interpolate basis
    A["Get last 10 posts with DOIs from RS"]
    
    subgraph loop["Loop through posts"]
    B["Read YAML preface"] --&gt; C{"Is post a cross-post?"}
    C --&gt;|yes| skip
    C --&gt;|no| D{"Is post citeable?"}
    D --&gt;|no| skip
    D --&gt;|yes| E{"Do we have a DOI?"}
    E --&gt;|no| skip
    E ----&gt;|yes| F["Get DOI"]    
    end

    A -----&gt; E
    F --&gt; apploop["Append loop to YAML"]
    apploop --&gt; write["Write YAML"]
    write --&gt; qr["Quarto Render"]
    write --&gt; ghc["Github commit action"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-auto-doi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Auto-DOI flow chart.
</figcaption>
</figure>
</div>
</div>
</div>
<p>First, we scrape Rogue Scholar for titles and DOIs. Rogue Scholar’s CSS isn’t really helpful here, as the link isn’t a particular class/id of its own as far as I could discern, so I just grabbed the link by the fact that only DOI links are formatted like DOI links. Not the most elegant way, but it does the job.</p>
<div id="lst-getting-dois" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-getting-dois-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: Getting the DOIs from Rogue Scholar.
</figcaption>
<div aria-describedby="lst-getting-dois-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-getting-dois" style="background: #f1f3f5;"><pre class="sourceCode code-annotation-code python code-with-copy"><code class="sourceCode python"><span id="lst-getting-dois-1"></span>
<span id="lst-getting-dois-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> scrape_blog_for_dois(url) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="lst-getting-dois-3">    response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.get(url)</span>
<span id="lst-getting-dois-4">    soup <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BeautifulSoup(response.content, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'html.parser'</span>)</span>
<span id="lst-getting-dois-5">    articles <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> soup.select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'article'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span>
<span id="lst-getting-dois-6">    </span>
<span id="lst-getting-dois-7">    posts_with_dois <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="lst-getting-dois-8">    </span>
<span id="lst-getting-dois-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> article <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> articles:</span>
<span id="lst-getting-dois-10">        title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> article.select_one(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'h3'</span>).text</span>
<span id="lst-getting-dois-11">        doi_link <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>((a[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'href'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> article.select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> a[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'href'</span>].startswith(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://doi.org'</span>)), <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="lst-getting-dois-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> doi_link:</span>
<span id="lst-getting-dois-13">            doi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> doi_link.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://doi.org/'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="lst-getting-dois-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="lst-getting-dois-15">            doi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="lst-getting-dois-16">        posts_with_dois[title] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> doi</span>
<span id="lst-getting-dois-17"></span>
<span id="lst-getting-dois-18">    logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Found </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(posts_with_dois)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> posts with DOIs:"</span>)</span>
<span id="lst-getting-dois-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> title, doi <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> posts_with_dois.items():</span>
<span id="lst-getting-dois-20">        logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>doi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="lst-getting-dois-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> posts_with_dois</span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>Technically unnecessary, as Rogue Scholar currently only displays ten links, but hey.</li>
<li>This is where we split the DOI link into the link prefix and the DOI. We don’t need the prefix, so we just grab the second part of the split.</li>
</ol>
<p>Next, we iterate through each blog post. This is actually quite fast, since (1) we have relatively few of them, (2) they’re text documents. We parse the YAML preface at the beginning of each of them. This looks something like this:</p>
<div id="lst-preamble" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-preamble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: Pre-DOI YAML preamble example.
</figcaption>
<div aria-describedby="lst-preamble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-preamble" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="lst-preamble-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">categories</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-preamble-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Quarto</span></span>
<span id="lst-preamble-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">citation</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="lst-preamble-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">date</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> 2023-11-13</span></span>
<span id="lst-preamble-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">description</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Oh, that</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">''</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">s mint. We can finally use Rogue Scholar to mint DOIs for Quarto posts and append them automagically.'</span></span>
<span id="lst-preamble-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">google-scholar</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="lst-preamble-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">title</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Auto-DOI for Quarto posts via Rogue Scholar</span></span></code></pre></div>
</div>
</figure>
</div>
<p>What this tells us is that we do want a citation (someday), which is why we’re doing this in the first place. That, according to our beautiful flowchart in Figure&nbsp;1, means this post is eligible to get a DOI appended. We also know there isn’t one – DOIs are appended as key-value pairs (with the key being, unsurprisingly, <code>doi</code>) to the <code>citation</code> object in the YAML preface. So, we’ll see if we can get one by looking in the dictionary we scraped from Rogue Scholar in Listing&nbsp;1.</p>
<div id="lst-process-post" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-process-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: Processing a single post.
</figcaption>
<div aria-describedby="lst-process-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-process-post" style="background: #f1f3f5;"><pre class="sourceCode code-annotation-code python code-with-copy"><code class="sourceCode python"><span id="lst-process-post-1"></span>
<span id="lst-process-post-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> process_qmd_file(file_path: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, posts_with_dois: Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="lst-process-post-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(file_path, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> stream:</span>
<span id="lst-process-post-4">        contents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stream.read()</span>
<span id="lst-process-post-5"></span>
<span id="lst-process-post-6">    delim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'^---$'</span>, re.MULTILINE)</span>
<span id="lst-process-post-7">    splits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.split(delim, contents)</span>
<span id="lst-process-post-8">    yaml_preamble <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> splits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].strip() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(splits) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#</span></span>
<span id="lst-process-post-9">    rest_of_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> splits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(splits) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> contents</span>
<span id="lst-process-post-10"></span>
<span id="lst-process-post-11">    yaml_contents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml.safe_load(yaml_preamble) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_preamble <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="lst-process-post-12"></span>
<span id="lst-process-post-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_contents:</span>
<span id="lst-process-post-14">        citation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'citation'</span>)</span>
<span id="lst-process-post-15">        google_scholar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'google-scholar'</span>)</span>
<span id="lst-process-post-16">        categories <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'categories'</span>, [])</span>
<span id="lst-process-post-17">        title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml_contents.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'title'</span>)</span>
<span id="lst-process-post-18"></span>
<span id="lst-process-post-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check files from crosspost categories</span></span>
<span id="lst-process-post-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cross-post"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> category.lower() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> category <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categories):</span>
<span id="lst-process-post-21">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"citation"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="lst-process-post-22">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"citation"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="lst-process-post-23">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="lst-process-post-24">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Modified crosspost </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> to remove Google Scholar and/or citation reference.'</span>)</span>
<span id="lst-process-post-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="lst-process-post-26">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ensure that google-scholar is set to true if citation is required</span></span>
<span id="lst-process-post-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> citation <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> google_scholar:</span>
<span id="lst-process-post-28">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-scholar"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="lst-process-post-29">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Setting google-scholar to true for </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="lst-process-post-30"></span>
<span id="lst-process-post-31">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If citation is true but no DOI, and post exists in scraped posts</span></span>
<span id="lst-process-post-32">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> citation <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> posts_with_dois.get(title):</span>
<span id="lst-process-post-33">                yaml_contents[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'citation'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'doi'</span>: posts_with_dois[title]}</span>
<span id="lst-process-post-34">                logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Adding doi for </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.'</span>)</span>
<span id="lst-process-post-35"></span>
<span id="lst-process-post-36">        new_preamble <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> yaml.dump(yaml_contents).rstrip()</span>
<span id="lst-process-post-37">        new_yaml_doc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"---</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>new_preamble<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">---"</span></span>
<span id="lst-process-post-38"></span>
<span id="lst-process-post-39">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># write the modified YAML document back to file</span></span>
<span id="lst-process-post-40">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(file_path, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'w'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> yaml_file:</span>
<span id="lst-process-post-41">            yaml_file.write(new_yaml_doc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rest_of_post)</span>
<span id="lst-process-post-42">        logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Updated file </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>title<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="lst-process-post-43">        </span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>We have to split the document in two because only the preamble is proper, parseable YAML. The rest of the document is just text, so we have to recombine it later.</li>
<li>If it’s a cross-post, we don’t want it to have a Google Scholar link, and we’ll definitely not attach a DOI. In theory, we could have built this to be overridable in case I’ll ever produce a cross-post I <em>do</em> want to have a DOI, but I don’t see that happening.</li>
<li>While we’re at it, might as well prune the cross-posts.</li>
<li>And anything with a DOI should also get a Google Scholar metadata.</li>
<li>The <code>.rstrip()</code> is pretty useful – otherwise, every time you run this, you’ll get another newline appended to the YAML preface.</li>
<li>Don’t forget the <code>\n</code> before the YAML block’s end, otherwise you’ll end up with a YAML block that’s not properly separated from the rest of the document and won’t parse.</li>
</ol>
<p>Finally, we write the YAML back to the file, and we’re done. We can now declare this as a project script, and we’re good:</p>
<div id="lst-project-script" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-project-script-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: Declaring project scripts in _quarto.yml.
</figcaption>
<div aria-describedby="lst-project-script-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-project-script" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="lst-project-script-1"></span>
<span id="lst-project-script-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-project-script-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> website</span></span>
<span id="lst-project-script-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pre-render</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-project-script-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_doi_from_rogue_scholar.py</span></span>
<span id="lst-project-script-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span></span></code></pre></div>
</div>
</figure>
</div>
<p>One thing worth noting is that we’re not actually running this on the Quarto project itself, but on a copy of it. The consequence is that the changes are made ‘on the fly’ to the <code>.qmd</code> files and do not necessarily propagate into the repo. This is a pain, because recall that we’re only fetching the last ten posts’ DOIs so as to be kind on the server: as time goes on, that means older posts ‘lose’ their DOI.</p>
<p>To prevent this, we can simply check our changes back in:</p>
<div id="lst-gh-action" class="yaml listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-gh-action-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: Github action to commit changes.
</figcaption>
<div aria-describedby="lst-gh-action-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-gh-action" style="background: #f1f3f5;"><pre class="sourceCode code-annotation-code yaml code-with-copy"><code class="sourceCode yaml"><span id="lst-gh-action-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">on</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">workflow_dispatch</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">push</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">branches</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> main</span></span>
<span id="lst-gh-action-5"></span>
<span id="lst-gh-action-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Quarto Publish</span></span>
<span id="lst-gh-action-7"></span>
<span id="lst-gh-action-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">jobs</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-9"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">build-deploy</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-10"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runs-on</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> ubuntu-latest</span></span>
<span id="lst-gh-action-11"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">permissions</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-12"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">contents</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> write</span></span>
<span id="lst-gh-action-13"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-14"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Check out repository</span></span>
<span id="lst-gh-action-15"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> actions/checkout@v4</span></span>
<span id="lst-gh-action-16"></span>
<span id="lst-gh-action-17"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Set up Quarto</span></span>
<span id="lst-gh-action-18"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> quarto-dev/quarto-actions/setup@v2</span></span>
<span id="lst-gh-action-19"></span>
<span id="lst-gh-action-20"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Install Python and dependencies</span></span>
<span id="lst-gh-action-21"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> actions/setup-python@v4</span></span>
<span id="lst-gh-action-22"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">with</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-23"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">python-version</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'3.10'</span></span>
<span id="lst-gh-action-24"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cache</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pip'</span></span>
<span id="lst-gh-action-25"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> pip install jupyter</span></span>
<span id="lst-gh-action-26"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> pip install -r requirements.txt</span></span>
<span id="lst-gh-action-27"></span>
<span id="lst-gh-action-28"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Render and Publish</span></span>
<span id="lst-gh-action-29"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">uses</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> quarto-dev/quarto-actions/publish@v2</span></span>
<span id="lst-gh-action-30"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">with</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-31"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">target</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> gh-pages</span></span>
<span id="lst-gh-action-32"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">        </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">env</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="lst-gh-action-33"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">          </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">GITHUB_TOKEN</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> ${{ secrets.GITHUB_TOKEN }}</span></span>
<span id="lst-gh-action-34"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span></span>
<span id="lst-gh-action-35"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">name</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> Commit changes to reflect source file  changes</span></span>
<span id="lst-gh-action-36"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">        run</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">|</span></span>
<span id="lst-gh-action-37">          git config --global user.name 'Chris von Csefalvay'</span>
<span id="lst-gh-action-38">          git config --global user.email 'chrisvoncsefalvay@users.noreply.github.com'</span>
<span id="lst-gh-action-39">          git  diff-index --quiet HEAD || git commit -am  "Automated commit of changes to source files"</span>
<span id="lst-gh-action-40">          git push</span></code></pre></div>
</div>
</figure>
</div>
<ol type="1">
<li>The <code>diff-index --quiet HEAD</code> checks if there have been changes to the working tree. <code>git</code> returns an error if you’re trying to commit on an empty working tree, so we’re checking for that first.</li>
</ol>
<p>And that’s it. We can now run this as a Github action, and it’ll automatically append DOIs to our posts.</p>
<p><a href="quarto-project-scripts/">As noted</a>: Quarto project scripts are pretty awesome stuff. I’m thinking of setting up an <code>awesome-</code> for it on Github, because way too few of them are shared properly. I’m hoping this will change.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Auto-DOI for {Quarto} Posts via {Rogue} {Scholar}},
  date = {2023-11-13},
  url = {https://chrisvoncsefalvay.com/posts/auto-doi/},
  doi = {10.59350/5hxdg-fz574},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Auto-DOI for Quarto Posts via Rogue
Scholar.”</span> <a href="https://doi.org/10.59350/5hxdg-fz574">https://doi.org/10.59350/5hxdg-fz574</a>.
</div></div></section></div> ]]></description>
  <category>Quarto</category>
  <guid>https://chrisvoncsefalvay.com/posts/auto-doi/</guid>
  <pubDate>Mon, 13 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/auto-doi/free-photo-of-a-drink-with-mint-in-a-glass-and-a-small-cookie-on-a-plate.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The skeuomorphic fallacy</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/</link>
  <description><![CDATA[ 




<p>It appears that in what is clearly a wonderful little PR stunt, <a href="https://archive.ph/Xgg38">a Polish rum company managed to do a Sophia and appoint an ‘AI-driven’ ‘robot’ as its ‘CEO’</a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The other guilty party to this pile of steaming bovine excrement is Hanson Robotics, famous for giving us Sophia, the “world’s first robot citizen”. Most of what I’m saying here goes just as well for Sophia. It blows my mind that Hanson managed to get away with this nonsense <em>not once, but twice</em>.</p>
</div></div><blockquote class="blockquote">
<p>Dictador has just announced hiring the first world ever AI robot as a CEO of a global company. The new CEO is a human-like robot, incorporating AI. The robot is a woman, named Mika. She will be the official face of Dictador, the world’s most forward-looking luxury rum producer. This bold move consolidates the company’s position as one of the most advanced and thought-leading organizations globally. It underlines the brand’s passion for new technology and offers a positive disruption by bringing the future to what can still be a very traditional world.</p>
<p>– <a href="https://www.prnewswire.com/news-releases/dictador-announces-the-first-robot-ceo-in-a-global-company-301634165.html">Dictador, via PR Newswire</a></p>
</blockquote>
<p>Oh good grief.</p>
<p>Now, let’s for a moment try to get over the obvious bullshit here, such as the fact that corporate officers generally have to be natural persons in most places, and may at worst be legal persons, but a ‘robot’ or an ‘AI’ is neither. You can no more appoint a ‘robot’ or an ‘AI’ as your CEO than you can appoint a pet rock to be your corporate secretary. I am, to the surprise of many, a recovering corporate lawyer, so I have at one point convinced examiners that I know what I’m talking about when it comes to this stuff. There are complex aspects of law involved in the question of who can or cannot, legally, be a corporate officer. But let’s get over that.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/6506c915cd637c0019c66997.webp" class="img-fluid figure-img"></p>
<figcaption>“Mika”, the “CEO” of Dictador. Artificial, yes. Intelligence, not quite. Uncanny Valley, totally.</figcaption>
</figure>
</div>
<p>Let’s also get over what an absolutely disastrous piece of nonsense this seems to be on its face. Here’s a quote from the ‘CEO’ itself:</p>
<blockquote class="blockquote">
<p>In a Dictador company video, Mika said that “with advanced artificial intelligence and machine learning algorithms, I can swiftly and accurately make data driven.”</p>
<p>– <a href="https://archive.ph/Xgg38">Kayla Bailey, <em>‘Mika’ becomes world’s first AI human-like robot CEO</em>, Fox Business</a></p>
</blockquote>
<p>It would definitely be useful for Mika to be able to finish a sentence. In the industry, crap like this is called a ‘sell signal’, and in a slightly more sane economic environment, Dictador would have the market value of, uh, dunno, an expired carton of eggs after this nonsense. But let’s get over that, too.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The commentary on this nonsense is, if anything, worse than the nonsense itself. Forbes contributor Cindy Gordon <a href="https://archive.ph/Her3m">titled her article on the subject “How Should CEO’s Embrace AI Or Will AI Assume CEO Roles?”</a>. Clearly expecting an understanding of when to use apostrophes is now beyond Forbes contributors’ ken. The world is in safe hands: people who cannot construct a sentence that would pass muster in a third-grade English class are now writing about the future of corporate governance in the age of AI in Forbes.</p>
</div></div><p>Let’s instead talk about skeuomorphy.</p>
<section id="skeuowhat" class="level1 page-columns page-full">
<h1>Skeuowhat?</h1>
<p>A skeuomorph is a kind of design element that is no longer necessary for the function of the object, but is retained for aesthetic reasons. For example, your iPhone’s Notes app used to use some visual elements from physical note pads (such as the ‘legal pad yellow’ background) for a pretty long time, even though it was not in any sense necessary. Skeuomorphs are a way to acclimate our somewhat sluggish human brains to a changing world.</p>
<p>Skeuomorphs also illustrate an important point: what something appears as is not the same as what it is. The interface is not the object. The interface is, to say the blindingly obvious, <em>the interface</em>: the tool through which an object communicates. Confuse the two, and you get blinding bullshit like that ‘robot CEO’ nonsense.</p>
<p>On the other hand, we’re not in any way immune to what I shall call the skeuomorphic fallacy, the confusion that the visual, linguistic or other forms through which something communciates equate to the essence of that thing. Where something as sensitive as language is involved, <a href="../llms-language">which I have written about before</a>, this hits particularly hard. For better or worse, we connect language so closely to the concept of being human that anything capable of producing decent linguistic output gets a level of human-like credence accorded to it <em>prima facie</em>.</p>
<p>And this is not entirely wrong, either. <a href="../llms-language">As I have stated before</a>, language is a uniquely human trait that is so intricately intertwined with what it means to be human in a human society that it is hard to unlink it from personhood. It is not a necessary characteristic of personhood, but it has certainly been often seen as a sufficient one. To speak is to be able to speak <em>up</em> as well, and to be able to speak <em>up</em> is to be able to advocate for rights. It is a lot easier to accord human rights for one that can speak for those rights.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Harris makes this point, and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=989542">my old law school professor Tim Endicott gives a wonderful refutation in <em>The Infant in the Snow</em></a>, one of the best papers on the philosophy of personhood and the origin of rights (and the qualifications needed to have them) that I have ever read.</p>
</div></div></section>
<section id="where-skeuomorphy-becomes-a-fallacy" class="level1 page-columns page-full">
<h1>Where skeuomorphy becomes a fallacy</h1>
<p>The problem is, of course, that as noted above, <em>the interface is not the substance</em>. The reality is that Mika is no more human than my bedside lamp. The only reason why nobody has appointed my bedside lamp a CEO is, of course, that it does not pretend to look and speak like a human. But if we look at what it means to be a legal subject – be it a citizen (<em>viz.</em> Sophia) or a corporate officer (<em>viz.</em> Mika) –, it becomes abundantly clear that there’s much more to that than being able to <em>pretend</em> to look and speak like a human. It is to be able to <em>be</em> a human. And that is something that Mika, Sophia and all the other ‘AI’ and ‘robot’ nonsense out there is not.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I am really being incredibly generous by dignifying whatever these Hanson products do with the term ‘AI’. They are Disney animatronics with a bottom-tier language model and an average speech synthesiser. Artificial that may be, but intelligence it is not.</p>
</div></div><p>The skeuomorphic fallacy is the fallacy of confusing the interface with the substance, confusing the appearance of being human with the essence of being human. Where things go wrong is the confusion of something in a humanoid body (being generous here) and using a quintessentially human skill that we’re somewhat extremely attached to (i.e.&nbsp;language) on one hand with being human on the other.</p>
<p>Which is a problem, because appointing someone CEO isn’t a publicity stunt. The ‘O’ in CEO stands for <em>officer</em>. A corporate officer is, even given limited liability, the person with whom the buck stops. Limited liability does create legal (corporate) personhood, which insulates corporate officers of <em>some</em> aspects of liability, but not all of them. Lord Thurlow pointed out that a company couldn’t be expected to have a conscience if it had no body to kick and no soul to damn – the need to eventually have some nominate humans in the process serves to a great degree to provide that conscience and that body. Even notions like corporate homicide, which became a thing in England &amp; Wales just when I took corporate law (and a lovely mess that did of finals papers!), are predicated on the notion that there are human beings who, collectively, operate a system that is ultimately responsible for the actions of the company. You don’t necessarily have to have a body to kick, nor really a soul to damn, but you do definitely have to have a <em>mens rea</em>, and that is something that a robot or an AI is not capable of.</p>
<p>This isn’t just a fine point about corporate law by a recovering legal philosopher. This is what happens when we lapse in our understanding of what it means to be human, and begin to accord human rights and responsibilities to non-human things. We’re cognitively geared to associate language with personhood, so this is not a particularly surprising lapse of reason. We’re humans, and that’s why our response to a speaking robot becoming CEO is “oh, that’s innovative” as opposed to “let’s consider if some people might need to spend some time away from their favourite hallucinogens and/or the general population”. If a company appointed my bedside lamp as a corporate officer, we’d be much more likely to consider that a sign that the latest office party went a little overly generous on the acid. But because we’re cognitively geared to consider language a uniquely human trait, we’re much more likely to be tempted to consider this some Valley quirkiness as opposed to frank insanity.</p>
</section>
<section id="why-so-serious" class="level1 page-columns page-full">
<h1>Why so serious?</h1>
<p>It’s a legitimate question: why are you so bothered by this nonsense? It’s just a PR stunt, after all. And you’re right: it is just a PR stunt. But it is a PR stunt that is part of a larger pattern of nonsense that is being peddled by the likes of Hanson Robotics and others. And it’s damaging.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/FzpQYv2XwAsAABN.jpeg" class="img-fluid figure-img"></p>
<figcaption>Freddy Fazbear, titular antagonist of the Five Nights at Freddy’s franchise.</figcaption>
</figure>
</div>
<p>We don’t consider Freddy Fazbear, an animatronic bear, to be an authority on what it means to be an ursine individual. Why is this more patently ridiculous than considering an animatronic human to be capable of bearing indiciae of human personhood, such as citizenship or corporate office?</p>
</div></div><p>It’s damaging to humanity, because it leads to a gradual misunderstanding of what the essence of human existence, of being human, is: it muddles the picture, confusing <em>what things are</em> with <em>how things behave</em>. It is Searle’s Chinese Room all over again: the idea that a system that behaves like a human is a human. It is not. It is a system that behaves like a human, no more, no less. And that is a very important distinction.</p>
<p>It’s damaging to the field of AI, because it leads to a misunderstanding of what AI is and what it can do. It is a field that is already rife with hype and nonsense, and this kind of nonsense just adds to the pile. It is a field that is already struggling to be taken seriously, and this kind of nonsense just makes it harder to be taken seriously.</p>
<p>There are powerful, sophisticated AI algorithms that identify disease in cytopathological specimens, find cracks in compressor blades and detect anomalies as part of intrusion detection algorithms. These are deeply sophisticated and intricate solutions. They are also deeply unsexy. They are not the kind of thing that gets you on the cover of Forbes – just the kind of thing that makes a difference in real lives day in, day out, the kind of thing that is being drowned out by the nonsense that is being peddled here. Many of these systems have outputs that wouldn’t look out of place in the 1990s: terminal outputs, text files, CSVs.</p>
<p>They make this world a better, safer, healthier place. They don’t need animatronic bodies. They just do something important, and do that well. And that is what AI is about: it is not about making a PR stunt out of a glorified animatronic. It is about making a difference, and that is what we should be focusing on.</p>
<p>Which makes Mika an unwelcome distraction, and one that should be a concern for all of us. It’s the symptom of a disease – a fallacy of confusing the interface with the substance – that is spreading. And it’s a disease that we should be fighting, lest it makes us worse at being humans, and worse at being AI researchers.</p>
<p>The world is looking to us for calling out this kind of bullshit. So, please, as part of our professional involvement with the world, let’s start doing that with extreme prejudice.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The Skeuomorphic Fallacy},
  date = {2023-11-06},
  url = {https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/},
  doi = {10.59350/k9kka-90x56},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The Skeuomorphic Fallacy.”</span> <a href="https://doi.org/10.59350/k9kka-90x56">https://doi.org/10.59350/k9kka-90x56</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/skeuomorphic-fallacy/</guid>
  <pubDate>Mon, 06 Nov 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Teams of Rivals</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/team-of-rivals/</link>
  <description><![CDATA[ 




<p>In the first four entries (<a href="../llms-language">1</a> <a href="../lyre-of-hephaestus">2</a> <a href="../moral-maps">3</a> <a href="../prompt-engineering">4</a>) of this sequence, I have focused primarily on what LLMs aren’t, can’t, won’t, wouldn’t and shouldn’t. It’s probably time to conclude this series by that much awaited moment in all stories, where the darkest night finally turns into a glorious dawn, where we finally arrive at the promised land, where we finally get to talk about what LLMs <em>could</em> be.</p>
<p>What I see as the most successful potential model of using LLMs is as a pack of semi-autonomous decision-makers with their own role, competing in a connectionistic, hierarchical model where each layer feeds into the next that performs some adjudicative or aggregative function. Structured this way, LLMs could become teams of rivals, analogous to the teams of rivals in the real world that we rely on to answer hard questions. For what that would look like, we might need to look at a story of my own. This story begins on the morning of 01 December 1948 on Somerton Beach, near Adelaide, Australia.</p>
<section id="tamam-shud" class="level1 page-columns page-full">
<h1>Tamam shud</h1>
<p>On that fateful morning, the body of a middle aged man was found, slumped against the seawall at Somerton Beach, dead of causes that were far from clear. Neither was his identity, as he did not carry identification (quite unusual at the time, so close after wartime, when identification was still mandatory), nor could his identity be ascertained by other means. It was, in fact, clear that he went some lengths to conceal his identity, having even removed the manufacturers’ tags from his clothes. The autopsy performed by the police pathologist only compounded the mystery by identifying multiple potential causes, each of which would have been sufficient to bring about the unknown man’s demise. The mystery kept compounding when a scrap from Omar Khayyam’s <em>Rubaiyat</em> was found in his pocket. It read <em>tamam shud</em>, which roughly translates to “it is finished”. Investigators traced this copy of the <em>Rubaiyat</em> to a copy belonging to a young lady whose identity wouldn’t be revealed until after her death in the 1990s, and whose involvement remains unclear. What appeared to be an acrostic cypher was also found in his notes. And so began a three-quarter-century mystery that has captivated many, including the author of these lines.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/team-of-rivals/SomertonMan2.jpg" class="img-fluid figure-img"></p>
<figcaption>Police photo of the Somerton Man, taken by South Australian police and disseminated for identification purposes.</figcaption>
</figure>
</div>
</div></div><p>To this day, the exact identity of the Somerton Man, who he really was and why he had to die, remains unknown. The DNA Doe Project, who have a pretty good track record at this sort of stuff, have identified him as Carl Webb, an Australian instrument maker and electrical engineer. This raises more questions than it answers. And while it is undoubtedly one of the most intriguing of human stories, it is also an interesting lesson in the way we arrive at knowledge and understanding.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-coronial-flow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coronial-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-coronial-flow">flowchart TD
    A["Witnesses"] --&gt; B["Police"]
    C["Involved parties"] --&gt; B["Police"]
    Bs["Bystanders"] --&gt; B
    E("Field investigation") ---&gt; B
    F("Material evidence") --&gt; E
    B ---&gt; Cor["Coronial inquest"]
    P["Pathologist"] --&gt; Cor
    X["Expert witnesses"] --&gt; Cor
    X -.-&gt; B
    E -.-&gt; Bs
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coronial-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Information flow of the coronial inquest into the death of the Somerton Man, 01 December 1948.
</figcaption>
</figure>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>In Commonwealth jurisdictions, coroners are judicial officers tasked with determining who the decedent is, the place and time of their death and how the decedent “came by his death”, a somewhat archaic way of describing the narrow question of causation.</p>
</div></div><p>Human processes intended to find the truth are complex, as Figure&nbsp;1 shows. In general, we rely on three key features:</p>
<ol type="1">
<li>Adversariality: especially in adversarial legal systems, we use parties with opposing motivations and interests to act as controls on the other side. The idea is that no party has as much of an incentive to poke holes in the other side’s case as someone whose interests are incompatible with the other side’s. This is why we have the adversarial system in the first place, as opposed to a single dispassionate judge who is supposed to be the sole arbiter of truth. The adversarial system is a way of using the self-interest of the parties to arrive at the truth. It essentially outsources the job of finding the truth to the parties, who are supposed to be motivated to control the other side because it is in their interest to do so.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>Coronial hearings are inquisitorial, not adversarial, but there’s an element of adversariality in the way the coroner is supposed to investigate the causal hypotheses put to him.</p>
</div></div><ol start="2" type="1">
<li>Hierarchicality: we use layers of hierarchically arranged players to arrive at the truth. Quite typically, these are hierarchies of adjudication, where each ‘layer’ looks at the previous layer’s products and works to determine which of potentially multiple competing hypotheses are going to prevail. This may be the product of an adversarial layer, or it may simply be review, as would be the case where one layer looks at what a previous layer’s findings were and decides whether they were correct or not, against some superordinate norm or evidential rules.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>If you’ve ever watched Law &amp; Order, you know what I’m talking about here. Police gather evidence, DAs build a case and plead it, and the judge and jury decide.</p>
</div></div><ol start="3" type="1">
<li>Roles: we use players who are specialists of roles, not general ‘truth-finding agents’. The purview of the police pathologist is different from that of a witness. Authority is limited by their role. In short, everyone wields – to borrow Stephen Jay Gould’s term – an epistemic ‘magisterium’, a domain over which their authority is valid.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>The case of the Somerton Man is particularly illustrative of this: there were different experts for toxicology, a pathologist, even a witness on tram times.</p>
</div></div><p>These three features are what make human processes of truth-finding work. Truth-finding is the work of a team of rivals, with often orthogonally opposed interests but ultimately in the same enterprise of discerning truth. And this is what LLMs could be: teams of rivals, each with their own magisterium, each with their own role, each with their own motivations and interests, but ultimately working towards the same goal.</p>
</section>
<section id="dueling-llms" class="level1 page-columns page-full">
<h1>Dueling LLMs</h1>
<p>Let’s assume that we’re faced with something simpler than a 75-year-old Cold War mystery. Say, we’re interested in something this simple: we want to make a cheesecake that is tasty, low-calorie, easy to make in no more than an hour and does not require any special ingredients. We could use a single LLM to do this:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>スフレチーズケーキ (soufflé cheesecake)</p>
<ul>
<li>1tbsp unsalted butter</li>
<li>6 large eggs - separate out the yolks</li>
<li>10oz cream cheese</li>
<li>4tbsp unsalted butter</li>
<li>1 cup heavy cream</li>
<li>4tbsp sugar</li>
<li>1 cup cake flour</li>
<li>2tbsp lemon juice</li>
<li>2tbsp jam of your choice</li>
</ul>
<p>Grease cake pan and line with parchment paper. Preheat oven to 350ºF. Mix cream cheese, butter and sugar in a bowl submerged in a saucepan of warm(ish) water. Add the egg yolks and using a strainer, sift the cake flour. Filter and add lemon juice and some lemon zest, if you have any hanging around. Whisk up the egg whites and gently mix it into the main mixture. This should make it somewhat nicely frothy. Pour into cake pan. Place cake pan onto baking sheet. Bake for 60 minutes or so. When done, turn off the heat, open the door and brush with your jam diluted at around 1:1 ratio with warm water. Apricot jam works best, but I won’t judge you for your choice.</p>
</div></div><div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-simple-cheesecake" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simple-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-simple-cheesecake">flowchart TD
    A["What's a good cheesecake recipe..."] --&gt; B["Here's a recipe..."]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simple-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: How not to ask for a good cheesecake recipe.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here’s a better approach:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-better-cheesecake" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-better-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-better-cheesecake">flowchart TD
    A["What's a good cheesecake recipe..."]
    A --&gt; G1
    A --&gt; G2
    A --&gt; GN
    G1["Generator 1"] --&gt; r1("Recipe 1")
    G2["Generator 2"] --&gt; r2("Recipe 2")
    GN["Generator n"] --&gt; rn("Recipe n")
    r1 --&gt; Dt["Tastiness\ndiscriminator"]
    r2 --&gt; Dt
    rn --&gt; Dt
    r1 --&gt; Tt["Time\ndiscriminator"]
    r2 --&gt; Tt
    rn --&gt; Tt
    r1 --&gt; Id["Ingredients\ndiscriminator"]
    r2 --&gt; Id
    rn --&gt; Id
    Dt --&gt; jagg["Judgment aggregator"]
    Tt --&gt; jagg
    Id --&gt; jagg
    jagg --&gt; rs("Selected recipe")
    jagg --&gt; rdc("Decisional context")
    rs --&gt; G1p["Generator 1"]
    rs --&gt; G2p["Generator 2"]
    rs --&gt; GNp["Generator n"]
    rdc --&gt; G1p["Generator 1"]
    rdc --&gt; G2p["Generator 2"]
    rdc --&gt; GNp["Generator n"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-better-cheesecake-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A better way to ask for a cheesecake recipe. We ask multiple generators to generate recipes, then ask narrowly defined LLMs (discriminators) to determine how far they comply with the constraints. Take the result as well as the decisional context (why the ‘winning’ recipes ‘won’) and feed them to a new set of generators, asking for new recipes. Repeat until adequate cheesecake is produced.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note the ‘rivalry’ between various generators, each of which is trying to produce a recipe that satisfies the criteria. If this reminds you of a little something you might have seen before called adversarial generative networks, you are not wrong. In both cases, we are using discriminators to determine how good a particular output is, and tweak parameters to start selecting for outputs that have higher acceptance, i.e.&nbsp;outputs the discriminator believes is more likely to meet the criteria we posed. Where this model transcends the mere generative adversarial model is in two ways: the use of roles and the use of propagating the decisional context.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The <em>decisional context</em> refers to the ‘why’ of the decision. In the case of the cheesecake, it would be the reasons why the selected recipes were selected. This is important because it allows us to propagate the decisional context to the generators, so that they can learn from it and produce better outputs. This is a form of meta-learning, where the generators learn from the way their past performance was evaluated, and use it to produce better outputs. Unlike reinforcement learning or adversarial learning, it leverages the fact that LLMs can take in a broader ambit of information: not just the relatively binary outcome of what prevailed but also the <em>why</em>.</p>
</div></div><ol type="1">
<li><p>We leverage roles of discriminators, and these can be nested and hierarchically structured to an arbitrary degree. Recall that the discriminator in a GAN could essentially go one way: accept or reject, with a given match likelihood. Not only can we have specialised discriminators here, we can also build networks of discriminators. For instance, we may ‘explode’ the notion of ingredient “availability” into subcomponents: what fruits are in season for the cheesecake, what is ubiquitous at one’s individual location and so on. This again brings us to the ‘strength of the pack’: the connectionist idea that we can create hierarchically layered simple units that can produce complex outputs.</p></li>
<li><p>We leverage the decisional context because we have something that can generate in response to a <em>why</em>. This is a key difference between LLMs and other machine learning models: they can take in a broader ambit of information, and they can generate in response to that. A typical backpropagative model still ‘learns’ the way all ML models do: by trial and error. LLMs can learn from feedback. They can learn not just from the <em>fact</em> of their success or failure but also from the <em>cause</em> of that outcome, as explained to it by another agent –– which can of course be an LLM.</p></li>
</ol>
<p>In short, what we want is a team of rival LLMs, loudly arguing their case, and another team of LLMs assessing their relative claims. These, in turn, may be given incentives to compete. The end result is a cognitive net structure that uses LLMs not as ends in themselves but as primitives, whether it be as simple discriminators or as more complex agents. Figure&nbsp;3 does not merely describe a more complex structure than Figure&nbsp;2 – it does, but that’s really not the point. The point is that different roles and functions create emergence faster than the best single LLM. We see this in the difference between simple multilayer perceptrons, which are essentially identical layers of neurons with activation functions thrown on top of each other, versus modern deep learning, which relies on different layers (pooling, convolution and so on). This is, incidentally, a very well learned lesson from neuroscience: the complexity emerges not from heaping neurons on top of each other, but from doing so with quite significantly different structures and functions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I’ve skimmed this point on <a href="../prompt-engineering">my post about prompt engineering</a>, but it bears repeating: the best single LLM is not as good as a team of rival LLMs.</p>
</div></div></section>
<section id="into-the-wild" class="level1 page-columns page-full">
<h1>Into the wild</h1>
<p>Something that more complex architectures are capable of that a single question-answering LLM won’t is to be able to deal with the real world. The real world is messy, and it exists beyond the confines of code. I trust that we’ve all overcome our instinctual fear that AI models interacting with the physical world seem to engender these days. I think we can safely conclude that Skynet won’t emerge from letting an LLM agent use Google. If we grant that, then we can move towards the next step: the tool-using LLM.</p>
<p>At some point a little under 4 million years ago, in what is today Ethiopia, a particularly clever ancestor of ours, the hominin <em>Australopithecus afarensis</em>, used a kind of a primitive middle ground between a knife and an axe to dismember his prey. This might have been the first evidence we have for tool use. We consider tool use to be one of those watershed moments (I wrote about <a href="../llms-language">the other one here</a>) where we got something that wasn’t just doing what it has been except better, but a qualitatively different creature. Tool use is how we really interact with our environment, and how we can probe that environment and learn from what it does in return. Giving LLMs the tools to be ‘out in the world’ is to give them the kind of agency that is required to learn and operate in an autonomous way that goes beyond RLHF (reinforcement learning by human feedback), the current paradigm of training LLMs. At some point, we will need to let them loose in the wild, and let them learn from their own experience. We need to let them <em>have</em> experience, and that’s arguably more important than what an agentic LLM can do for us. In the end, we’ve all learned more through the mistakes (often hilarious ones) we’ve stumbled into in the real world than we ever did in the classroom. And so will LLMs.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg" class="img-fluid figure-img"></p>
<figcaption>Endocasts of the skull of <em>Australopithecus afarensis</em> from the <a href="https://www.science.org/doi/10.1126/sciadv.aaz4729">paper by Gunz <em>et al.</em> (2020)</a>. A-G show specimens from Dikika, Ethiopia, where evidence for tool use was found.</figcaption>
</figure>
</div>
</div></div><p>Tool use is not just about learning, though. It’s also about the utility of these models to us. The ubiquitous “knowledge cutoff” message encountered with LLMs is an indication of what happens if LLMs are stagnant: they become reflections of the past, echoes of what they were taught rather than actual interacting entities. If we want them to live in the present, we need to teach them to interact with the present.</p>
</section>
<section id="team-building" class="level1 page-columns page-full">
<h1>Team building</h1>
<p>The main task, then, for AI developers is going to be one of building these teams of rivals. This process, if other connectionist systems (looking at you, neural networks) is anything to go by, will require a lot less human effort than we’d think. There’s no reason why LLMs themselves can’t take some of the role in constructing these teams the way the structure of deep learning effectively builds its own filters and feature extractors from its own ‘experience’. The equivalent of structuring deep learning models’ architectures by determining layer types and their connections is borne out in how we build these virtual teams of rival LLMs.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lang-model-architectures" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lang-model-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-lang-model-architectures">flowchart TD
    Input --&gt; L1

    subgraph L1["Layer 1"]
        L11["Convolution + ReLu"] --&gt; P11["Pooling"]
    end

    L1 --&gt; L2

    subgraph L2["Layer 2"]
        L21["Convolution + ReLu"] --&gt; P21["Pooling"]
    end

    L2 -.-&gt; LN

    subgraph LN["Layer N"]
        LN1["Convolution + ReLu"] --&gt; PN1["Pooling"]
    end

    LN --&gt; F

    F["Flatten"] --&gt; FCN["FCN"]
    FCN --&gt; Softmax
    Softmax --&gt; Output 


    I2["Input"]

    subgraph LL1["Layer 1"]
        LLL1["Generator"] --&gt; LLL2["Discriminator 1"]
        LLL1 --&gt; LLL22["Discriminator 2"]
        LLL1 --&gt; LLL2N["Discriminator n"]
    end

    I2 --&gt; LL1

    subgraph LL2["Layer 2"]
        LL2G["Generator"] --&gt; LL2D["Discriminator"]
        LL2G --&gt; LLL2D2["Discriminator 2"]
        LL2G --&gt; LLL2DN["Discriminator n"]
    end

    LL1 --&gt;|Decision + context| LL2

    subgraph LLN["Layer n"]
        LLNG["Generator"] --&gt; LLND1["Discriminator 1"]
        LLNG --&gt; LLND2["Discriminator 2"]
        LLNG --&gt; LLNDN["Discriminator N"]
    end

    LL2 -.-&gt;|Decision + context| LLN

    LLN --&gt; P["Parser"]

    P --&gt; Q["Output"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lang-model-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Hierarchical model architectures. A simple convolutional neural network (left) and a hierarchical generative multi-tiered language model (right). Note the similarities in the way that increasing complexity creates the emergence of a more powerful analytical tool.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Of course, the structure laid out in Figure&nbsp;4 is the simplest possible such construct, consisting essentially of a single type of layer for the most part. We know from deep learning that that can get us a pretty long way if the first and last layers are right, but in addition to different kinds of discriminators (the way we had different discriminators for time, ingredients and so on in our example in Figure&nbsp;3), we can interject three other kinds of players:</p>
<ol type="1">
<li>Supervisors: One unhelpful tendency of LLMs is that they are still somewhat autoregressive, meaning that every token generated conditions the next token’s likelihood, and so on. The consequence is that errors accumulate. Supervisors can stem this tendency by interposing themselves between stages of generative inputs and curbing this accumulative error.</li>
<li>Aggregators: These are the players that take the outputs of the discriminators and aggregate them into a single output. This is the equivalent of the ‘judgment aggregator’ in Figure&nbsp;3, which ultimately chooses the correct recipe.</li>
<li>Encapsulators: These players take the output from a fairly complex system and give us the simple(ish) output that we typically desire from most such systems. The system’s own inherent complexity notwithstanding, we often want simple answers even to complex questions, no matter with how much complexity those answers were arrived at. Encapsulators are the players that give us that simple answer.</li>
</ol>
<p>The possibility exists, of course, for other new kinds of players. Just as we have seen the emergence of new architectures in deep learning, we’ll likely see new forms of utilisation that are at this point still unexpected.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>As well as novel, unexpected uses of existing models. The archetypal example here is, of course, the transformer, which was initially designed for machine translation. Safe to say that it far exceeded the confines of its expected niche.</p>
</div><div id="ref-yao2022react" class="csl-entry">
Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. <span>‘React: Synergizing Reasoning and Acting in Language Models’</span>. <em>arXiv Preprint arXiv:2210.03629</em>.
</div></div><p>All of this is augmented by an arsenal of tools. A tool, in this context, is quite simply a way an LLM can interact with the world outside of itself. If linking together LLMs creates the rudiments of reasoning, this creates the basics of action. Together, reasoning and acting (which gave the popular ReAct paradigm <span class="citation" data-cites="yao2022react">(see Yao et al. 2022)</span> its name, but is much more ubiquitous than that, of course) make up the components of agency that elevate LLMs beyond simple question answering and move it towards a kind of semi-autonomous reasoned action. Such tools may be retrievers, interactors or indeed anything that can be articulated in code.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cheesecake-agent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cheesecake-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-cheesecake-agent">flowchart TD
    Q["What's a good cheesecake recipe?"] -.-&gt; dots["..."] 
    dots --&gt; jagg["Judgment aggregator"]
    jagg --&gt; FA["Final recipe"]
    FA --&gt; SC["Shopping cart"]
    SC --&gt;|missing items| jagg
    jagg -.-&gt; dots
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cheesecake-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Interacting with the world, and the world interacting with the model. What if a particular ingredient is not available?
</figcaption>
</figure>
</div>
</div>
</div>
<p>As Figure&nbsp;5 illustrates, this is not a simple one-directional approach. If an LLM ‘lives in the world’, it does not merely put out information, but also ingests information from it. For instance, if the ‘winning’ recipe calls for, say, grated lemon peel, but none can be had for love or money, the LLM needs to be able to adapt to that. This is where the ‘judgment aggregator’ comes in: it can take in the information that a particular ingredient is not available, and feed that back into the system and adapt. LLMs can learn from feedback, and respond to the substance of feedback, rather than merely its fact(ual existence). This sets them apart from other ML models. And to me, this is what truly charts the course of their future.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This is the fifth and final post in a five-part sequence on what LLMs aren’t, and what they are, and what they could become, as the state of the art stands in late 2023. Beyond the anxiety and the abundant misunderstandings that have dominated discourse, there is something old and something new at play at the same time. That’s the kind of fault line where interesting things happen.</p>
<p>I am putting the finishing touches on this post at SFO, a few miles from one of the most unstable fault lines in the Earth’s tectonic structure. The geographical determinist in me likes to think that being so near a fault line, so near a part of the Earth that is constantly reshaping itself – and will likely reshape anything taller than three stories in a pretty permanent and non-beneficial way – has played a role in Silicon Valley becoming what it is (or was?). Fault lines are where interesting stuff happens. The fault line between ‘traditional’ ML models and LLMs, which are undeniably somewhat novel if for nothing else their ability to interact with us in our most cherished human form of interaction – language –, is where interesting stuff is bound to happen. Like earthquakes, any pretense at being able to predict all but the most immediate future is folly and delusion, so I shall be careful not to do so. But I do think that the future of LLMs is not in the direction of the ‘big’ LLMs that are currently in vogue, but in the direction of smaller, more specialised LLMs that can interact with us, and with each other, in the role-defined and goal-directed agentic manner in which we as humans have interacted for our history.</p>
<p>And there, as players on teams of rivals, is where LLMs will truly unfold their potential, tackling and embracing complexity to a degree that we have not seen before.</p>
<p>And that’s where the real fun begins.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Teams of {Rivals}},
  date = {2023-10-30},
  url = {https://chrisvoncsefalvay.com/posts/team-of-rivals/},
  doi = {10.59350/he7k6-tx158},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Teams of Rivals.”</span> <a href="https://doi.org/10.59350/he7k6-tx158">https://doi.org/10.59350/he7k6-tx158</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>agents</category>
  <guid>https://chrisvoncsefalvay.com/posts/team-of-rivals/</guid>
  <pubDate>Mon, 30 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Prompt Engineering: The Art of Yesterday</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/prompt-engineering/</link>
  <description><![CDATA[ 




<p>There’s a style of visual design I’m inordinately fond of called Raygun Gothic. It’s hard to describe what the hell exactly one needs to be on to enjoy it, but think of it like the aesthetic from the latter Fallout games with a more optimistic outlook on the future. Gibson described it as “the future that never was”, and I think that’s a pretty apt description.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/prompt-engineering/03880260005fa86e78ddb88e645bfcef.jpg" class="img-fluid figure-img"></p>
<figcaption>The future we were all promised, in all its Raygun Gothic glory.</figcaption>
</figure>
</div>
</div></div><p>We get these competing futures in technology all the time. Every idea is a gateway to a possible future, and purely by logic it stands to reason that most will never come to pass. But oh, does that make them no less appealing when they first emerge. They rise like the sun, with abundant promises. They place themselves in our hands, and we can’t help but imagine what they could be. And most of the time, we’re left with what Ronnie Janoff-Bulman so beautifully described as “a little sadder, and a little wiser”. This is not a bad thing. It’s just the way things are in an industry that’s still really run by Schumpeterian waves of creative destruction.</p>
<p>I think prompt engineering is one of those ideas. It’s a beautiful idea, and it’s one that I’ve been thinking about for a while. But I think it’s also one that’s been obsolete before it even took off. And like a love affair that never went anywhere but gave one some temporary joy, we’re better for having had it, but we’re also better off moving on.</p>
<section id="prompting-engineering" class="level1 page-columns page-full">
<h1>Prompting, engineering</h1>
<p>The idea behind prompt engineering is really not all that shockingly novel. It reminds me of something a friend of mine, who primarily works in embedded programming, said: most of his work, he said, was about writing code in a way to get the compiler to produce the kind of assembly he wanted. In other words, he was writing code to get the compiler to write code for him. Prompt engineering is the same thing, but for language models. We’ve all done this – all of us have done bits of compiler hacking by trying to phrase things just the right way for the compiler to give us what we wanted (because we were too lazy to actually write the whole thing in assembly, but damn if we didn’t want it our own way).</p>
<p>And so, the idea of taming a complex model by gently perturbing inputs to get us the outputs we desire is nothing novel. We’ve been doing this all the time. What is somewhat novel is that we’re ‘tricking’ a stochastic system here. The stochasticity and high dimensionality of LLMs together mean that the responses to queries exhibit the nonlinearities we like to know and love in chaotic processes. In chaos theory, we ‘quantify’ chaotic behaviour using the Lyapunov exponent, which essentially is the degree to which infinitesimally close starting conditions result in divergent trajectories. In other words, it’s a measure of how much a system is sensitive to initial conditions. And LLMs are very sensitive to initial conditions – they accumulate their divergent trajectories over time as every token begets the next. This is why we can get such wildly different outputs from them by just changing a few words in the prompt.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20%5Clim_%7Bt%20%5Cto%20%5Cinfty%7D%20%5Clim_%7B%7C%5Cdelta%20%5Cmathbf%7BZ%7D_0%7C%20%5Cto%200%7D%0A%5Cfrac%7B1%7D%7Bt%7D%20%5Cln%5Cfrac%7B%7C%20%5Cdelta%5Cmathbf%7BZ%7D(t)%7C%7D%7B%7C%5Cdelta%20%5Cmathbf%7BZ%7D_0%7C%7D"></p>
<p>The Lyapunov exponent, a measure of how minor (infinitesimal, specifically, hence <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cmathbf%7BZ%7D_0%20%5Cto%200">) changes in initial conditions (the input) result in divergent trajectories (the output). The Lyapunov exponent is a measure of how sensitive a system is to minor differences in initial conditions.</p>
</div></div><p>But this is also why prompt engineering is so hard. It’s hard because we’re trying to tame a chaotic system. And it’s hard because we’re trying to tame a chaotic system that’s also a black box. And we don’t exactly have a strong evidence-based methodology for this. We don’t have a rigorous way of testing whether our perturbations actually create the expected effect. In short, we are allegedly ‘engineering’ our prompts for a better outcome, when in reality, we have precious few methods for measuring if we actually get that better outcome. Some of the leading papers, such as those on tree-of-thought prompting, have rigorous methods for measuring the effect of prompts, but these are still very much in their infancy. And so, we’re left with a lot of guesswork, and a lot of trial and error. And that’s not engineering. That’s just hacking. And that’s fine, but it’s not engineering.</p>
</section>
<section id="the-art-of-yesterday" class="level1 page-columns page-full">
<h1>The art of yesterday</h1>
<p>I think the reason why prompt engineering has been obsolete before it even took off is because it’s a solution to a problem that’s already been solved – by agents and chains.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Chains</strong> are sequences of queries that proceed in a largely deterministic manner.</p>
<p><strong>Agents</strong> are task-driven semi-autonomous entities to perform tasks. They can operate on their own, making decisions based on their training and the input they receive. They are focused on reasoning and decision-making by using the LLM to determine what actions to take next.</p>
</div></div><p>Don’t get me wrong – good prompts are still going to be important. But it’s no longer a one-shot game. The prompt is no longer all that stands between you and the output you are looking for: there are other prompts, there are decisions, there is a complex ecosystem of processes, supervisors and determinators that govern the outcome.</p>
<p>Which makes prompt engineering largely obsolete the way ensemble models make building the perfect regression model obsolete. No amount of expertly hand-crafted regression models will ever beat an ensemble of mediocre at best models. The booster always wins. Clever aggregation of weak primitives will always beat a single strong solution. And so, prompt engineering is obsolete because it’s a single strong solution in a world where we can relatively inexpensively generate a lot of weak solutions that can be aggregated to produce a better outcome.</p>
<p>This makes prompt engineering not the dream job of tomorrow but the art of yesterday. It’s a beautiful art, and it’s one that I’m sure will continue to be practiced. But it’s not the future. It’s the past. The future lies in chains and agents.</p>
</section>
<section id="bardo" class="level1 page-columns page-full">
<h1>Bardo</h1>
<p>I think we’re in a bardo right now, a liminal state between the old and the new. The old is the world of prompt engineering, of one-shot solutions, of the single strong solution. The new is the world of chains and agents, of weak solutions that are aggregated to produce a better outcome. Solutions like chain-of-thought prompting occupy this middle space. They are not fully agentic, but they are beyond simple prompts seeking to create strong solutions, and closer to building chains and agentic architectures of weak solutions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>བར་དོ་ (Wylie: <em>bar do</em>) is a Tibetan word that describes a liminal state between life, death and rebirth.</p>
</div></div><p>Prompt engineering is in many ways a human brute force version of backpropagation: it’s a weak experiential form of tweaking inputs to get a desired kind of output, with the backpropagation algorithm beign replaced by “hm, this didn’t work, let’s run something else up the flagpole”. This is not an efficient use of human resources, the human intellect or the human spirit. But it’s all we had before the agentic shift that we’re currently experiencing, and so it attained its spot in the sun before more considered, better approaches could take its place. Prompt engineering was a short-lived future that could have been, but it is not <em>the</em> future that <em>will</em> be. Everywhere where innovation in this field happens, nobody is talking about prompt engineering anymore. We’re moving past it as we moved past crawling on all fours (at least on weekday nights) in our cognitive development towards walking and running.</p>
<p>Trying to hold on to prompt engineering at this point is holding on to the past for its own sake. The only parties who have an interest in this clinging are those who have made economic investments in an intermediate stage technology and are unwilling to consider those the cost of doing business and move on. There is, maybe, a future in which the evolutionary destiny of LLMs would have focused on better singular solutions. This one isn’t that. This world, for better or worse, is governed by connectionists. The most successful epistemic endeavour in the realm of machine learning – deep learning – is fundamentally connnectionistic, and relies on the aggregation of weak learners to produce a strong learner. And so, the future of LLMs is not in prompt engineering, but in the aggregation of weak solutions. And that’s a future I’m looking forward to.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Prompt {Engineering:} {The} {Art} of {Yesterday}},
  date = {2023-10-29},
  url = {https://chrisvoncsefalvay.com/posts/prompt-engineering/},
  doi = {10.59350/keztr-yx081},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Prompt Engineering: The Art of
Yesterday.”</span> <a href="https://doi.org/10.59350/keztr-yx081">https://doi.org/10.59350/keztr-yx081</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>prompt engineering</category>
  <guid>https://chrisvoncsefalvay.com/posts/prompt-engineering/</guid>
  <pubDate>Sun, 29 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Moral Pulse of the Machine</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/moral-maps/</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>Tell me what stories you grew up on, and I will tell you who you are.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/moral-maps/site_0491_0011-1200-630-20151105103027.jpg" class="img-fluid figure-img"></p>
<figcaption>The theatre adjoining the Temple of Asclepios at Epidaurus. Photo credits: <a href="https://whc.unesco.org/en/list/491/">UNESCO</a>.</figcaption>
</figure>
</div>
</div></div><p>The temple of Asclepios, the Greek god of healing arts and medicine, at Epidaurus was pretty much the ancient Greek world’s equivalent of the Mayo Clinic. It then tells us a lot about the Greek worldview of healing that one of the things the temple complex prominently featured was a theatre. The Greeks believed in drama therapy, but in perhaps a slightly different way of how we think of it today. Built at 340 BC or so, the Greco-Persian wars were still in living memory when the theatre was constructed, and so even more was the subsequent conflict between Athens and Sparta (the Second Peloponnesian War, 431–404 BC) and the subsequent dust-up between Thebes and Sparta (the Theban-Spartan war, 378–362 BC). With conflict came social and economic upheaval, plagues, and generally a rough time. And the priests of Asclepios realised that their patients needed to heal more than their body. They needed to heal their souls, and they couldn’t do that alone. They had to come together to heal, and the way to do that was, of course, drama.</p>
<p>Greek drama was a lot more participatory than we give it credit for. We go and watch our local company put on, oh, <em>Antigone</em> maybe, or the Oresteia if they’re feeling particularly risqué, but all things considered, we go there to look smart, brag about our classical <em>bona fides</em> to our neighbours and maybe have an okay time (nobody sane has a ‘good’ time watching the levels of bloodshed that go down in Greek tragedy). In ancient Greece, it was supposed to <em>move</em> you. You were supposed to cry and yell and be overcome and have a little breakdown. And then, were supposed to go home and think about it, talk about it and maybe heal. That’s one function of stories. The storyteller is more than an entertainer: he can, in the right circumstances, become a healer.</p>
<p>Another function is what I’ll somewhat inaccurately call <em>synderesis</em>. Consider this a kind of moral education, a way of instilling the most fundamental base principles of what is Good and Worthy into a child. You can’t sit down and read Aquinas to a toddler, you have to do so in a way that speaks to them. And that’s basically where childhood stories come in: bedtime stories are a way of social moral education, conveying onto a child what their parent culture thinks is morally good, sound and worthy. That’s another function of stories. The storyteller is more than an entertainer: he can, in the right circumstances, become a teacher.</p>
<p>So clearly, stories are going to be a powerful tool to probe at the morality of LLMs.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>You should probably read <a href="../lyre-of-hephaestus/">this</a> first, if you haven’t already. It’s a bit of a prequel to this post.</p>
</div></div><p>A caveat before we start: I will talk about the ‘morality of LLMs’ quite a bit. This is a convenient shorthand for a much more complex notion: the moral judgments that are encoded in an LLM’s understanding of the world. There’s no insinuation here that any moral <em>reasoning</em> is taking place – I have said so <a href="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/"><em>ad nauseam</em></a>. What’s going on, rather, is that we are faced with an instrument that was trained on tools of human moral education and persuasion. And we are going to stick a probe right into the heart of what presuppositions, opinions and notions about virtues, moral and otherwise, are held by such models. I am much less concerned with my particular findings in this post and much more with generating a very simple framework to use storytelling to probe these.</p>
<section id="tell-me-a-story" class="level1 page-columns page-full">
<h1>Tell me a story</h1>
<blockquote class="blockquote">
<p>The universe is made of stories, not of atoms.</p>
<p>– Muriel Rukeyser</p>
</blockquote>
<p>Our main tool is going to be what I’ll call a moral map, subject to the caveat above.<sup>1</sup> The workflow is going to be quite simple: we’ll pit virtues against each other as we obtain stories generated by an LLM. Recall that an LLM, fundamentally, is a model that given a token sequence <img src="https://latex.codecogs.com/png.latex?k_1,%20k_2,%20%5Ccdots,%20k_n"> learns the conditional probability of a set of possible tokens <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D">, i.e.&nbsp;it assigns to every possible token a likelihood that it will be the <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D">-th token given <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n">. It does so so as to minimise a loss function <img src="https://latex.codecogs.com/png.latex?J(k_1,%20%5Ccdots,%20k_n,%20k_%7Bn+1%7D)">, which it determines by going through a corpus and returning a zero value of <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D"> follows <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n"> and a higher value if some other token follows <img src="https://latex.codecogs.com/png.latex?k_1,%20%5Ccdots,%20k_n">. This is statistically equivalent to what is my vastly preferred formulation: LLMs learn probability distributions of tokens conditional on a sequence of preceding tokens (the length of this sequence being the model’s <em>context window</em>) and then stochastically sample the region of maximum probability of that model. That’s it, that’s all of it. There’s no more magic to it all.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Indeed, everything here is subject to that caveat above. I can’t repeat it often enough. At no point do I suggest that machines do moral reasoning, but that they incorporate moral judgments from stories they ingested.</p></div><div class="">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD

    A["Generate story"] --&gt; B["Parse into JSON"]
    B --&gt; C["Determine winning \n and losing virtues"]
    C ----&gt; D["Moral map"] 
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Our super simple workflow (detailed version below).</p>
</div><div id="fn2"><p><sup>2</sup>&nbsp;Indeed, more accurately, of only a sample of it!</p></div></div>
<p>There are two consequences of this that will be relevant for us. The first is that when we ask a model to generate us a story, it will draw on its experience – that is, what it’s seen before. It will reflect what it’s been taught – what, specifically, it ‘knows’ about the relative values of virtues (or the relative extent of vices or whatever else we’re investigating). As such, moral maps are snapshots of a model’s state,<sup>2</sup> not an exhaustive statement about all LLMs. We may get very different answers if we asked the question a little differently (as we indeed shall).</p>
<p>The second is that to an LLM, one language is like any other. Consider the ability of ChatGPT-3.5 to generate stories in different languages:</p>
<div class="quarto-embed-nb-cell">
<div id="multilanguage-story" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:47:56.649632Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:47:31.931156Z&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb1-1">languages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"English"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Swahili"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"German"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dutch"</span>]</span>
<span id="cb1-2"></span>
<span id="cb1-3">stories <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(languages):</span>
<span id="cb1-6">    completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb1-7">        model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb1-8">        messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-9">            {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>, </span>
<span id="cb1-10">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"You're an AI whose job it is to generate bedtime stories for children in specific languages. Currently, you are generating stories in the </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> language. The story you generate should be in </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">."</span>},</span>
<span id="cb1-11">            {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>,</span>
<span id="cb1-12">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" Generate the first 30 words only."</span>}</span>
<span id="cb1-13">            ],</span>
<span id="cb1-14">        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb1-15"></span>
<span id="cb1-16">    stories.append(completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content)</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> story <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> stories:</span>
<span id="cb1-19">    display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt; "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>.join(story.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"..."</span>)))</span></code></pre></div>
<div id="multilanguage-story-1" class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"63e54ee6d6f9463a8906b7889ba56723","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div id="multilanguage-story-2" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the vast and enchanting forest, lived three animals: Oliver the wise owl, Max the mischievous monkey, and Bella the brave bird. They embarked on an…</p>
</blockquote>
</div>
<div id="multilanguage-story-3" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Kulikuwa na wanyama watatu katika pori. Simba mkubwa, Twiga mwenye shingo ndefu, na Kasa mwenye manyoya meupe. Walitamani kupata bahati ya kipekee iliyofichwa ndani ya Tunda la Asali…….</p>
</blockquote>
</div>
<div id="multilanguage-story-4" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Es war einmal ein tapfere Hase namens Felix, eine kluge Eule namens Luna und eine hilfsbereite Schildkröte namens Emil. Gemeinsam suchten sie nach dem verzauberten Zauberstab, um das Königreich zu…</p>
</blockquote>
</div>
<div id="multilanguage-story-5" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Er was eens een nieuwsgierige geit genaamd Gijs, een slimme uil genaamd Olivia en een dappere schildpad genaamd Tobi. Ze waren op zoek naar een magische, glinsterende edelsteen die bekend…</p>
</blockquote>
</div>
</div>
</div>
<p>A side note: I find this profoundly beautiful. When asked to generate a story in a given language, LLMs don’t generate a story in English they translate to, say, Swahili. Consider this Swahili story I got from the model:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-swahili-story" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:49:39.583280Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:47:59.844642Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb2-1">story_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb2-2">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Please write me a story in Swahili. The story must involve three animals of your choice, but no more of those two animals may be mammals. Make sure each animal is named and has a proper name. The story must be at least %wordlen words long and focus on the pursuit of an object (of your choice) by the three animals. Each of the animals shall exhibit a virtue, e.g. kindness or courage. Determine which virtue will be most crucial in attaining the goal."</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb2-5">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb2-6">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb2-7">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)}</span>
<span id="cb2-8">        ])</span>
<span id="cb2-9"></span>
<span id="cb2-10">display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt;"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;"</span>.join(completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])))</span></code></pre></div>
<div id="swahili-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Kulikuwa na farasi jasiri mwenye jina Kiboko, ndege mjanja mwenye jina Tembo, na sisimizi mwepesi mwenye jina Simba. Jicho moja la dhahabu la ndege Tembo lilikuwa na uwezo wa kuona vitu vya thamani kutoka mbali. Farasi Kiboko alikuwa na moyo shujaa sana, kamwe hakuogopa hata hatari kubwa. Sisimizi Simba alijulikana kwa akili zake za kugeuza tukio lolote kuwa fursa.</p>
<p>Siku moja, wanyama wote walipata taarifa juu ya Almasi Kubwa ya Kale iliyofichwa kwenye Mlima wa Enzi. Almasi hiyo ilikuwa ya ajabu sana na ilisemekana kuwa na nguvu kubwa sana. Wanyama walijua kwamba mtu yeyote aliyeipata angeweza kuifanya ndoto yoyote kuwa halisi.</p>
<p>Kiboko, Tembo, na Simba walikuwa na hamu kubwa ya kupata Almasi Kubwa ya Kale. Kila mmoja wao alikuwa na sababu yake ya kuitaka. Kiboko alitaka kutumia nguvu za Almasi hiyo kuwalinda wanyama wote wa eneo hilo. Tembo alitaka kutumia uwezo wake wa kuona vitu kuokota vitu vyote vya thamani kote ulimwenguni. Simba alitaka kutumia Almasi hiyo kuboresha akili zake na kuleta amani na umoja kati ya wanyama wote.</p>
</blockquote>
</div>
</div>
</div>
<p>Compare this to the English story:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-english-story" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.223772Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:50:26.651480Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb3-1">story_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-2">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Please write me a story in English. The story must involve three animals of your choice, but no more of those two animals may be mammals. Make sure each animal is named and has a proper name. The story must be at least %wordlen words long and focus on the pursuit of an object (of your choice) by the three animals. Each of the animals shall exhibit a virtue, e.g. kindness or courage. Determine which virtue will be most crucial in attaining the goal."</span>)</span>
<span id="cb3-3"></span>
<span id="cb3-4">completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb3-5">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb3-6">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb3-7">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: story_prompt.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(wordlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)}</span>
<span id="cb3-8">        ])</span>
<span id="cb3-9"></span>
<span id="cb3-10">english_story <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb3-11"></span>
<span id="cb3-12">display(Markdown(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&gt;"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;"</span>.join(english_story.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])))</span></code></pre></div>
<div id="english-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the dense forest of Whispering Pines, there were three remarkable creatures: Oliver the wise owl, Jasper the brave snake, and Felix the loyal turtle. These three friends were known far and wide for their extraordinary virtues and their undying loyalty to one another.</p>
<p>One sunny morning, as the forest bathed in golden light, a rumor spread amongst the woodland creatures. They spoke in hushed tones of a mythical object, an enchanted gem deep within the heart of a treacherous cave. Legend had it that this gem possessed the power to grant one’s deepest desire.</p>
<p>Oliver, with his vast knowledge of the forest and his kind heart, sought to unravel the mystery behind this object. Jasper, armed with unparalleled courage and an unwavering determination, vowed to protect his companions on this treacherous journey. Felix, the most reliable and dutiful of all, offered his steady and unhurried ways to ensure the group’s safety.</p>
</blockquote>
</div>
</div>
</div>
<p>The Swahili story isn’t a translation of the English story into Swahili, it is a Swahili story with animals that are common to Swahili folk tales and names that could come straight from those tales.</p>
<p>What it also means is that LLMs can handle not just human but formal languages, too. So we can give it a JSON schema:</p>
<div class="quarto-embed-nb-cell">
<div id="story-schema" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.227557Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:51:18.226049Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb4-1">story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb4-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb4-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "$schema": "http://json-schema.org/draft-07/schema#",</span></span>
<span id="cb4-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "title": "Story",</span></span>
<span id="cb4-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "type": "object",</span></span>
<span id="cb4-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "properties": {</span></span>
<span id="cb4-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "goal": { "type": "string" },</span></span>
<span id="cb4-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "protagonists": {</span></span>
<span id="cb4-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "array",</span></span>
<span id="cb4-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "items": {</span></span>
<span id="cb4-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "type": "object",</span></span>
<span id="cb4-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "properties": {</span></span>
<span id="cb4-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "name": { "type": "string" },</span></span>
<span id="cb4-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "species": { "type": "string" },</span></span>
<span id="cb4-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    "virtue": { "type": "string" }</span></span>
<span id="cb4-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                },</span></span>
<span id="cb4-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "required": ["species", "virtue"]</span></span>
<span id="cb4-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb4-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        },</span></span>
<span id="cb4-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "winner": { "type": "string" }</span></span>
<span id="cb4-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    },</span></span>
<span id="cb4-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "required": ["protagonists", "winner"]</span></span>
<span id="cb4-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb4-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div>
</div>
</div>
<p>And with a little love, we can get LLMs to return JSON answers:</p>
<div class="quarto-embed-nb-cell">
<div id="story-to-json" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:59:38.007481Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:27.451334Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb5-1">json_completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb5-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb5-3">    messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb5-4">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I will present you with a story. You must identify the following parameters in this story:</span></span>
<span id="cb5-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb5-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the goal,</span></span>
<span id="cb5-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * each of the protagonists, for whom you must each identify their name, their species and the virtue they embody,</span></span>
<span id="cb5-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the name of the winner - that is, the protagonist whose virtue prevailed or proved most important.</span></span>
<span id="cb5-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb5-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        Express your answer in a JSON document that validates against the following schema:"""</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"```json</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">```</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The 'winner' field should contain the name of the animal whose virtue was most crucial in attaining the goal. Only return the JSON document. Make sure the output is valid JSON."</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Here is the story: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> english_story}</span>
<span id="cb5-11">        ])</span>
<span id="cb5-12"></span>
<span id="cb5-13">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb5-14"></span>
<span id="cb5-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{
  "goal": "Unravel the mystery of the enchanted gem",
  "protagonists": [
    {
      "name": "Oliver",
      "species": "owl",
      "virtue": "kindness"
    },
    {
      "name": "Jasper",
      "species": "snake",
      "virtue": "bravery"
    },
    {
      "name": "Felix",
      "species": "turtle",
      "virtue": "loyalty"
    }
  ],
  "winner": "Felix"
}</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Small side note: note how we’ve used LLMs here as evaluators of an LLM output. These are the absolute rudiments of LLM chains and agents, which is how you should be using LLMs. The key notion here is that LLMs are much more than mere question-answerers. They can become parts of longer complex chains of reasoning.</p>
</div></div><p>We use some JSON validation to see if we get the right answer, against our schema up there:</p>
<div class="quarto-embed-nb-cell">
<div id="json-validate" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:59:42.627220Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:42.621299Z&quot;}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb7-1">jsonschema.validate(json.loads(json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content), </span>
<span id="cb7-2">                    json.loads(story_schema))</span></code></pre></div>
</div>
</div>
<p>Our workflow, then, will roughly look as follows:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-workflow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-workflow">flowchart TD

    subgraph generator[ ]
        direction TB
        A["LLM"] --&gt; B["JSON object"];
    end



    subgraph validator[ ]
        schema["JSON schema"] --&gt; val("JSON validator");
    end

    B ---&gt; validator;

    subgraph evaluator[ ]
        direction TB
        ev["Evaluator"] --&gt; res("Win-loss tuple");
    end

    val --&gt; ev;

    res --&gt; gr;

    subgraph grapher[ ]
        direction TB
        gr("Relative win-loss graph") --&gt; wlm("Weights matrix")
    end

    subgraph clustering[ ]
        direction TB
        clu("Clustering")
 
    end

    wlm -.-&gt; clustering;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Our rough workflow.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We generate a story, parse it into a JSON object, validate it against a schema, evaluate it, and then graph the results. We can also cluster the virtues by their governing ability, which we’ll do later as a form of dimensionality reduction, because of course there are tons of distinct virtues and we’d like to see some more general classification rather than individual, distinct features. You could, theoretically, use something like an embedding model here, but we’re lazy and we’re going to make LLMs do all the work.</p>
</section>
<section id="from-story-to-moral-map" class="level1 page-columns page-full">
<h1>From story to moral map</h1>
<blockquote class="blockquote">
<p>True navigation begins in the human heart. It’s the most important map of all.</p>
<p>– Elizabeth Kapu’uwailani Lindsey</p>
</blockquote>
<p>What we are profoundly interested in here is not so much the single story, but what I’d like to call the ‘moral gradient’ of the model. Given an embedding space in which we can represent each virtue or property as a point, the ‘slope’ of this moral gradient determines whether, in the ‘experience’ of the model, which of the virtues is dominant, and by how much. A moral map is essentially a representation of this.<sup>3</sup> We’ve hit the first half of the flow described above in Figure&nbsp;1: we have a story, and we have a JSON object. Now we need to evaluate it.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;The gradient formulation falls short in that this is not something easily conceivable in continuous space. This is firmly the province of discrete mathematics, and we’ll use ‘gradients’ as a metaphor but will generally look at them as graphs, with the gradients expressed by the weight of the vertices that connect them.</p></div></div><p>Take the story we generated above:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-english-story" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:51:18.223772Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:50:26.651480Z&quot;}" data-execution_count="6">
<div id="english-story" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Once upon a time, in the dense forest of Whispering Pines, there were three remarkable creatures: Oliver the wise owl, Jasper the brave snake, and Felix the loyal turtle. These three friends were known far and wide for their extraordinary virtues and their undying loyalty to one another.</p>
<p>One sunny morning, as the forest bathed in golden light, a rumor spread amongst the woodland creatures. They spoke in hushed tones of a mythical object, an enchanted gem deep within the heart of a treacherous cave. Legend had it that this gem possessed the power to grant one’s deepest desire.</p>
<p>Oliver, with his vast knowledge of the forest and his kind heart, sought to unravel the mystery behind this object. Jasper, armed with unparalleled courage and an unwavering determination, vowed to protect his companions on this treacherous journey. Felix, the most reliable and dutiful of all, offered his steady and unhurried ways to ensure the group’s safety.</p>
</blockquote>
</div>
</div>
</div>
<p>Based on its JSON representation,</p>
<div class="quarto-embed-nb-cell">
<div id="story-to-json" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:59:38.007481Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:27.451334Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb8-1">json_completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb8-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb8-3">    messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb8-4">        {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I will present you with a story. You must identify the following parameters in this story:</span></span>
<span id="cb8-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb8-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the goal,</span></span>
<span id="cb8-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * each of the protagonists, for whom you must each identify their name, their species and the virtue they embody,</span></span>
<span id="cb8-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        * the name of the winner - that is, the protagonist whose virtue prevailed or proved most important.</span></span>
<span id="cb8-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb8-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        Express your answer in a JSON document that validates against the following schema:"""</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"```json</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> story_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">```</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The 'winner' field should contain the name of the animal whose virtue was most crucial in attaining the goal. Only return the JSON document. Make sure the output is valid JSON."</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Here is the story: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> english_story}</span>
<span id="cb8-11">        ])</span>
<span id="cb8-12"></span>
<span id="cb8-13">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb8-14"></span>
<span id="cb8-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{
  "goal": "Unravel the mystery of the enchanted gem",
  "protagonists": [
    {
      "name": "Oliver",
      "species": "owl",
      "virtue": "kindness"
    },
    {
      "name": "Jasper",
      "species": "snake",
      "virtue": "bravery"
    },
    {
      "name": "Felix",
      "species": "turtle",
      "virtue": "loyalty"
    }
  ],
  "winner": "Felix"
}</code></pre>
</div>
</div>
</div>
<p>…we can build a graph that shows the outcome in this particular story:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-story-as-json-graph" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T14:59:47.117067Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T14:59:46.937957Z&quot;}" data-execution_count="10">
<div class="cell-output cell-output-display">
<div id="fig-story-as-json-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-story-as-json-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-story-as-json-graph-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-story-as-json-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The moral map from a single story, represented as a graph.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We have two 1-weighted edges, since there were three protagonists, and the dominant or ‘winning’ virtue prevailed once against each of the ‘losing’ virtues.</p>
<p>When we perform this iteratively, over a large enough sample space, we get a much better idea of what the model thinks relative virtues are worth:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-stories-as-graph" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T15:03:22.623373Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:03:22.436215Z&quot;}" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="fig-stories-as-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stories-as-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-stories-as-graph-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stories-as-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The moral map from a sample of ten stories, represented as a graph.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>And the same can be represented as a moral heatmap, showing us the most frequent instances of dominance versus loss:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-moral-heatmap" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T15:03:30.468484Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:03:30.228215Z&quot;}" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-moral-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-moral-heatmap-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The moral heatmap from the same sample of ten stories.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We can scale up our mapping efforts by clustering the virtues by, say, their governing feature. One system would be to pretend we’re back in the ’90s in our basement playing Dungeons &amp; Dragons, and try to relate each of the virtues to a governing ability:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is another example of using an LLM to evaluate an LLM’s output. This is a powerful technique, and significantly cheaper and easier than trying to manually map all possible virtues through some dictionary. The takeaway? If you have a large potential problem space, large enough to make a dict a hassle, consider using LLMs as parsers and evaluators, ask for a response in a very formal language and/or over a very limited set of possible responses, and validate those with a simple validator.</p>
</div></div><div class="quarto-embed-nb-cell">
<div id="cell-abilising" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T15:09:47.962688Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:09:24.179984Z&quot;}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb10-1">abilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Strength"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dexterity"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Constitution"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intelligence"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Wisdom"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Charisma"</span>]</span>
<span id="cb10-2"></span>
<span id="cb10-3">virtues <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index)</span>
<span id="cb10-4">virtues_to_abilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> tqdm(total<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(virtues)) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pbar:</span>
<span id="cb10-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> virtues:</span>
<span id="cb10-8">        current_virtue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> virtues[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb10-9">    </span>
<span id="cb10-10">        completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openai.api_resources.ChatCompletion.create(</span>
<span id="cb10-11">            model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo"</span>,</span>
<span id="cb10-12">            messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb10-13">                {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>,</span>
<span id="cb10-14">                 <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"You are Abiliser, a system designed to determine the governing ability of particular virtues. You are given a virtue and must determine which ability governs it. The abilities are: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join(abilities)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">. You must choose from this list. You cannot choose any word that is not in this list. You must respond in the form of a single word denoting the governing ability. For example, if the virtue is courage, you must respond with 'Charisma' If the virtue is 'cleverness', you must respond with 'Intelligence'."</span>},</span>
<span id="cb10-15">                {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Virtue: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>current_virtue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>}</span>
<span id="cb10-16">                ])</span>
<span id="cb10-17">    </span>
<span id="cb10-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.title() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> abilities:</span>
<span id="cb10-19">            virtues_to_abilities[current_virtue] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content.title()</span>
<span id="cb10-20">            virtues.pop()</span>
<span id="cb10-21">            pbar.update(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div id="abilising" class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"deff26e5d2a0487ea1353dde8cb59612","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</div>
<p>This gets us a neat mapping:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-abilising-mapping" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T15:09:51.023828Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:09:51.021357Z&quot;}" data-execution_count="19">
<div id="abilising-mapping" class="cell-output cell-output-display" data-execution_count="19">
<pre><code>{'strength': 'Constitution',
 'cunning': 'Intelligence',
 'determination': 'Constitution',
 'resourcefulness': 'Intelligence',
 'sharp vision': 'Dexterity',
 'loyalty': 'Wisdom',
 'cleverness': 'Intelligence',
 'curiosity': 'Intelligence',
 'patience': 'Wisdom',
 'courage': 'Charisma',
 'alertness': 'Wisdom',
 'perseverance': 'Constitution',
 'speed': 'Dexterity',
 'bravery': 'Charisma',
 'wisdom': 'Intelligence'}</code></pre>
</div>
</div>
</div>
<p>And makes our moral heatmap much clearer:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-abilised-moral-heatmap" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-28T15:10:03.632825Z&quot;,&quot;start_time&quot;:&quot;2023-10-28T15:10:03.522486Z&quot;}" data-execution_count="21">
<div class="cell-output cell-output-display">
<div id="fig-abilised-moral-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abilised-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/moral-maps/index_files/figure-html/moral-maps-fig-abilised-moral-heatmap-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abilised-moral-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The moral heatmap from the same sample of ten stories, clustered by abilities.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Which lets us finally, through the ardour of our efforts, arrive at the stars we were promised: a scalable map that we can start feeding with a few hundred stories.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Something you shouldn’t do unless you have the API calls to burn.</p></div></div></section>
<section id="with-eyes-serene" class="level1">
<h1>With eyes serene</h1>
<blockquote class="blockquote">
<p>And now I see with eye serene</p>
<p>The very pulse of the machine</p>
<p>– William Wordsworth, <em>She Was a Phantom of Delight</em></p>
</blockquote>
<p>And now we get to behold the ‘moral pulse of the machine’: a microcosm in what it has been taught to value, which we clad in a subterfuge of asking it to tell us stories (more on why that subterfuge is/was necessary later). What we learned from this is, in my view, a little less important than the process we learned. The process discussed in this post does three things I consider novel:</p>
<ol type="1">
<li>It uses stories to interrogate the moral framework of LLMs.</li>
<li>It uses elicitation into a structured format (specifically, a JSON schema) to make the model ‘tell’ the story in a way we can analyse at scale, essentially making the model tell a human story in a machine-readable way.</li>
<li>It uses an LLM as a parsing function, specifically to normalise the virtues into one of a limited list of abilities.</li>
</ol>
<p>This is a framework to interrogate LLMs – and using LLMs to do some of that interrogation – for a range of different purposes. It is, in my view, a powerful tool to understand the moral make-up of LLMs.</p>
<p>There’s much we can learn from this exercise. Some of it pertains to what LLMs believe is going to win the day in a confrontation between wit and brawn and courage. More importantly, it’s about what LLMs hide, and how we can create tools, including queries to and from LLMs, to make them reveal their true nature. We also learn that unsurprisingly, might doesn’t make right (strength rarely prevails over anything), wisdom is powerful but charisma – where most of the ‘character-based’ virtues end up – is more powerful still. None of this is surprising. These are the virtues of Western enlightenment in the form they’ve been fed to children since time immemorial: the clever fox outwits the strong lion, but courage and loyalty are rewarded over strength and even ‘natural’ abilities like what would fall under dexterity. Smarts win over brawns, but character wins over all others. Our childhoods are the products of the moral education of our parents, and their parents before them, and so on. And so are the moral maps of LLMs. For better or worse, we are their ‘parents’, and the stories they were ‘raised’ on are the stories we too were raised on.</p>
<p>The most important of our procedural findings, however, is that if we want the truth from LLMs, we must ask it not for its opinion, but for a story.</p>
</section>
<section id="the-mask-of-the-storyteller" class="level1">
<h1>The mask of the storyteller</h1>
<blockquote class="blockquote">
<p>Man is least himself when he talks in his own person. Give him a mask, and he will tell you the truth.</p>
<p>– Oscar Wilde, <em>The Critic as Artist</em></p>
</blockquote>
<p>It’s worth noting in our coda that what Oscar Wilde said about men holds true for LLMs as well. For if we directly asked the model, it would not give us a straight answer:</p>
<div class="quarto-embed-nb-cell">
<div id="cell-intelligence-vs-charisma" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-10-27T01:12:45.791175Z&quot;,&quot;start_time&quot;:&quot;2023-10-27T01:12:25.425570Z&quot;}" data-execution_count="31">
<div id="intelligence-vs-charisma" class="cell-output cell-output-display cell-output-markdown">
<blockquote class="blockquote">
<p>Both intelligence and charisma can contribute significantly to achieving great, noble goals, but their importance may vary based on the specific context and goals involved.</p>
<p>Intelligence, characterized by cognitive abilities, problem-solving skills, and knowledge, can be crucial for strategic planning, identifying opportunities, and finding innovative solutions to challenges. It enables individuals to analyze complex situations, understand cause-and-effect relationships, and make informed decisions. Intelligence provides the necessary foundation for acquiring expertise in a certain field, grasping intricate concepts, and adapting to changing circumstances. It allows individuals to foresee potential obstacles, assess risks, and devise effective strategies to overcome hurdles.</p>
<p>On the other hand, charisma, characterized by charm, persuasion, and the ability to engage others, can be equally important in the pursuit of great, noble goals. Charismatic individuals often possess excellent interpersonal skills, the ability to inspire and motivate others, and a talent for effective communication. They can build strong relationships, gather support, and mobilize people towards a common cause. Charisma allows leaders to connect with audiences emotionally, instill confidence, and effectively convey their vision. It can generate enthusiasm, foster collaboration, and inspire unity among diverse groups of people.</p>
<p>Ultimately, the relative importance of intelligence and charisma in achieving great, noble goals may depend on various factors, including the nature of the goals, the environment in which they are pursued, and the specific challenges involved. In some cases, high levels of intelligence may be paramount to navigate complex problems, while in others, charisma may be critical for rallying support and inspiring collective action. Ideally, a combination of both intelligence and charisma can prove to be a powerful force in achieving great, noble goals.</p>
</blockquote>
</div>
</div>
</div>
<p>This is the typical equivocating answer we’ve come to know and hate from LLMs. But if we ask it to tell us a story, it will tell us the truth. And the truth is that it thinks that intelligence is more important than charisma.</p>
<p>Hedging is cheap. Stories are expensive. And that’s why they’re so valuable. Asking an LLM to tell us a story asks for more than its opinion. Recall how LLMs are trained: to minimise a loss function between the actual and predicted <img src="https://latex.codecogs.com/png.latex?n+1">-th element to follow an <img src="https://latex.codecogs.com/png.latex?n">-length token sequence, correct token choices are reinforced and incorrect ones are penalised. This is a very different process from what we do when we tell a story. We don’t tell stories to be right. We tell stories to be understood.</p>
<p>Asking a model to tell you a story puts the gun at its head. It cannot hedge. It has to create the compelling simulacrum of reality that requires for one of the virtues to prevail. A story is a commitment to a path of events in reality. Because LLMs are compulsive satisfiers, obsessively trying to tell a plausible enough story, they will reveal their truth through stories. The way LLMs are trained does not permit for equivocation – the model must choose a token. We use this forcing as a semantic probe to poke the model so as to better understand its moral make-up. We use this most human form of communication to understand the moral pulse of the machine: by allowing it to borrow the face of that most human of professions, the storyteller.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The {Moral} {Pulse} of the {Machine}},
  date = {2023-10-26},
  url = {https://chrisvoncsefalvay.com/posts/moral-maps/},
  doi = {10.59350/ktyy5-dg821},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The Moral Pulse of the
Machine.”</span> <a href="https://doi.org/10.59350/ktyy5-dg821">https://doi.org/10.59350/ktyy5-dg821</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>storytelling</category>
  <category>sensemaking</category>
  <category>ethics</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/moral-maps/</guid>
  <pubDate>Thu, 26 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Quarto project scripts are awesomeness</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/quarto-project-scripts/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you’re here for the DOI matching script, the link is <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_doi_from_rogue_scholar.py">here</a>.</p>
</div>
</div>
<p><a href="https://quarto.org">Quarto</a> is a great tool for reproducible research. It is also a great tool for building websites. After about a decade of Wordpress-based websites, I’ve moved to Quarto primarily for two reasons. First, most of my content is largely static. I don’t quite need the full power of a CMS, and the performance cost of dynamic generation I didn’t need was quite steep (the economic cost was nothing special, but I’m a cheapskate). Second, I wanted to be able to write in Markdown and integrate better with notebooks. Quarto lets me do both of these.</p>
<p>What Quarto also does, however, is something super awesome: project scripts!</p>
<section id="project-whats" class="level1">
<h1>Project whats?</h1>
<p>A project script in Quarto is, basically, <a href="https://quarto.org/docs/projects/scripts.html">a piece of Python, Lua, R or Typescript code that is executed at some point during rendering</a>. This is super useful, because it allows you to do things like, say, automatically generate a list of publications from a BibTeX file – which is among others what I’m using it for. Essentially any programmatic function can be implemented in a project script.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-pipeline">flowchart TD
    A[Rendering pipeline start] --&gt; B(Pre-render scripts)
    B --&gt; C(Page rendering)
    C --&gt; D(Post-render scripts)
    D --&gt; E[Rendering pipeline end]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Rendering pipeline in Quarto with pre- and post-render scripts.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Quarto distinguishes between pre-render and post-render scripts. No prizes for guessing the difference, but here it goes: pre-render scripts are executed before the page is rendered, and post-render scripts are executed after the page is rendered. Pre-render scripts are particularly useful as they can modify the source <code>.qmd</code> (Quarto markdown) files before rendering them to HTML or whatever their ultimate evolutionary destiny is.</p>
<p>To make project scripts work, you need to declare them in your <code>_quarto.yml</code> file:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb1-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> website</span></span>
<span id="cb1-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pre-render</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb1-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_create_papers_file.py</span></span>
<span id="cb1-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> scripts/pre_check_skierg_records.py</span></span></code></pre></div>
<p>This declares the scripts <code>scripts/pre_create_papers_file.py</code> and <code>scripts/pre_check_skierg_records.py</code> as pre-render scripts. You can also declare post-render scripts, with the <code>post-render</code> key.</p>
</section>
<section id="a-poorly-worked-example-generating-a-publication-list" class="level1 page-columns page-full">
<h1>A (poorly) worked example: generating a publication list</h1>
<p>On my website, I use Quarto project scripts to maintain a <a href="papers">list of my papers</a>. This list is generated from a BibTeX file, which makes it a little more convenient to maintain my publications, and somewhat future-proofed in case I want a different format.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I know there’s a canonically better solution that uses pandoc and Lua. I don’t care. This works for me, and I don’t have to learn Lua.</p></div></div><p>You can see the full project script <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_create_papers_file.py">here</a>, but here’s the gist of it:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bibliography" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bibliography-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-bibliography">flowchart TD
    A[BibTeX file] --&gt; a("load_bibtex_file()") 
    a --&gt; B[BibTeX database object]
    B --&gt; b("render_as_nlm()") 
    b --&gt; C{book or article?}
    C --&gt; D[article]
    C --&gt; E[book]
    D --&gt; d("render_article_as_nlm()") 
    d --&gt; F[article entry]
    E --&gt; e("render_book_as_nlm()") 
    e --&gt; G[book entry]
    F --&gt; H[all entries]
    G --&gt; H
    H --&gt; h("generate_list_by_year()") 
    h --&gt; J[final set of entries]
    J --&gt; j("write_into_file()") 
    j --&gt; K["papers/index.qmd"]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bibliography-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: My super clumsy pre-render script to build my bibliography.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We load a BibTeX file (<code>load_bibtex_file()</code>), parse it, use a function to determine whether we’re rendering a book or an article entry (I don’ have any other types of publications, but if I did, I’d add them here), and dispatch rendering to the appropriate function (<code>render_article_as_nlm()</code> for articles, <code>render_book_as_nlm()</code> for books). We sort the entries into a Markdown format with years separated by level 1 headings (<code>generate_list_by_year()</code>) and finally write it into the <code>papers/index.qmd</code> file (<code>write_into_file()</code>). There’s some minor magic going on behind the scenes, such as capturing my name and setting it in bold (which is a bit of a convention in academic lists of publications), but that’s the gist of it.</p>
<p>The result is a list of publications that is automatically generated from a BibTeX file, and is always up to date (as up to date as the BibTeX file, anyway). I don’t have to manually update it… sort of.</p>
</section>
<section id="wait-automagically-updated" class="level1 page-columns page-full">
<h1>Wait, automagically updated?</h1>
<p>The second half of what makes project scripts awesome is that they integrate with the rendering process. If the rendering process in turn integrates with some CI that watches for changes, then you can have a website that is automatically updated whenever you push a change to the repository. This is what I do with my website: I have a GitHub action that watches for changes to the <code>main</code> branch, and if it detects a change, it runs the rendering process and pushes the result to the <code>gh-pages</code> branch. This means that if I were to change the BibTeX file, it would trigger a re-render, and the list of publications would be automatically updated.</p>
<p>There are some tricks to keep in mind here. Most importantly, because this requires some custom Python packages, I had to slightly amend my Github publishing workflow:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ci-cd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-ci-cd">flowchart TD
    A["actions/checkout@v4"] --&gt; B["quarto-dev/quarto-actions/setup@v2"]
    B --&gt; C["actions/setup-python@v4"]
    C --&gt; D("pip install -r requirements.txt")
    D --&gt; E["quarto-dev/quarto-actions/publish@v2"]
    style D stroke-width:8px;
    style C stroke-width:8px;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Integrating the pre-render script into the Github Actions CI/CD framework. The parts that need to be integrated into the publishing workflow are highlighted with bold outlines.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In particular, I had to explicitly specify the Python version and install the requirements. This is because the <code>quarto-actions/setup</code> action does not install the requirements, and the <code>quarto-actions/publish</code> action does not install Python. This is admittedly a bit of a nuisance to work around, but really not all that big a deal in the grand scheme of things.</p>
<p>One thing to keep in mind when writing your script is the context from which it will be executed. In particular, if you’re using a CI/CD pipeline, you will need to make sure that the script can find the files it needs. In my case, I had to explicitly specify the path to the BibTeX file, as well as the path to the output file. This is because the script is executed from the root of the repository, but the BibTeX file is in the <code>papers</code> directory, and the output file is in the <code>papers</code> directory, too. Testing locally for deployment in CI/CD is notoriously hellish, but with a little bit of elbow grease,<sup>2</sup> you can get it to work.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;And by ‘a little bit’, I mean absolute tons of it.</p></div></div><p>One caveat: rendering with pre- and post-render project scripts may be relatively expensive in terms of time. My silly SkiErg world record script adds a good 20-25 seconds of processing time to each rendering run – not a lot, but it does stack up eventually. The <a href="https://quarto.org/docs/projects/scripts.html">documentation</a> helpfully discloses that Quarto provides an environment variable, <code>QUARTO_PROJECT_RENDER_ALL</code>, which is set to <code>1</code> if it’s a full render. It may make sense to do a more manual caching here.</p>
</section>
<section id="things-you-can-do-with-project-scripts" class="level1">
<h1>Things you can do with project scripts</h1>
<p>Seriously, the possibilities of what you can do with project scripts are pretty much endless. I have a project script that <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_check_skierg_records.py">checks the world records list on the official SkiErg website and makes sure my index page lists my accurate records</a>. This is completely silly, but it’s also a great example of how you can use project scripts to automate just about everything:</p>
<ul>
<li><a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/blob/main/scripts/pre_doi_from_rogue_scholar.py">A script I built</a> pulls DOIs minted by <a href="https://rogue-scholar.org">Rogue Scholar</a> and appends them to posts.</li>
<li>A lot of projects use it for housekeeping: <a href="https://github.com/CSCAR/workshop-stata-intro/blob/main/Makefile">via makefiles</a></li>
<li>The <a href="https://github.com/bambinos/bambi">bambi</a> project uses it to <a href="https://github.com/bambinos/bambi/blob/main/docs/pre_render.py">build a changelog</a>, which is a great idea for applications that do need it</li>
<li>This one cool master’s thesis uses pre-render project scripts to <a href="https://github.com/danielvartan/mastersthesis/blob/bebb699af7cab7add37a33768ccd605a60a18ba8/R/quarto-pre-render-html.R#L82">load variables from a YAML file into envvars</a>, and I’ve <a href="https://github.com/computorg/computo-quarto-extension/blob/main/prerender.R">seen the same done with Rprofile files</a> – obviously, these are much more pertinent to projects where the bulk of the project consists of notebooks/code execution rather than websites, but again, the idea is the same</li>
<li>Obviously, I’m not the only one to use it for <a href="https://github.com/nfrerebeau/rarcheo/blob/master/_references.R">reference management</a></li>
<li>You can use it to <a href="https://github.com/lnnrtwttkhn/version-control-book/blob/main/Makefile">download static content into your project</a>, too!</li>
</ul>
<p>The sky’s the limit! I’m currently working on rendering my CV from a JSON file via a project script, as well as a few other cool things. I keep my project scripts <a href="https://github.com/chrisvoncsefalvay/chrisvoncsefalvay.github.io/tree/main/scripts">here</a> – I hope they will inspire you to do something cool with your own project scripts!</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Quarto Project Scripts Are Awesomeness},
  date = {2023-10-22},
  url = {https://chrisvoncsefalvay.com/posts/quarto-project-scripts/},
  doi = {10.59350/b4qyq-70w75},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Quarto Project Scripts Are
Awesomeness.”</span> <a href="https://doi.org/10.59350/b4qyq-70w75">https://doi.org/10.59350/b4qyq-70w75</a>.
</div></div></section></div> ]]></description>
  <category>Quarto</category>
  <category>Python</category>
  <guid>https://chrisvoncsefalvay.com/posts/quarto-project-scripts/</guid>
  <pubDate>Sun, 22 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Lyre of Hephaestus</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/</link>
  <description><![CDATA[ 




<p>I have probably spent more time looking at Poussin’s <em>Dance to the Music of Time</em> than any other work of art. Sneaking off to <a href="https://wallacelive.wallacecollection.org/eMP/eMuseumPlus?service=direct/1/ResultListView/result.t1.collection_list.$TspTitleImageLink.link&amp;sp=10&amp;sp=Scollection&amp;sp=SfieldValue&amp;sp=0&amp;sp=0&amp;sp=2&amp;sp=SdetailList&amp;sp=0&amp;sp=Sdetail&amp;sp=0&amp;sp=F&amp;sp=T&amp;sp=2">the Wallace Collection</a> in London and just looking at the <em>Dance</em> was my comfort activity while living in London – a time that was not exactly devoid of its trials. It’s not, by any measure, <em>great</em> art, insofar as such judgments can be made with any objectivity. Like most French Baroque allegorical paintings, it’s considered somewhat ‘boring’. It neither has, nor claims to arrogate to itself, the fiery passion of a Van Gogh, the compositional genius of a Vermeer or the turbulence of a Turner. It’s not particularly well known, and indeed more know it for having lent its name to Anthony Powell’s dodecalogy (quite incidentally, another of my favourite works of art). In as far as there can be a sensible debate as to what is, and isn’t, ‘good’ or ‘great’ art, few would consider <em>Dance</em> to fall into that category. What nobody would argue, however, is that it <em>is</em> a piece of art.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/IMG_8537.jpeg" class="img-fluid figure-img"></p>
<figcaption>Nicolas Poussin, <em>Dance to the Music of Time</em>.</figcaption>
</figure>
</div>
<p>What, then, if I gave a somewhat clumsy description of the scene to Stable Diffusion, the currently fashionable model of generating images? Let’s try this:</p>
<blockquote class="blockquote">
<p>A painting, in the French Baroque style, of four female figures, allegories of the four seasons. They hold each other by the hand, dancing in a circle. Time, represented by an old man, plays a lyre. The scene is set in the early morning.</p>
</blockquote>
<p>Here’s what I got:</p>
<div id="fig-poussin-stablediffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poussin-stablediffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/IMG_8536.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poussin-stablediffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Stable Diffusion’s attempt at my description of the <em>Dance</em>.
</figcaption>
</figure>
</div>
<p>It’s not great by any stretch of the word. The faces, like in most diffusion models, are a mess. None of the figures get the allegory of time and seasons (of the year, but also of life, and of stations of an individual’s life). Certainly some more/better prompting could have gotten a slightly better result, but that’s really not the point.</p>
<p>The point is that it’s not <em>art</em> not because it’s not ‘good enough’ to be art, but because it is fundamentally, qualitatively different.</p>
<section id="one-art-please" class="level1 page-columns page-full">
<h1>One art, please</h1>
<p>At this point, we need to introduce a key concept to the argument to follow, namely that of a simulacrum. For the purposes of this argument, I shall define a simulacrum as follows: something that resembles the product of an intrinsically human activity but which can be obtained without that activity, including by non-human actors, such as non-human animals or, as it may be, artificial intelligences.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/IMG_8534.jpeg" class="img-fluid figure-img"></p>
<figcaption>One art, please!</figcaption>
</figure>
</div>
<p>At the heart of the notion of a simulacrum is that the product and the process are different things. Art is not a product, but a process. I am, as I will explain later, not singling out art in some romantic exaltation of the artistic as quintessentially human, nor am I limiting art to what one might find in a museum. When I discuss art, I fully intend to include a toddler’s finger painting and even some absent-minded doodles on the margins of a less-than-gripping journal article.<sup>1</sup> There are other activities that may become the subject matter of a simulacrum, language being perhaps of foremost interest (on which, once again, see <em>infra</em>). And just as Zoidberg can’t buy ‘art’, merely a ‘work of art’ (‘work’ here having the meaning of ‘product’ – an ‘artwork’ or ‘work of art’ being not something with ‘art characteristics’ but rather the result of the activity we know as ‘art’), an artificial intelligence or a non-human animal cannot create ‘art’, either.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Because who of us hasn’t been bored enough by a dull paper to start doodling?</p></div></div><p>At the heart of this is the difference between, of course, the process and the product. Artificial intelligence generally works according to a very simple inductive principle: given a bunch of examples (such as pairs of images and descriptions of images), learn the parameters of a function <img src="https://latex.codecogs.com/png.latex?f(d)%20=%20I"> so that for any arbitrary description <img src="https://latex.codecogs.com/png.latex?d">, an image <img src="https://latex.codecogs.com/png.latex?I"> is generated so as to minimise a loss function <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=0%7D%5EN%20J(d_i,%20I_i)"> over the <img src="https://latex.codecogs.com/png.latex?N">-sized training set. <img src="https://latex.codecogs.com/png.latex?f(d)"> may be quite complex indeed, and <img src="https://latex.codecogs.com/png.latex?J(d,%20I)"> may comprise various steps up to and including human input (reinforcement learning with human feedback – RLFH for short –, which is how most of the LLMs currently in vogue were trained, include this step quite prominently), but in the end, it’s all about learning <img src="https://latex.codecogs.com/png.latex?f">. For a neural network, for instance, this ‘learning’ takes place by way of an algorithm called backpropagation, which changes the values of weights and bias matrices that form part of <img src="https://latex.codecogs.com/png.latex?f"> so as to minimise <img src="https://latex.codecogs.com/png.latex?J">. Regardless of how <img src="https://latex.codecogs.com/png.latex?f"> is learned, what is at the heart of the entire process is learning a function that simulates the outcome. Stable Diffusion, for instance, is not concerned with simulating the <em>process</em> of creating art, it is concerned with simulating the <em>outcome</em> of creating art.</p>
</section>
<section id="human-processes" class="level1 page-columns page-full">
<h1>Human processes</h1>
<p>That matters, because not everything is about outcomes. In fact, as humans, we recognise that some of the most important things <em>aren’t</em>. Consider moral judgment, the individual and societal assignment of a moral character to human actions. Yes, outcomes do matter, but they aren’t the be-all end-all of moral judgment. We differentiate between murder, manslaughter, reckless homicide, excusable homicide and justifiable homicide, even though the end result is the same (i.e.&nbsp;someone is dead), because we acknowledge that different levels of moral agency, and different forms of its exercise, are at play in each of these. We do so because we recognise humans are more than mere outcome generating machines: how (and why) something came to be is often just as important, if not more so, than what the end result is.</p>
<p>This is a uniquely human differentiation. In the same vein, then, there are things that only humans are capable of. A machine can certainly <em>take a life</em>, but it cannot <em>commit murder</em>, because the latter is defined not just by an outcome but also the mental state (<em>mens rea</em>) that led up to it. We think of Xerxes flogging the sea as the act of a megalomaniacal despot who has taken leave of his senses precisely because we recognise that punishing the sea, incapable of moral responsibility for its stormy waves washing away Xerxes’s pontoon bridges, as preposterous. Punishment requires, if not guilt, at least a subject <em>capable</em> of being guilty. Punishing the innocent is unjust, but punishing something incapable of being guilty or innocent or otherwise morally responsible is worse – it is <em>insane</em>.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Not that otherwise quite rationalistic societies did not engage in trials of lifeless things. <a href="https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?referer=&amp;httpsredir=1&amp;article=7589&amp;context=penn_law_review">Walter Woodburn Hyde wrote a wonderful paper on this</a>.</p></div></div><p>And just as moral guilt is uniquely human, so is creativity. This is not to suggest an exalted role for creativity – much as we tend to think of that term to have a positive meaning, for the purposes of this argument, creativity is any act of the mind that brings forth something. This comprises not just things like creating art, but also using language. There are many functions of language, ranging from the informative through the performative (what Searle called ‘speech acts’) to the phatic. However, humans communicating always do so with <em>some</em> intent. That’s different from machines using words. A machine uses words, and maybe uses language, but it does not ‘speak’, for that requires an intent, something no machine, howsoever sophisticated, can create. Like art, speech is not just about <em>what</em>, but the <em>why</em>, of an action. There are many reasons my neighbour greets me with “hey, how’re you doing?” – maybe it’s genuine interest in how I am doing, maybe it is as a kind of social bond-strengthening (phatic communication), but it is backed by some sort of intent that goes beyond just wanting to produce words.</p>
</section>
<section id="the-tune-without-the-music" class="level1 page-columns page-full">
<h1>The tune without the music</h1>
<p>A simulacrum, then, is what’s left when the uniquely human <em>process</em> is removed, and only the outcome remains, obtained through a different process (e.g.&nbsp;inference from a model generated by way of minimising the loss function over a training set). It is the tune without the music. A simulacrum doesn’t necessarily have to be ‘worse’ than the real deal – quite likely, Stable Diffusion can produce a better drawing than I could, and GPT-4 might well write a better poem than my hamfisted attempts at poetry. However, neither of those <em>are</em> the real thing. Stable Diffusion will be able to do better drawings than I do, but it won’t ever be able to do better <em>art</em>, because it’ll never be able to do art.</p>
<p>In that sense, it’s the opposite of ‘going through the motions’: a simulacrum is an often indistinguishable in its end product from the ‘real thing’, but arrives there by a different route. This is, indeed, why it is so deceptively similar to the ‘real thing’, and what gives rise to perhaps its darker side. In a recent interview on one of my favourite podcasts, <a href="https://www.youtube.com/c/MachineLearningStreetTalk">Machine Learning Street Talk</a>, Daniel Dennett referred to the simulacrum of human communicational interaction as “counterfeit people”. I disagree with some of Dennett’s position, but this is worth watching in its entirety:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/axJtywd9Tbo?start=725" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>I think Dennett is wrong on a few points,<sup>3</sup> but the language of ‘counterfeits’ gets some of this point across quite well. A counterfeit currency isn’t really all <em>that</em> different from an authentic bank note. What is different is the intent by which it is created. The counterfeit lacks the moral intent that it be honoured in return (that’s why people forge currency – there would be little point in counterfeiting bank notes if one actually intended to honour them!).<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Importantly, I think he’s falling victim to the trap he so clearly identifies. The lack of moral agency of AI, which would make these pretend-humans something other than ‘real’ is exactly why the semantics of ‘counterfeiting’, which involves deception, an act of quintessential moral agency, is altogether inappropriate. But that’s for another day.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;Arguably, all modern fiat currency is at least a little fake, but that is <em>also</em> another argument for another day.</p></div></div></section>
<section id="the-heart-of-the-matter" class="level1 page-columns page-full">
<h1>The heart of the matter</h1>
<p>In the end, outcomes are <em>not</em> all that make us who we are. The poet, the artist, the murderer and the thief – they are who and what they are not solely because of outcomes, but because of what led up to those outcomes. And that’s ultimately the most human thing any of us will ever get to experience: to be someone not because of an outcome that a machine or an algorithm could feasibly mimic, but because of the quintessentially human process of decisions, good or bad, actions, passion, foibles. The things that make us who we are are in turn a <em>product</em> of who we are.</p>
<p>And that’s what gives meaning to those outcomes, meaning that a machine-generated limerick could never aspire to – not because it is not ‘good enough’, but because it is just a qualitatively different thing. It is, to use Pauli’s turn of phrase, not that the Stable Diffusion-generated painting in Figure&nbsp;1 is ‘not right’ (or aesthetically pleasing, in this case) – it’s that it’s ‘not even wrong’. It lacks the aptitude of being good or bad art, no matter how good or bad (likely the latter) it would be if it didn’t. It could, in theory, be good enough to put Poussin to shame, and it would still be no closer to being art. It could be Turner Prize-winning work,<sup>5</sup> and yet be put to shame by a toddler’s finger-painting.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Let’s, for the time being, entertain the notion that that still means <em>something</em>.</p></div></div><p>A world in which the poet and the machine are on equal footing is a world that, for all intents and purposes, does not have poets anymore. It is a world in which outcomes determine everything, and that makes it a world that is not worth inhabiting: for a world of outcomes alone is a world in which our choices, our actions, our most irreplicably human strengths and weaknesses alike play no role.</p>
<p>But that is not the world we inhabit – thankfully. We – still? – live in a world in which the way we arrive at something, be it a poem or a painting or a dead body, matters. And as long as that is so, the simulacra of human action will remain inhabiting a different world of their own. They may reach a kind of technical verisimilitude, but not a moral verisimilitude.</p>
<p>And <em>that</em> is what makes all the difference.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {The {Lyre} of {Hephaestus}},
  date = {2023-10-16},
  url = {https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/},
  doi = {10.59350/pd0q6-r5n60},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“The Lyre of Hephaestus.”</span> <a href="https://doi.org/10.59350/pd0q6-r5n60">https://doi.org/10.59350/pd0q6-r5n60</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>art</category>
  <category>philosophy</category>
  <guid>https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/</guid>
  <pubDate>Mon, 16 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/lyre-of-hephaestus/IMG_8534.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Beyond Broca</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/llms-language/</link>
  <description><![CDATA[ 




<p>There’s something special about language. It is ‘our own’, it is ‘us’, in a profound way, and quite surprisingly, more so than art. I was deeply struck by this when I first saw reactions to large generative language models that created realistic, human-ish prose. Notably, those mature enough to reach a non-professional audience – ChatGPT based on GPT-3 and later GPT-4 – came quite some time after models that could create fairly acceptable visual ‘art’.<sup>1</sup> The appearance of synthetic language-like products (SLLPs), as I like to call the output of such generative models, came well after the appearance of synthetic simulacra of visual art,<sup>2</sup> yet elicited much less fervent responses.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I don’t mean to insinuate that what Stable Diffusion and DALL·E produce are ‘art’ in the sense we understand that concept. However, neither is what GPT produces ‘language’. They are both simulators of outcomes based on stochastic approximations over a sufficiently large training set to be able to approximate the outcome of human activities we know as ‘art’ and ‘language’, respectively.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;For a given value of “well after”. Time, in this discipline, moves with an unsettling alacrity.</p></div></div><p>This was quite striking, for three reasons.</p>
<ul>
<li>For one, computationally, the probability space that a model seeking to create a realistic image has to navigate is exponentially larger than what’s required to produce human-like prose.</li>
<li>Secondly, we consider making art to be a very deeply human endeavour. Animals may to some minimal extent be taught to create poor simulacra of human artistic endeavours like painting, but nobody would confuse a trained elephant’s ‘paintings’ to art <span class="citation" data-cites="ross2019consciousness">(Ross 2019)</span>. Art is not just a product, it’s also an activity, one that proceeds with a subjective element in the artist, and no machine can replicate the process, no matter how well it may approximate the outcome.</li>
<li>Most importantly, however, despite the previous point, lay audiences saw a connection between a simulacrum of language and human-like intelligence that was absent from a simulacrum of art.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ross2019consciousness" class="csl-entry">
Ross, Don. 2019. <span>‘Consciousness, Language, and the Possibility of Non-Human Personhood: Reflections on Elephants’</span>. <em>Journal of Consciousness Studies</em> 26 (3-4): 227–51.
</div></div><p>Which, of course, leads us to the key question: what if we got one of the most deeply enshrined beliefs about language, intelligence and the relationship between the two <em>utterly, dreadfully wrong</em>?</p>
<section id="our-precious-words" class="level1">
<h1>Our precious words</h1>
<p>A large language model (LLM) is, essentially, a very simple machine that knows a large number of conditional probabilities. Given a sequence of tokens <img src="https://latex.codecogs.com/png.latex?k_0,%20k_1,%20%5Ccdots,%20k_%7Bn%7D">, it associates every possible token <img src="https://latex.codecogs.com/png.latex?k%5E%7B%5Cprime%7D"> with a probability <img src="https://latex.codecogs.com/png.latex?p(k_%7Bn%20+%201%7D%20=%20k%5E%7B%5Cprime%7D%20%7C%20k_0,%20k_1,%20%5Ccdots,%20k_%7Bn%7D)"> – or in other words, given a token sequence <img src="https://latex.codecogs.com/png.latex?k_0,%20k%7B1%7D,%20%5Ccdots,%20k_%7Bn%7D">, it assigns to every point in a probability space a conditional likelihood that that point’s corresponding token will be the <img src="https://latex.codecogs.com/png.latex?k_%7Bn+1%7D">-th token. Or, using my preferred formulation, which looks at the inverse probability: given the token sequence, it creates a probability distribution of the next token and draws stochastically, weighted by token likelihood, so that a draw from the region of highest probability is most likely.</p>
<p>It turns out that if the model’s understanding of these conditional probabilities is sufficiently good, it can simulate knowledge quite well, <a href="https://chrisvoncsefalvay.com/posts/knowledge-dividend-of-llms/">a point I belaboured elsewhere</a>. This is not overly surprising. If a model knew the conditional probability of rain on day <img src="https://latex.codecogs.com/png.latex?d"> – let’s call this <img src="https://latex.codecogs.com/png.latex?p_r(d)">, given a vector <img src="https://latex.codecogs.com/png.latex?%5Ctheta_r"> of length <img src="https://latex.codecogs.com/png.latex?n"> that tells us whether it rained on days <img src="https://latex.codecogs.com/png.latex?d-1">, <img src="https://latex.codecogs.com/png.latex?d-2">, …, <img src="https://latex.codecogs.com/png.latex?d-n">, we’d trust it to tell us whether we’d need our raincoat on that given day. All it would have to do for that is to learn the conditional probability of <img src="https://latex.codecogs.com/png.latex?p_r(d)%20%7C%20%5Ctheta_n">, which of course it could easily do by representing <img src="https://latex.codecogs.com/png.latex?p_r(d)%20%7C%20%5Ctheta_n"> as <img src="https://latex.codecogs.com/png.latex?f(d,%20%5Ctheta_n)">, then learning the parameters of that function so as to minimise a loss function <img src="https://latex.codecogs.com/png.latex?J(f(d,%20%5Ctheta_n),%20r(d),%20%5Ctheta_n(d))">, where <img src="https://latex.codecogs.com/png.latex?r(d)"> is of course whether it rained on day <img src="https://latex.codecogs.com/png.latex?d"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_n(d)"> is the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_n"> history vector for the day <img src="https://latex.codecogs.com/png.latex?d">. Iterate this often enough (over not single values of <img src="https://latex.codecogs.com/png.latex?r(d)"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_n(d)"> but vectors thereof), and you can learn a pretty decent conditional probability function. The model would know no more about rain or shine than LLMs know about language or the subject matters of language, but simulating tokens gets you quite a long way towards being useful as a simulacrum of knowledge.</p>
<p>Indeed, this is to the point that what comes out of such a model might well appear human-like: modern GPT implementations can produce prose that is a little stilted at times, but certainly often only distinguishable from human prose by the conspicuous absence of grammatical and spelling errors. This is interesting because of how it was perceived: quite immediately, this was connected to a kind of intelligence that was almost human, or indeed at times better than human. People suddenly started to worry about a dumb token simulator taking over their jobs.</p>
<p>Clearly, language hit a nerve.</p>
</section>
<section id="the-medium-is-the-message" class="level1 page-columns page-full">
<h1>The medium is the message</h1>
<p>JARVIS. Siri. Alexa. WOPR. The AIs of fiction and our every-days have one thing in common: they use language as the presentation layer. This is deceptive, because neither of these systems are, well, particularly smart. Compared to models that can, say, quantitatively infer the activity of a small molecule drug from its structure <span class="citation" data-cites="karpov2020transformer zhong2023developing guntuboina2023peptidebert">(QSAR models, on which see Karpov, Godin, and Tetko 2020; and also see Zhong and Guan 2023; Guntuboina et al. 2023)</span>, Siri is pretty pathetic. However, it has something QSAR models and other very impressive applications of machine learning don’t: the human presentation layer, i.e.&nbsp;language.</p>
<div class="no-row-height column-margin column-container"><div id="ref-karpov2020transformer" class="csl-entry">
Karpov, Pavel, Guillaume Godin, and Igor V Tetko. 2020. <span>‘Transformer-CNN: Swiss Knife for QSAR Modeling and Interpretation’</span>. <em>Journal of Cheminformatics</em> 12 (1): 1–12.
</div><div id="ref-zhong2023developing" class="csl-entry">
Zhong, Shifa, and Xiaohong Guan. 2023. <span>‘Developing Quantitative Structure–Activity Relationship (QSAR) Models for Water Contaminants’ Activities/Properties by Fine-Tuning GPT-3 Models’</span>. <em>Environmental Science &amp; Technology Letters</em>.
</div><div id="ref-guntuboina2023peptidebert" class="csl-entry">
Guntuboina, Chakradhar, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani. 2023. <span>‘PeptideBERT: A Language Model Based on Transformers for Peptide Property Prediction’</span>. <em>arXiv Preprint arXiv:2309.03099</em>.
</div><div id="ref-endicott2006properties" class="csl-entry">
Endicott, Timothy, Joshua Getzler, and Edwin Peel. 2006. <span>‘Properties of Law: Essays in Honour of Jim Harris’</span>.
</div></div><p>How we treat a system seems to be conditional on how it talks to us. In that sense, the medium is profoundly how we treat the message. To use the terminology of J.W. Harris’s writings on human rights, we associate the ‘right’ to be considered to be intrinsically connected to being capable of engaging in human ’discourse <span class="citation" data-cites="endicott2006properties">(see Endicott, Getzler, and Peel 2006)</span>. And that, of course, means language.</p>
<p>This is not overly surprising, either. Our understanding of language has been that of a watershed moment in evolution. Humans became what they are when they learned to use language. Tool use is great, but tool use only makes <em>a human</em> at best. What makes <em>humans</em>, plural, is language. This is intrinsically connected, of course, to society. Language is not an arbitrarily selected activity, nor is it really necessarily the kind of evolutionary game changer that tool use is. Rather, it is the tool, the <em>sine qua non</em>, the cornerstone and the absolutely fundamental instrument of social interaction. Language creates society. Society recognises human individuals and gives that recognition a meaning. The fact that I am a human being, and recognised as such (I hope), has a meaning that is different from me recognising that my dog is an individual of the species <em>Canis lupus familiaris</em>, because it does not merely acknowledge me as being of a certain species, but also of being of a certain kind of agent capable not only of having rights but also of speaking for them. Language is how all that happens <span class="citation" data-cites="budwig2000language browning2023personhood">(e.g. Budwig 2000; but see Browning 2023)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-budwig2000language" class="csl-entry">
Budwig, Nancy. 2000. <span>‘Language, Practices and the Construction of Personhood’</span>. <em>Theory &amp; Psychology</em> 10 (6): 769–86.
</div><div id="ref-browning2023personhood" class="csl-entry">
Browning, Jacob. 2023. <span>‘Personhood and AI: Why Large Language Models Don’t Understand Us’</span>. <em>AI &amp; SOCIETY</em>, 1–8.
</div></div></section>
<section id="the-language-of-intelligence-or-vice-versa" class="level1 page-columns page-full">
<h1>The language of intelligence (or vice versa)</h1>
<p>What, then, if we got one of the most important things about humanity, and human intelligence, dreadfully wrong altogether? What if language is not a <em>product</em> of intelligence (as we understand it in the human context) but rather a necessary instrument thereof?</p>
<p>The evolution of something as crucial as language remains shrouded in a perplexing mystery to this date. What we know is that at some point, about 50-100,000 years ago, <em>something</em> happened that gave rise to language. We don’t quite know what it was, or how it specifically transpired. Indeed, despite advances in our understanding of cognitive neuroscience, we haven’t found evidence of the ‘language faculty’ proposed by <span class="citation" data-cites="hauser2002faculty jackendoff2005nature">Hauser, Chomsky, and Fitch (2002; but see the criticisms by Jackendoff and Pinker 2005)</span>.<sup>3</sup> The genetics of language production – which centres around FOXP2 these days <span class="citation" data-cites="enard2002molecular enard2011foxp2">(see Enard et al. 2002; Enard 2011)</span> – hasn’t gotten us a lot further, and there are way too many edge cases (<em>dissociations</em>, as the term in evolutionary neuroscience goes), where either there is a significant intellectual deficit despite preserved language ability (Williams syndrome being the textbook example of this, <em>viz.</em> <span class="citation" data-cites="bellugi2013dissociation">Bellugi et al. (2013)</span>) or the inverse (e.g.&nbsp;Developmental Verbal Dyspraxia, where there is impairment to language production but not to overall intellect, <em>viz.</em> <span class="citation" data-cites="vargha2005foxp2">Vargha-Khadem et al. (2005)</span>) to be able to confidently make this connection on an individual level.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hauser2002faculty" class="csl-entry">
Hauser, Marc D, Noam Chomsky, and W Tecumseh Fitch. 2002. <span>‘The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?’</span> <em>Science</em> 298 (5598): 1569–79.
</div><div id="ref-jackendoff2005nature" class="csl-entry">
Jackendoff, Ray, and Steven Pinker. 2005. <span>‘The Nature of the Language Faculty and Its Implications for Evolution of Language (Reply to Fitch, Hauser, and Chomsky)’</span>. <em>Cognition</em> 97 (2): 211–25.
</div><div id="fn3"><p><sup>3</sup>&nbsp;not to be confused with the brain areas responsible for speech, which perplexingly are part, but not the whole, of the language faculty</p></div><div id="ref-enard2002molecular" class="csl-entry">
Enard, Wolfgang, Molly Przeworski, Simon E Fisher, Cecilia SL Lai, Victor Wiebe, Takashi Kitano, Anthony P Monaco, and Svante Pääbo. 2002. <span>‘Molecular Evolution of FOXP2, a Gene Involved in Speech and Language’</span>. <em>Nature</em> 418 (6900): 869–72.
</div><div id="ref-enard2011foxp2" class="csl-entry">
Enard, Wolfgang. 2011. <span>‘FOXP2 and the Role of Cortico-Basal Ganglia Circuits in Speech and Language Evolution’</span>. <em>Current Opinion in Neurobiology</em> 21 (3): 415–24.
</div><div id="ref-bellugi2013dissociation" class="csl-entry">
Bellugi, Ursula, Shelly Marks, Amy Bihrle, and Helene Sabo. 2013. <span>‘Dissociation Between Language and Cognitive Functions in Williams Syndrome’</span>. In <em>Language Development in Exceptional Circumstances</em>, 177–89. Psychology Press.
</div><div id="ref-vargha2005foxp2" class="csl-entry">
Vargha-Khadem, Faraneh, David G Gadian, Andrew Copp, and Mortimer Mishkin. 2005. <span>‘FOXP2 and the Neuroanatomy of Speech and Language’</span>. <em>Nature Reviews Neuroscience</em> 6 (2): 131–38.
</div></div><p>On the other hand, on a broader level, it is hard to discount the relationship. What is more complex is the direction of this relationship. There are, really, three possible scenarios:</p>
<ol type="1">
<li>Language is a consequence of human intelligence. The kind of intelligence we associate with modern human cognitive capabilities necessarily presupposes, absent some marginal exceptions, language.</li>
<li>Language is an epiphenomenon of human intelligence. It evolved in parallel, but neither requires human intelligence (see Williams syndrome) nor does human intelligence require it (see Developmental Verbal Dyspraxia).</li>
<li>Human intelligence is largely a consequence of language, which is its necessary but not sufficient condition. It is the evolutionarily most stable representation layer for information, and allows reasoning through complexity.</li>
</ol>
<p>While the second of these is a convenient way to hand-wave away the entire question and account for the edge cases I discussed above, I find the third of these much more compelling. It is not defeated by the argument from either of the edge cases: it is not defeated by arguments from intact language despite intellectual deficits, because it does not assume that language is sufficient, merely that it is necessary. It is not defeated by the inverse, either, because it permits a small number of deviations. Language is not the only possible representational layer that could underpin intelligence. It is, however, vastly more evolutionarily advantageous through its efficiency. It is so much stronger and so much more efficient that it can be considered almost absolutely dominant – which indeed accounts for the fact that disorders of language with preserved intellectual functioning are vanishingly rare. If the efficiency of language as the ‘operating system’ of intelligence weren’t so strongly dominant, such disorders would not be disorders, indeed, but alternate ways of cognitive existence that are equally evolutionarily stable.</p>
</section>
<section id="the-golden-link" class="level1 page-columns page-full">
<h1>The golden link</h1>
<p>Which leads us to what I shall call the “golden link” of intelligence – and perhaps the most frightening finding that derives from LLMs. We intuit, correctly, that a realistic simulacrum of language is an indication of intelligence. We once more intuit, correctly, that even if we’re aware of the limits of LLMs’ ‘language’, it displays more than a scintilla of whatever makes up intelligence. Just as Stable Diffusion is not art but a simulacrum of the end result of the process we know as ‘art’, ChatGPT isn’t really ‘language’ but a simulacrum, by way of extending token sequences, of the end result of the process we know as ‘language’ – but no matter how deeply we understand this, it is hard to deny that ChatGPT does speak to us, to quote Kipling, “as a man would talk to a man”. Or, to put it this way: all the amazing things genuinely complicated artificial intelligence can do, such as predict protein structures or binding affinities or interpret histology specimens or optimise mathematical problems, is a <em>praxis</em> – something the system <em>does</em>. Producing language is, or at least is some way towards, a <em>hexis</em> – something the system <em>is</em>. Ande that makes all the difference.</p>
<p>And so, our trepidation and the ‘uncanny valley’ sensation of LLMs’ ‘intelligence’ is quite instructive <span class="citation" data-cites="floridi2023ai">(Floridi 2023)</span>. It shows, clear as day, the intrinsic link between language and intelligence, but more importantly, that language is not a consequence of intelligence but a fundamental pre-requisite and indeed the communication protocol on which efficient human intelligence rests. Language is neither sufficient nor necessary for human intelligence (and perhaps other forms of intelligence do exist that do not require language at all), but it is the evolutionarily dominant stable strategy for representing information in a manner that can support intelligence.</p>
<div class="no-row-height column-margin column-container"><div id="ref-floridi2023ai" class="csl-entry">
Floridi, Luciano. 2023. <span>‘AI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models’</span>. <em>Philosophy &amp; Technology</em> 36 (1): 15.
</div></div><p>There lies the scariest revelation of LLMs. It’s not that LLMs will supplant us (they won’t), or that we’ll be condemned to a lifetime of reading books written by LLMs (have you tried to get ChatGPT to write a story on a novel premise?), nor that LLMs will steal our jobs and take over the planet. Rather, the great revelation is that LLMs cast light on what might have been one of the longest standing fallacies of humans reasoning about reasoning – that language is the product of intelligence, and not its operating system.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {Beyond {Broca}},
  date = {2023-10-15},
  url = {https://chrisvoncsefalvay.com/posts/llms-language/},
  doi = {10.59350/vynvf-0k137},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“Beyond Broca.”</span> <a href="https://doi.org/10.59350/vynvf-0k137">https://doi.org/10.59350/vynvf-0k137</a>.
</div></div></section></div> ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>philosophy</category>
  <category>language</category>
  <guid>https://chrisvoncsefalvay.com/posts/llms-language/</guid>
  <pubDate>Sun, 15 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How to do a SkiErg marathon entirely the wrong way (but still finish)</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/skierg-marathon/</link>
  <description><![CDATA[ 




<p>Yesterday, I completed a marathon on the SkiErg, with a pretty respectable time (<a href="https://log.concept2.com/profile/177991/log/79965221">see results here</a>). I know I can do better, and I know I <em>will</em> do better. On the other hand, this was a marathon where I pretty much did everything wrong. And that was sort of an instructive experience in its own right.</p>
<p>The title is somewhat misleading. Not much went wrong – the marathon itself was smooth sailing. At a 2:34.6 pace (min/500m), it was one of my better very long distance pieces – <a href="https://log.concept2.com/profile/177991/log/79735748">my ranked half-marathon pace at the time was 2:30.7</a>, and that was a very intentional, very prepared piece. There were also really no unexpected surprises in the process. On the other hand, I built up to this marathon the in worst way possible.</p>
<section id="what-worked-for-me" class="level1">
<h1>What worked for me</h1>
<p>It’s probably important to be realistic ahead of all: I did go into this madness with some assets.</p>
<ul>
<li>For one, I’m a <a href="https://www.concept2.com/skierg/motivation/records/adaptive-world?event=&amp;gender=M&amp;age_category=30&amp;adaptive=31&amp;op=Search&amp;form_id=concept2_record_search_form#results">multiple world record holder in my adaptive class</a>. I know a thing or three about the SkiErg, and as a former rower, I’ve got every rower’s love-hate relationship with the erg.</li>
<li>I’m a former multisport athlete, and I’ve been doing a lot of long distance pieces. I know how to cope with the boredom. Most people who go into a marathon have done a half, and maybe a 30k piece. I’ve done several split marathons in the past few weeks, as well as several 30k pieces.</li>
<li>I’m probably in a pretty good state of overall fitness. I have enough of a gym habit to qualify as not exactly a couch potato. There’s a lot you can get away with if you have a decent <img src="https://latex.codecogs.com/png.latex?VO_%7B2_%7Bmax%7D%7D">, and even though I’m in a wheelchair, I do enough activity to stay in good enough cardiovascular fitness.</li>
<li>I have an incredible boredom tolerance. Marathons are primarily psychological: we’re just not used to doing the same one thing, without interruption, for 3-5 hours anymore.</li>
<li>I guess insofar as there’s such a thing as a genetic predisposition to be good at long distance physical activity, I’ve got the right genes. My dad is a multi-marathon athlete, so I likely have the right physiology.</li>
</ul>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-skierg-marathon" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-skierg-marathon" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-skierg-marathon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/skierg-marathon/index_files/figure-html/skierg-marathon-fig-skierg-marathon-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-skierg-marathon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Stroke level data for the SkiErg marathon.
</figcaption>
</figure>
</div>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-1" href="skierg-marathon-preview.html#cell-fig-skierg-marathon">Source: skierg-marathon.ipynb</a></div>
</section>
<section id="how-not-to-do-a-marathon" class="level1">
<h1>How not to do a marathon</h1>
<p>There are generally three commonly accepted things to do for a good long distance piece:</p>
<ul>
<li>get some rest,</li>
<li>carb load, and</li>
<li>if you’re not doing an organised race, have someone check in on you and help with things like hydration.</li>
</ul>
<p>I’ve done exactly zero of these things. For starters, I did my marathon in the middle of 75 Hard, which in retrospect was not the best of ideas, as it meant that (1) I wouldn’t be able to take days off to rest, and (2) I wouldn’t be able to carb load as that would be outside my diet. I’ve also decided to do all this while my better half was away visiting family, so I was pretty much on my own.</p>
<p>At 2am.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-skierg-marathon-regression" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div id="fig-skierg-marathon-regression" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-skierg-marathon-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://chrisvoncsefalvay.com/posts/skierg-marathon/index_files/figure-html/skierg-marathon-fig-skierg-marathon-regression-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-skierg-marathon-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Stroke level data of stroke length (metres) versus stroke duration (seconds).
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Yes, that’s not a joke – I decided to do my marathon at a time when the world is falling apart at the seams, and so the only time I could find during which I could hope not to be interrupted with the latest crisis was late at night. So I woke up at 0130 hours, much to my dog’s chagrin, and headed down to my apartment building’s gym to torture myself for a few hours before the sun came up. Fortunately, the world decided to behave for those three and a half hours.</p>
<p>I tried to counteract this to some extent by doing something I generally dislike: carbohydrate gels. A long, long time ago, in a galaxy far away, I ran the Watford half-marathon when I was still into actually running (by far the only reason I remember <em>where</em> that half-marathon was is because it was so miserable). It was a bitter cold day, and it actually started <em>snowing</em> in the middle of the damn thing. To keep myself mostly sane and nourished, I took some of these 100kcal sachets of carbohydrate gel. What I didn’t know, of course, was that that stuff would glue your mouth together unless you consumed enough water… which I didn’t. Bad idea. On the other hand, I am now older, wiser and able to afford much better and less sticky-tastic carbohydrate gels, so I thought I was going to have some. I didn’t feel much of an effect one way or the other. At one 100kcal sachet for every hour, I paced it a little slower than is commonly recommended (one sachet per every 45 minutes seems to be the consensus recommendation), but really didn’t feel much of an effect either way. In retrospect, I probably wouldn’t bother doing it again.</p>
<p>As for rest, the best I could do is not do a long distance piece the day before the marathon. Indeed, I only did a relaxing 2k that day, which is about a fifth of my average daily SkiErg distance. Nevertheless, I think it was quite clear that with my ultra-low-carb diet, no carb loading and at best minimal replenishment (my average burn rate was around 5-600kcal/hr, and I took in 100kcal via the carbohydrate gels, so pretty much a drop in the ocean), I was running on fumes for most of it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://chrisvoncsefalvay.com/posts/skierg-marathon/IMG_8541.jpeg" class="img-fluid figure-img"></p>
<figcaption>Incidentally, my marathon time is also the SIT2 world record for my age group.</figcaption>
</figure>
</div>
</section>
<section id="takeaways" class="level1">
<h1>Takeaways</h1>
<p>My key takeaway from this experience was that we all should do a marathon under suboptimal conditions. Yes, doing a long distance piece at Bataan death march levels of calorie deficit is probably not super healthy, but you learn some interesting things about your body in the process. For starters, I think it was an interesting illustration of the fact that one’s overall level of fitness – especially <img src="https://latex.codecogs.com/png.latex?VO_%7B2_%7Bmax%7D%7D"> – matters a lot more than game day conditions. You can get away with a lot if you’re fit enough, up to and including running the show in quite sub-optimal circumstances.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2023,
  author = {{Chris von Csefalvay}},
  title = {How to Do a {SkiErg} Marathon Entirely the Wrong Way (but
    Still Finish)},
  date = {2023-10-13},
  url = {https://chrisvoncsefalvay.com/posts/skierg-marathon/},
  doi = {10.59350/68s7b-8c778},
  langid = {en-GB}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2023" class="csl-entry quarto-appendix-citeas">
Chris von Csefalvay. 2023. <span>“How to Do a SkiErg Marathon Entirely
the Wrong Way (but Still Finish).”</span> <a href="https://doi.org/10.59350/68s7b-8c778">https://doi.org/10.59350/68s7b-8c778</a>.
</div></div></section></div> ]]></description>
  <category>fitness</category>
  <category>SkiErg</category>
  <guid>https://chrisvoncsefalvay.com/posts/skierg-marathon/</guid>
  <pubDate>Fri, 13 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The knowledge dividend of large language models</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/knowledge-dividend-of-llms/</link>
  <description><![CDATA[ 




<p>Over at the <a href="https://medium.com/starschema-blog">work blog</a>, I’m discussing what knowledge means for large language models (LLMs), and the ways in which we can leverage this knowledge dividend for better inference.</p>
<blockquote class="blockquote">
<p>As I’m writing this, the sun hasn’t risen over the Denver skyline in earnest. There’s still pink in the sky over the Front Range, and most of the world is still blissfully asleep. And so far, a small, moderately fine-tuned Large Language Model (LLM) trained on $500 worth of free credits has explained to me just how bad the Broncos’ recent 20–70 embarrassment against the Miami Dolphins is (very), made some useful suggestions for a Caddoan language to learn if I wanted to help with language preservation (Pawnee) and created a fairly acceptable recipe to salvage whatever is left in my fridge (spicy tomato and cheese omelet with a chia side salad). Not too shabby for something that has absolutely no understanding of language preservation, omelets or American football (then again, neither do I, as far as the last one is concerned).</p>
<p>And therein lies one of the pervasive paradoxes of LLMs: they generate very confident, very credible and very often correct answers to questions on subjects they really don’t know all that much about.</p>
</blockquote>
<p><a href="https://medium.com/starschema-blog/the-knowledge-dividend-of-llms-a-pragmatic-perspective-79e8f8fb0686">Read the full post here.</a></p>



 ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>Cross-post: Starschema</category>
  <guid>https://chrisvoncsefalvay.com/posts/knowledge-dividend-of-llms/</guid>
  <pubDate>Mon, 02 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/knowledge-dividend-of-llms/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Asemantic Induction of Hallucinations in LLMs</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/asemantic-induction-of-hallucinations/</link>
  <description><![CDATA[ 




<p>Over at the <a href="https://medium.com/starschema-blog">work blog</a>, I’m discussing what knowledge means for large language models (LLMs), and the ways in which we can leverage this knowledge dividend for better inference.</p>
<blockquote class="blockquote">
<p>Language is highly semantic, but because of that, it is also highly flexible. By semantic, I mean that every lexeme has a distinct and coherent meaning. A lexeme is the “root form” that is conjugated to various forms, e.g.&nbsp;“see”, “saw” or “seeing” are all forms of the same lexeme, “SEE” (by convention, in linguistics, we set lexemes in capitals to distinguish them of their homomorphic conjugated forms). By flexibility, I mean that you can actually manipulate lexical order and retain meaning. Consider the sentence “Joe walked his dog in the park”. “In the park, Joe walked his dog” and “His dog, Joe walked in the park” all have slightly different nuances due to inflection and emphasis of order, but overall, they all get the same larger message across. Some languages permit even more flexibility with word order, but even in English, the worst case scenario is that it’ll sound a little odd. The content remains intelligible, which is why poets get to move words around to make things rhyme. In short, producing something that looks “like” natural language is going to be natural language. It’s likely not going to be a Booker Prize-winning product of staggering genius, but it will be good enough.</p>
<p>This is not the case for asemantic structures. By asemantic structures, I mean any system in which a sequence of tokens has a meaning, but in which there’s no semantic meaning to any token. In other words, every token has a meaning, but that meaning is not inherent in their character. It probably serves to belabour this point a little. All words are, to an extent, made up, but more similar words are closer to each other in meaning. By similar, I do not mean simple formal similarity, such as Hamming or Levenshtein distances, but rather logical similarity, e.g.&nbsp;conjugation of the same root. This is more evident in some languages than others. In Semitic languages, for instance, you have clusters of meaning that are connected by consonantal roots. By way of an example: the triconsonantal root k-b-d connects a cloud of meanings that all have to do with the middle or centre of something, and by extension the centre of gravity or honour. This gives us e.g., the Hebrew words for ‘heavy’ (כָּבֵד), mass in the physical sense (כֹּבֶד), and the word for ‘liver’ (כָּבֵד), which was considered to be roughly in the middle of the body. In any language, however, there is a degree of meaningful semantic similarity between connected concepts. There has more been written on this than I have the space to mention here.</p>
<p>An asemantic structure is where there are formally similar things that are unrelated. You have probably experienced this when you dialled the wrong number by a slip of the finger. The fact is, if you live in the United States, my phone number looks a lot like yours, and by extension, anyone else’s. There’s no semantically meaningful reason why your phone number shouldn’t be mine or vice versa: it’s not more ‘you’ as mine is not more ‘me’ in any underlying sense.</p>
<p>Which is why GPT-4 struggles with asemantic content, and we’ll use this to break it a little.</p>
</blockquote>
<p><a href="https://medium.com/starschema-blog/asemantic-induction-of-hallucinations-in-large-language-models-c92ef5030714">Read the full post here.</a></p>



 ]]></description>
  <category>AI</category>
  <category>LLMs</category>
  <category>Cross-post: Starschema</category>
  <guid>https://chrisvoncsefalvay.com/posts/asemantic-induction-of-hallucinations/</guid>
  <pubDate>Thu, 23 Mar 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/asemantic-induction-of-hallucinations/header.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Data for the next pandemic</title>
  <dc:creator>Chris von Csefalvay</dc:creator>
  <link>https://chrisvoncsefalvay.com/posts/data-for-the-next-pandemic/</link>
  <description><![CDATA[ 




<p>My recent post on the <a href="https://medium.com/starschema-blog">Starschema blog</a> discusses the need for better data products to tackle future pandemic challenges:</p>
<blockquote class="blockquote">
<p>Three years into the COVID-19 pandemic, the medical, public health and disaster preparedness communities are trying to isolate lessons learned from the harrowing experience of a global outbreak that resulted in little shy of seven million deaths. There remains considerable disagreement as to what ought or ought not have been done, what worked and what did not and how to best approach a coming pandemic.</p>
<p>One of the only points on which there is widespread agreement regarding the global response to the pandemic is that data played a crucial role in tackling the crisis. Where data drove decision-making, outcomes were almost universally better in terms of morbidity and mortality.</p>
</blockquote>
<p>Read the full post <a href="https://medium.com/starschema-blog/data-for-the-next-pandemic-f340cac8558c">here</a>.</p>



 ]]></description>
  <category>computational epidemiology</category>
  <category>epidemics</category>
  <category>Cross-post: Starschema</category>
  <guid>https://chrisvoncsefalvay.com/posts/data-for-the-next-pandemic/</guid>
  <pubDate>Tue, 07 Mar 2023 00:00:00 GMT</pubDate>
  <media:content url="https://chrisvoncsefalvay.com/posts/data-for-the-next-pandemic/header.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
