<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris von Csefalvay">
<meta name="dcterms.date" content="2026-01-04">
<meta name="description" content="The part where you have to take all that you have learned and work hard to completely unlearn it. Here be dragons.">

<title>The post-training instrument cluster – Part III – Chris von Csefalvay</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a4d8066ab99c821fadc425098389dfee.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=395640625"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', '395640625', { 'anonymize_ip': true});
</script>
<script type="application/ld+json">{"@context":"http://www.schema.org","@type":"person","name":"Chris von Csefalvay","jobTitle":"Director of Biomedical AI/ML","height":"74 inches","gender":"male","description":"Chris von Csefalvay is a computational epidemiologist and data scientist working at the intersection of AI/ML, computational dynamics and public health. He is the author of Computational Modeling of Infectious Disease and a number of research papers.","url":"https://chrisvoncsefalvay.com","image":"https://chrisvoncsefalvay.com/img/IMG_5986.jpeg","address":{"@type":"PostalAddress","addressLocality":"Denver","addressRegion":"CO","postalCode":"80204","addressCountry":"United States"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University of Oxford","sameAs":"https://en.wikipedia.org/wiki/University_of_Oxford"},{"@type":"CollegeOrUniversity","name":"Cardiff University","sameAs":"https://en.wikipedia.org/wiki/Cardiff_University"}],"worksFor":[{"@type":"Organization","name":"HCLTech"}],"birthDate":"1986-07-15","birthPlace":"Budapest, Hungary","memberOf":[{"@type":"Organization","name":"Royal Society for Public Health"},{"@type":"Organization","name":"TOPRA"},{"@type":"Organization","name":"IEEE"}],"nationality":[{"@type":"Country","name":"United Kingdom"},{"@type":"Country","name":"Hungary"}]}</script>
<style>
img, .cell-output-display img {
  max-width: 100%;
  height: auto;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="The post-training instrument cluster – Part III – Chris von Csefalvay">
<meta property="og:description" content="The part where you have to take all that you have learned and work hard to completely unlearn it. Here be dragons.">
<meta property="og:image" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/grpo_dashboard.png">
<meta property="og:site_name" content="Chris von Csefalvay">
<meta property="og:image:height" content="1268">
<meta property="og:image:width" content="1386">
<meta name="twitter:title" content="The post-training instrument cluster – Part III – Chris von Csefalvay">
<meta name="twitter:description" content="The part where you have to take all that you have learned and work hard to completely unlearn it. Here be dragons.">
<meta name="twitter:image" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/grpo_dashboard.png">
<meta name="twitter:image-height" content="1268">
<meta name="twitter:image-width" content="1386">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="The post-training instrument cluster -- Part III">
<meta name="citation_author" content="Chris von Csefalvay">
<meta name="citation_publication_date" content="2026-01-04">
<meta name="citation_cover_date" content="2026-01-04">
<meta name="citation_year" content="2026">
<meta name="citation_online_date" content="2026-01-04">
<meta name="citation_fulltext_html_url" content="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/">
<meta name="citation_language" content="en-GB">
<meta name="citation_reference" content="citation_title=DeepSeekMath: Pushing the limits of mathematical reasoning in open language models;,citation_author=Zhihong Shao;,citation_author=Peiyi Wang;,citation_author=Qihao Zhu;,citation_author=Runxin Xu;,citation_author=Junxiao Song;,citation_author=Mingchuan Zhang;,citation_author=Y. K. Li;,citation_author=Y. Wu;,citation_author=Daya Guo;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2402.03300;">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Chris von Csefalvay</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers"> 
<span class="menu-text">Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../media"> 
<span class="menu-text">Media</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">The Notebook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://computationalinfectiousdisease.com"> 
<span class="menu-text">Computational Modeling of Infectious Disease</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://craftofposttraining.com"> 
<span class="menu-text">The Craft of Post-Training</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-grpo-paradigm-shift" id="toc-the-grpo-paradigm-shift" class="nav-link active" data-scroll-target="#the-grpo-paradigm-shift">The GRPO paradigm shift</a></li>
  <li><a href="#why-loss-going-down-is-not-necessarily-your-friend" id="toc-why-loss-going-down-is-not-necessarily-your-friend" class="nav-link" data-scroll-target="#why-loss-going-down-is-not-necessarily-your-friend">Why loss going down is not (necessarily) your friend</a></li>
  <li><a href="#why-reward-cannot-substitute-for-loss" id="toc-why-reward-cannot-substitute-for-loss" class="nav-link" data-scroll-target="#why-reward-cannot-substitute-for-loss">Why reward cannot substitute for loss</a></li>
  <li><a href="#the-instruments-that-actually-matter" id="toc-the-instruments-that-actually-matter" class="nav-link" data-scroll-target="#the-instruments-that-actually-matter">The instruments that actually matter</a>
  <ul class="collapse">
  <li><a href="#instrument-1-reward-standard-deviation" id="toc-instrument-1-reward-standard-deviation" class="nav-link" data-scroll-target="#instrument-1-reward-standard-deviation">Instrument 1: reward standard deviation</a></li>
  <li><a href="#instrument-2-partial-reward-decomposition" id="toc-instrument-2-partial-reward-decomposition" class="nav-link" data-scroll-target="#instrument-2-partial-reward-decomposition">Instrument 2: partial reward decomposition</a></li>
  <li><a href="#instrument-3-completion-length-dynamics" id="toc-instrument-3-completion-length-dynamics" class="nav-link" data-scroll-target="#instrument-3-completion-length-dynamics">Instrument 3: completion length dynamics</a></li>
  <li><a href="#instrument-4-the-two-faces-of-clipping" id="toc-instrument-4-the-two-faces-of-clipping" class="nav-link" data-scroll-target="#instrument-4-the-two-faces-of-clipping">Instrument 4: the two faces of clipping</a></li>
  <li><a href="#instrument-5-entropy-dynamics" id="toc-instrument-5-entropy-dynamics" class="nav-link" data-scroll-target="#instrument-5-entropy-dynamics">Instrument 5: entropy dynamics</a></li>
  <li><a href="#instrument-6-kl-divergence" id="toc-instrument-6-kl-divergence" class="nav-link" data-scroll-target="#instrument-6-kl-divergence">Instrument 6: KL divergence</a></li>
  </ul></li>
  <li><a href="#putting-it-together-the-grpo-dashboard" id="toc-putting-it-together-the-grpo-dashboard" class="nav-link" data-scroll-target="#putting-it-together-the-grpo-dashboard">Putting it together: the GRPO dashboard</a></li>
  <li><a href="#practical-recommendations" id="toc-practical-recommendations" class="nav-link" data-scroll-target="#practical-recommendations">Practical recommendations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The post-training instrument cluster – Part III</h1>
  <div class="quarto-categories">
    <div class="quarto-category">AI</div>
    <div class="quarto-category">LLMs</div>
    <div class="quarto-category">LLMOps</div>
    <div class="quarto-category">post-training</div>
    <div class="quarto-category">reinforcement learning</div>
    <div class="quarto-category">GRPO</div>
  </div>
  </div>

<div>
  <div class="description">
    The part where you have to take all that you have learned and work hard to completely unlearn it. Here be dragons.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris von Csefalvay <a href="mailto:chris@chrisvoncsefalvay.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-3131-0864" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">4 January 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Hey, I’m writing a book about this!
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m actually writing a book about this stuff. It turns out there isn’t a lot of literature on how to do post-training at the level too big for single-GPU laptop-sized hobby projects and requiring enterprise reliability on one hand, but not quite at the scale of multi-team distributed post-training you’d get in foundation labs. That’s a problem, because a lot of the current value in fine-tuning applications comes exactly out of that large, crucial market. I am in the last phases of putting together the manuscript for <em>The Frontier Playbook</em>, a set of curated tactics and techniques for real world operationalisation of LLMs. <a href="https://aifrontierplaybook.substack.com">Sign up for updates here</a>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This is Part III of a series
</div>
</div>
<div class="callout-body-container callout-body">
<p>This post builds on <a href="../../posts/post-training-instrument-cluster/index.html">Part I</a> (SFT monitoring) and <a href="../../posts/post-training-instrument-cluster-rl/index.html">Part II</a> (RL monitoring fundamentals). If you haven’t read those yet, I recommend starting there – this post assumes familiarity with the basic instruments and the general philosophy of comprehensive training instrumentation. Here, we dive deep into the peculiarities of Group Relative Policy Optimisation (GRPO), where many of your hard-won SFT intuitions will actively mislead you.</p>
</div>
</div>
<p>In Parts I and II, we built an increasingly sophisticated instrument cluster for monitoring post-training runs. First came the eight instruments for supervised fine-tuning, then four more for preference optimisation and reinforcement learning. Together, they form a comprehensive dashboard for most training scenarios.</p>
<p>But GRPO is different. Not just different in the way that all RL methods are different from SFT – different in ways that can make experienced practitioners confidently misread their dashboards. The monitoring intuitions you’ve developed over years of watching loss curves descend gracefully towards convergence? They don’t apply here. The reward signal you’ve learned to trust as your north star in RL? It can lead you astray. Monitoring reinforcement learning is infuriatingly complex at the best of times, and GRPO arguably raises this to an entirely new level.</p>
<p>This post is about unlearning those intuitions and replacing them with ones appropriate for GRPO’s peculiar optimisation dynamics.</p>
<section id="the-grpo-paradigm-shift" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-grpo-paradigm-shift">The GRPO paradigm shift</h2>
<p>GRPO, developed by DeepSeek and described in the DeepSeekMath paper <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span>, takes a fundamentally different approach to policy optimisation than its predecessors. Where PPO maintains a separate value function to estimate advantages, GRPO computes advantages <em>relative to a group of completions for the same prompt</em>. This seemingly minor architectural choice has profound implications for monitoring.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shao2024deepseekmath" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. <span>‘DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models’</span>. <em>arXiv Preprint arXiv:2402.03300</em>.
</div><div class="">
<p>The key insight behind GRPO is that you don’t need a value function if you can compare multiple completions directly. By generating several responses to each prompt and computing advantages relative to the group mean, GRPO sidesteps the value function entirely – and with it, many of the training instabilities that plague PPO.</p>
</div></div>
<p>The standard GRPO objective is:</p>
<p><span class="math display">\[\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left( r_{i,t} \hat{A}_i, \text{clip}(r_{i,t}, 1-\epsilon, 1+\epsilon) \hat{A}_i \right) \right]
\]</span></p>
<p>where <span class="math inline">\(r_{i,t} = \frac{\pi_\theta(o_{i,t} | q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t} | q, o_{i,&lt;t})}\)</span> is the probability ratio and <span class="math inline">\(\hat{A}_i\)</span> is the advantage computed relative to the group. You may have seen this formulation a few hundred times, but it’s worth pausing to really internalise what it means for monitoring, because it can be your sole fixed point of reference in what can rapidly become a sea of confusing signals.</p>
<p>The crucial difference is in how advantages are computed. Instead of using a learned value function, GRPO computes:</p>
<p><span class="math display">\[\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G)}\]</span></p>
<p>where <span class="math inline">\(r_i\)</span> is the reward for completion <span class="math inline">\(i\)</span> and the normalisation is computed across the group of <span class="math inline">\(G\)</span> completions for the same prompt.</p>
<p>This has three important consequences for monitoring:</p>
<ol type="1">
<li><p><strong>The loss is not what you think it is.</strong> The GRPO “loss” is a clipped surrogate objective that can increase, decrease, or oscillate – and all of these can be perfectly healthy behaviour. No “line goes down, all is well with the world” here.</p></li>
<li><p><strong>Reward alone tells you nothing.</strong> Because advantages are normalised within groups, absolute reward values are meaningless. What matters is the <em>distribution</em> of rewards.</p></li>
<li><p><strong>Completion diversity is load-bearing.</strong> The entire algorithm depends on having meaningful variance within completion groups. If completions collapse to similar outputs, the advantage signal vanishes.</p></li>
</ol>
<p>Let me unpack each of these.</p>
</section>
<section id="why-loss-going-down-is-not-necessarily-your-friend" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-loss-going-down-is-not-necessarily-your-friend">Why loss going down is not (necessarily) your friend</h2>
<p>In supervised learning, we have a simple contract with the fabric of reality: loss measures how wrong we are, and while its absolute value is generally not very useful, its change/trend is. Training is supposed to make us less wrong, so loss going down is good. In GRPO, this contract is void.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is perhaps the single most common source of confusion I see in practitioners new to GRPO. They see the loss oscillating or even trending upward and assume something has gone wrong. Usually, nothing has – they’re just reading the wrong instrument.</p>
</div></div><p>The GRPO loss is a <em>surrogate objective</em>, not a measure of model quality. It’s designed to provide useful gradients for policy improvement, not to track progress towards a goal–or as I like to tell my students, in GRPO, loss doesn’t belong to you. It’s the model’s tool to work with, not yours. Recall the formula we laid out above: loss actually measures the clipped probability ratio times the advantage, averaged across tokens and completions. This quantity can behave in counterintuitive ways:</p>
<p><strong>Scenario 1: Loss decreases because the model is getting worse.</strong> If the model learns to produce completions that are all similarly mediocre, the advantage variance shrinks. Smaller advantages mean smaller loss values. The loss went down, but your model just got less capable of distinguishing good from bad responses. This is not good. Your model technically ‘improved’ but is, in fact, dying.</p>
<p><strong>Scenario 2: Loss increases because the model is improving.</strong> As the model learns to strongly prefer high-reward completions, the probability ratios for those completions increase. If they exceed the clipping threshold, the clipped term becomes the active constraint. Loss goes up, even though the model is actually improving.</p>
<p><strong>Scenario 3: Loss oscillates healthily.</strong> The policy is exploring, finding better responses, exploiting them, then exploring again. The loss oscillates because the advantage landscape is constantly shifting as the model improves.</p>
<div id="cell-fig-loss-paradox" class="cell" data-fig-height="8" data-fig-width="10" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-loss-paradox" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-paradox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-loss-paradox-output-1.png" width="952" height="759" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-paradox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The GRPO loss paradox: all four scenarios can represent healthy training, and the smoothly decreasing curve can actually indicate a problem.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The diagnostic implication, in my mind, is to <strong>stop watching the loss curve</strong>. If you must, watch it for sudden discontinuities, but don’t try to interpret its trend. The loss in GRPO is not a progress metric.</p>
</section>
<section id="why-reward-cannot-substitute-for-loss" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-reward-cannot-substitute-for-loss">Why reward cannot substitute for loss</h2>
<p>If loss isn’t meaningful, surely we can just watch the reward? After all, we’re optimising for reward, so higher reward means better model.</p>
<p>This is where GRPO’s group-relative nature becomes critical. The reward you see in your logs is typically the average reward across all completions. But GRPO doesn’t optimise for absolute reward – it optimises for <em>relative advantage within groups</em>. These are fundamentally different objectives with different failure modes.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This distinction matters more than it might seem. You can have a GRPO run where mean reward increases steadily while the model is actually getting <em>worse</em> at the task. Conversely, you can have a run where mean reward plateaus but the model is learning exactly what you want.</p>
</div></div><p>Consider a simple scenario with binary rewards (0 or 1) and group size 4:</p>
<ul>
<li><strong>Epoch 1</strong>: Group completions get rewards [0, 0, 1, 0]. Mean = 0.25. The model learns to prefer the completion that got reward 1.</li>
<li><strong>Epoch 50</strong>: Group completions get rewards [1, 1, 1, 1]. Mean = 1.0. The model has learned to always produce high-reward completions.</li>
</ul>
<p>The mean reward went from 0.25 to 1.0 – wonderful! But what’s the advantage signal in epoch 50? Zero. Every completion has reward 1, so every normalised advantage is 0. There’s nothing the model can learn from it, despite the perfect reward.</p>
<p>If this is sustained, you get <strong>reward saturation</strong>, the core problem of GRPO training. GRPO’s group emphasis requires diversity <em>within groups</em> to compute meaningful advantages. When all completions for a prompt achieve similar rewards, the learning signal vanishes, regardless of how high the absolute reward is.</p>
<div id="cell-fig-reward-saturation" class="cell" data-fig-height="5" data-fig-width="10" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-reward-saturation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reward-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-reward-saturation-output-1.png" width="952" height="471" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reward-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Reward saturation in GRPO: mean reward can increase while learning signal vanishes.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-instruments-that-actually-matter" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-instruments-that-actually-matter">The instruments that actually matter</h2>
<p>So if loss and reward don’t tell you what you need to know, what does? GRPO monitoring requires a different set of primary instruments.</p>
<section id="instrument-1-reward-standard-deviation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-1-reward-standard-deviation">Instrument 1: reward standard deviation</h3>
<p>The single most important metric for GRPO training is the standard deviation of rewards within completion groups. This directly measures the strength of your learning signal.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>TRL logs this as <code>reward_std</code> by default when using GRPOTrainer. If you’re using a custom implementation, you should compute and log this yourself – it’s that important.</p>
</div></div><div id="9a6a415a" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Reward standard deviation monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainerCallback</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RewardStdCallback(TrainerCallback):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor reward standard deviation as primary health indicator."""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, min_std_threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.05</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_std_threshold <span class="op">=</span> min_std_threshold</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.std_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"reward_std"</span> <span class="kw">in</span> logs:</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            std <span class="op">=</span> logs[<span class="st">"reward_std"</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.std_history.append(std)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.std_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                stds <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.std_history)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Trend: is variance collapsing?</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"reward_std/trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(stds)), stds, <span class="dv">1</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                <span class="co"># How close to danger zone?</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"reward_std/headroom"</span>] <span class="op">=</span> std <span class="op">/</span> <span class="va">self</span>.min_std_threshold</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Warning if below threshold</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> std <span class="op">&lt;</span> <span class="va">self</span>.min_std_threshold:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"reward_std/warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"WARNING: Reward std (</span><span class="sc">{</span>std<span class="sc">:.4f}</span><span class="ss">) below threshold "</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"(</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>min_std_threshold<span class="sc">}</span><span class="ss">). Learning signal weak."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Reward std declining towards zero</strong>: The model is saturating. All completions are achieving similar rewards, and the learning signal is vanishing. This is the most common GRPO failure mode.</li>
<li><strong>Reward std too high and not declining</strong>: The model isn’t learning to prefer high-reward completions. Check your reward function and learning rate.</li>
<li><strong>Sudden drops in reward std</strong>: Often indicates the model has found a “shortcut” – a simple pattern that achieves reasonable reward without genuine task understanding.</li>
</ul>
</section>
<section id="instrument-2-partial-reward-decomposition" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-2-partial-reward-decomposition">Instrument 2: partial reward decomposition</h3>
<p>If you’re using composite reward functions (and you probably should be), monitoring the decomposition of rewards into their components is essential. Different reward components can have wildly different dynamics, and aggregating them hides critical information.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Composite rewards are increasingly common in GRPO training. A typical setup might combine format compliance (does the output match the expected structure?), correctness (is the answer right?), and style (is it appropriately concise?). Each component can behave differently during training, and almost always, models learn them one by one.</p>
</div></div><div id="359e0ce7" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Partial reward decomposition monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PartialRewardCallback(TrainerCallback):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor individual reward components in composite reward functions."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, reward_components: <span class="bu">list</span>[<span class="bu">str</span>], window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward_components <span class="op">=</span> reward_components</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.component_histories <span class="op">=</span> {</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            comp: deque(maxlen<span class="op">=</span>window_size) <span class="cf">for</span> comp <span class="kw">in</span> reward_components</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> component <span class="kw">in</span> <span class="va">self</span>.reward_components:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            key <span class="op">=</span> <span class="ss">f"reward/</span><span class="sc">{</span>component<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            std_key <span class="op">=</span> <span class="ss">f"reward/</span><span class="sc">{</span>component<span class="sc">}</span><span class="ss">_std"</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">in</span> logs:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>                value <span class="op">=</span> logs[key]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.component_histories[component].append(value)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.component_histories[component]) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                    values <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.component_histories[component])</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Trend for this component</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">/trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">range</span>(<span class="bu">len</span>(values)), values, <span class="dv">1</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                    )[<span class="dv">0</span>]</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Also track component-specific std if available</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> std_key <span class="kw">in</span> logs:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                comp_std <span class="op">=</span> logs[std_key]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Warn if any component has collapsed variance</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> comp_std <span class="op">&lt;</span> <span class="fl">0.01</span>:</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="ss">f"</span><span class="sc">{</span>std_key<span class="sc">}</span><span class="ss">/warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"WARNING: </span><span class="sc">{</span>component<span class="sc">}</span><span class="ss"> reward has near-zero variance. "</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                          <span class="st">"This component is no longer providing learning signal."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>A common failure pattern: one reward component saturates early (e.g., format compliance reaches 100%) while others are still learning. The saturated component contributes zero variance, effectively reducing your reward dimensionality. If that component had high weight, it can drag down the overall learning signal significantly.</p>
<div id="cell-fig-partial-rewards" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-partial-rewards" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-partial-rewards-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-partial-rewards-output-1.png" width="951" height="567" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-partial-rewards-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Partial reward decomposition reveals dynamics hidden by aggregate metrics.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="instrument-3-completion-length-dynamics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-3-completion-length-dynamics">Instrument 3: completion length dynamics</h3>
<p>We covered generation length in Part II, but it deserves special attention for GRPO because length manipulation is the easiest form of reward hacking, and GRPO’s group-relative structure makes it particularly susceptible.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Length hacking in GRPO often manifests differently than in PPO. Instead of uniformly longer or shorter outputs, you might see <em>bimodal</em> length distributions within groups – the model learns that either very short or very long completions tend to score better than medium-length ones.</p>
</div></div><p>The key metrics in GRPO’s TRL implementation are:</p>
<ul>
<li><code>completions/mean_length</code>: Average tokens across all completions</li>
<li><code>completions/mean_terminated_length</code>: Average length of properly terminated completions (those ending with EOS)</li>
<li><code>completions/clipped_ratio</code>: Fraction of completions truncated at <code>max_completion_length</code></li>
</ul>
<p>But for GRPO, you should also track:</p>
<ul>
<li><strong>Within-group length variance</strong>: Are completions for the same prompt similar lengths, or diverse?</li>
<li><strong>Length-reward correlation</strong>: Is there a systematic relationship between completion length and reward?</li>
</ul>
<div id="914fd351" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>GRPO-specific completion length monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GRPOLengthCallback(TrainerCallback):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor completion length dynamics specific to GRPO training."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.variance_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Track mean length</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"completions/mean_length"</span> <span class="kw">in</span> logs:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            length <span class="op">=</span> logs[<span class="st">"completions/mean_length"</span>]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.length_history.append(length)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.length_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>                lengths <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.length_history)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Length trend</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"completions/length_trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(lengths)), lengths, <span class="dv">1</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Length stability (variance of lengths over time)</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"completions/length_stability"</span>] <span class="op">=</span> np.std(lengths)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Track within-group variance if available</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"completions/length_std"</span> <span class="kw">in</span> logs:</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            var <span class="op">=</span> logs[<span class="st">"completions/length_std"</span>]</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.variance_history.append(var)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.variance_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                variances <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.variance_history)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Is within-group diversity collapsing?</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"completions/diversity_trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(variances)), variances, <span class="dv">1</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Warn on low diversity</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> var <span class="op">&lt;</span> <span class="dv">10</span>:  <span class="co"># Less than 10 token std within groups</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"completions/diversity_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"WARNING: Low within-group length diversity (</span><span class="sc">{</span>var<span class="sc">:.1f}</span><span class="ss"> tokens). "</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>                          <span class="st">"Completions may be converging to templates."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="cell-fig-length-dynamics" class="cell" data-fig-height="8" data-fig-width="10" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-length-dynamics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-length-dynamics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-length-dynamics-output-1.png" width="952" height="759" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-length-dynamics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Completion length dynamics in GRPO: watch for bimodal distributions and collapsing within-group variance.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Within-group variance collapsing</strong>: Completions for each prompt are becoming similar lengths. This often precedes content mode collapse.</li>
<li><strong>Bimodal length distribution</strong>: The model has learned that extremes (very short or very long) score better than moderate lengths. Investigate your reward function.</li>
<li><strong>Systematic length drift</strong>: Mean length trending strongly up or down. Almost always indicates reward hacking.</li>
<li><strong>High clipped ratio</strong>: Many completions hitting <code>max_completion_length</code>. Either increase the limit or investigate why the model wants to generate such long outputs.</li>
<li><strong>Gap between <code>mean_length</code> and <code>mean_terminated_length</code></strong>: A large gap indicates many completions are being truncated before natural termination. This is a sign that your <code>max_completion_length</code> is too restrictive for your task.</li>
</ul>
<section id="length-truncation-and-its-discontents" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="length-truncation-and-its-discontents">Length truncation and its discontents</h4>
<p>The <code>completions/clipped_ratio</code> metric deserves special attention because truncation creates subtle training artifacts. When a completion is cut off at <code>max_completion_length</code>, you’re not just limiting output length – you’re potentially:</p>
<ol type="1">
<li><p><strong>Scoring incomplete thoughts</strong>: The reward function sees a truncated response and scores it, but the model never “finished” that generation. You’re training on artifacts of the length limit rather than genuine model behaviour.</p></li>
<li><p><strong>Biasing advantage estimates</strong>: If longer completions tend to score differently (higher or lower) than shorter ones, systematic truncation distorts the reward distribution within groups.</p></li>
<li><p><strong>Creating perverse incentives</strong>: If truncated completions score poorly, the model learns to avoid truncation by staying short – which may not be what you want. Conversely, if they score well (perhaps by avoiding mistakes that would have come later), the model learns that getting cut off is fine.</p></li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p>The interaction between truncation and reward can be insidious. Consider a coding task where correctness is checked by execution: a truncated function might not compile at all, receiving a zero reward, even if the model was heading in the right direction. The model learns “shorter is safer” rather than “correct is better”.</p>
</div></div><p>A useful diagnostic: compare the reward distribution for truncated versus naturally-terminated completions. If they differ significantly, your truncation threshold is affecting your training signal.</p>
<p>To be quite clear—unlike certain pathological signals in traditional ML, these do not mean “your model is broken beyond repair.” They may often be an indication that your model has either found a shortcut (don’t want) or has run out of learning signal (need to adjust). In either case, you can often recover by adjusting your reward function, hyperparameters, or training setup. Or, y’know, call it a day.</p>
</section>
</section>
<section id="instrument-4-the-two-faces-of-clipping" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-4-the-two-faces-of-clipping">Instrument 4: the two faces of clipping</h3>
<p>“Clipping” in GRPO refers to two distinct mechanisms that serve different purposes and require separate monitoring.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Conflating them is a common source of confusion.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Completion length truncation is sometimes called “clipping” as well, but it’s fundamentally different – it’s about generation limits, not optimisation constraints. We covered it in Instrument 3.</p></div></div><p><strong>Ratio clipping</strong> (inherited from PPO) constrains the probability ratio <span class="math inline">\(r_{i,t}\)</span> to the trust region <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span>. This prevents the policy from changing too drastically in a single update. When people talk about “clip fraction” in PPO/GRPO, they usually mean this.</p>
<p><strong>Reward clipping/scaling</strong> controls how extreme reward values affect the advantage computation. In TRL’s GRPO implementation, the <code>scale_rewards</code> parameter determines whether rewards are normalised per-group (the standard GRPO behaviour) or left raw. Additionally, explicit reward clipping can bound reward magnitudes to prevent outliers from dominating gradients.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>TRL logs three ratio clip metrics: <code>clip_ratio/region_mean</code> (fraction within the trust region), <code>clip_ratio/low_mean</code> (fraction clipped below), and <code>clip_ratio/high_mean</code> (fraction clipped above). The asymmetry between low and high clipping is often diagnostic. Reward clipping, when enabled, is typically logged separately.</p>
</div></div><p>Let’s examine each in turn.</p>
<section id="ratio-clipping" class="level4">
<h4 class="anchored" data-anchor-id="ratio-clipping">Ratio clipping</h4>
<p>The probability ratio <span class="math inline">\(r_{i,t} = \frac{\pi_\theta(o_{i,t} | q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t} | q, o_{i,&lt;t})}\)</span> measures how much more (or less) likely the current policy makes each token compared to the policy that generated the completion. Clipping constrains this ratio to <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span>.</p>
<div id="72d0fefa" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>GRPO clip ratio monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GRPOClipRatioCallback(TrainerCallback):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor clipping behaviour in GRPO training."""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.high_clip_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.low_clip_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        high_clip <span class="op">=</span> logs.get(<span class="st">"clip_ratio/high_mean"</span>, <span class="dv">0</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        low_clip <span class="op">=</span> logs.get(<span class="st">"clip_ratio/low_mean"</span>, <span class="dv">0</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> high_clip <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">or</span> low_clip <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.high_clip_history.append(high_clip)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.low_clip_history.append(low_clip)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute asymmetry</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            total_clip <span class="op">=</span> high_clip <span class="op">+</span> low_clip</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> total_clip <span class="op">&gt;</span> <span class="fl">0.01</span>:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"clip_ratio/asymmetry"</span>] <span class="op">=</span> (high_clip <span class="op">-</span> low_clip) <span class="op">/</span> total_clip</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Trend analysis</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.high_clip_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"clip_ratio/high_trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.high_clip_history)),</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">list</span>(<span class="va">self</span>.high_clip_history), <span class="dv">1</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"clip_ratio/low_trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.low_clip_history)),</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">list</span>(<span class="va">self</span>.low_clip_history), <span class="dv">1</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Warnings</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> total_clip <span class="op">&gt;</span> <span class="fl">0.4</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"clip_ratio/excessive_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"WARNING: Excessive clipping (</span><span class="sc">{</span>total_clip<span class="sc">:.1%}</span><span class="ss">). "</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"Learning rate may be too high."</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> total_clip <span class="op">&lt;</span> <span class="fl">0.01</span>:</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"clip_ratio/minimal_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"WARNING: Minimal clipping (</span><span class="sc">{</span>total_clip<span class="sc">:.1%}</span><span class="ss">). "</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"Policy updates may be too conservative."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>The key insight for GRPO is that <strong>asymmetric clipping is diagnostic</strong>:</p>
<ul>
<li><strong>High clipping dominates</strong>: The policy is becoming much more confident in tokens it already preferred. This can be healthy (the model is learning strong preferences) or pathological (the model is collapsing to a narrow mode).</li>
<li><strong>Low clipping dominates</strong>: The policy is becoming less confident in tokens it previously liked. This often indicates the model is “unlearning” – either intentionally (correcting mistakes) or problematically (forgetting good behaviour).</li>
<li><strong>Balanced clipping</strong>: Updates are aggressive but not systematically biased. Usually healthy if total clipping is moderate (10-30%).</li>
</ul>
<div id="cell-fig-clip-asymmetry" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="9">
<div class="cell-output cell-output-display">
<div id="fig-clip-asymmetry" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clip-asymmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-clip-asymmetry-output-1.png" width="943" height="375" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clip-asymmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Clip ratio asymmetry in GRPO: the balance between high and low clipping tells you how the policy is shifting.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="reward-clipping-and-scaling" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="reward-clipping-and-scaling">Reward clipping and scaling</h4>
<p>Reward clipping operates at a different level entirely. Where ratio clipping constrains <em>how much the policy can change</em>, reward clipping constrains <em>how much any single completion can influence that change</em>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The interaction between reward scaling and advantage normalisation is subtle. When <code>scale_rewards=True</code> (the default), rewards are normalised per-group before advantage computation. When <code>scale_rewards=False</code>, raw rewards are used, which can lead to very different gradient magnitudes depending on your reward function’s scale.</p>
</div></div><p>In GRPO, the standard behaviour is to normalise advantages within each group:</p>
<p><span class="math display">\[\hat{A}_i = \frac{r_i - \mu_G}{\sigma_G}\]</span></p>
<p>where <span class="math inline">\(\mu_G\)</span> and <span class="math inline">\(\sigma_G\)</span> are the mean and standard deviation of rewards within the group. This normalisation is itself a form of “soft” reward scaling – it makes the absolute magnitude of rewards irrelevant, only their relative ordering within the group matters.</p>
<p>But what happens when <span class="math inline">\(\sigma_G \approx 0\)</span>? Division by near-zero creates numerical instability and enormous gradients. TRL handles this by adding a small epsilon to the denominator, but the fundamental problem remains: groups with low reward variance produce unreliable advantage estimates.</p>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Reward variance across groups</strong>: If some groups have high variance and others near-zero, your effective batch is smaller than you think. Only the high-variance groups are contributing meaningful gradients.</li>
<li><strong>Reward magnitude drift</strong>: If you’re using <code>scale_rewards=False</code> or have explicit reward clipping, watch for the raw reward distribution shifting over training. A reward function that worked well early may need recalibration as the model improves.</li>
<li><strong>Clipped reward fraction</strong>: If you’re using explicit reward bounds (e.g., clipping rewards to <span class="math inline">\([-1, 1]\)</span>), track how often clipping is triggered. High clipping rates indicate your reward function’s dynamic range exceeds your bounds.</li>
</ul>
</section>
</section>
<section id="instrument-5-entropy-dynamics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-5-entropy-dynamics">Instrument 5: entropy dynamics</h3>
<p>Policy entropy measures how “spread out” the probability distribution is over possible tokens. High entropy means the model is uncertain; low entropy means it’s confident. In GRPO, entropy dynamics are particularly diagnostic because the group-relative objective can push entropy in unintuitive directions.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>TRL logs <code>entropy</code> as the mean entropy across tokens and completions. Some implementations also log <code>entropy_std</code>, which measures variation in entropy across tokens – useful for detecting whether the model is uniformly confident or has “confident” and “uncertain” regions.</p>
</div></div><div id="b15ce4d5" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Policy entropy monitoring</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EntropyCallback(TrainerCallback):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Monitor policy entropy for mode collapse detection."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>, collapse_threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.collapse_threshold <span class="op">=</span> collapse_threshold</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.entropy_history <span class="op">=</span> deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_log(<span class="va">self</span>, args, state, control, logs<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> logs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"entropy"</span> <span class="kw">in</span> logs:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            entropy <span class="op">=</span> logs[<span class="st">"entropy"</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.entropy_history.append(entropy)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.entropy_history) <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>                entropies <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.entropy_history)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Trend</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"entropy/trend"</span>] <span class="op">=</span> np.polyfit(</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">range</span>(<span class="bu">len</span>(entropies)), entropies, <span class="dv">1</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>                )[<span class="dv">0</span>]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Rate of decline (acceleration)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(entropies) <span class="op">&gt;=</span> <span class="dv">20</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                    first_half <span class="op">=</span> np.mean(entropies[:<span class="bu">len</span>(entropies)<span class="op">//</span><span class="dv">2</span>])</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>                    second_half <span class="op">=</span> np.mean(entropies[<span class="bu">len</span>(entropies)<span class="op">//</span><span class="dv">2</span>:])</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                    logs[<span class="st">"entropy/acceleration"</span>] <span class="op">=</span> second_half <span class="op">-</span> first_half</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Warning for low entropy</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> entropy <span class="op">&lt;</span> <span class="va">self</span>.collapse_threshold:</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                logs[<span class="st">"entropy/collapse_warning"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"WARNING: Low entropy (</span><span class="sc">{</span>entropy<span class="sc">:.3f}</span><span class="ss">). "</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"Policy may be collapsing to deterministic outputs."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>Entropy declining too fast</strong>: The model is becoming too confident too quickly. This often precedes mode collapse. Consider adding an entropy bonus to the reward.</li>
<li><strong>Entropy not declining at all</strong>: The model isn’t learning strong preferences. Check that your reward signal has meaningful variance.</li>
<li><strong>Entropy suddenly spiking</strong>: Often indicates the model has “forgotten” what it learned and is reverting to more uncertain behaviour. Check for gradient issues or data anomalies.</li>
</ul>
</section>
<section id="instrument-6-kl-divergence" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="instrument-6-kl-divergence">Instrument 6: KL divergence</h3>
<p>I covered KL divergence extensively in Part II, but it deserves revisiting in the GRPO context.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>KL divergence is only logged when <code>beta &gt; 0</code> in your GRPOConfig. If you’re running without a KL penalty (beta = 0), you’re flying without this instrument – which is risky for long training runs.</p>
</div></div><p>The KL divergence measures how far your policy has drifted from the reference model. In GRPO, this serves as a crucial stability indicator: even when reward and other metrics look healthy, runaway KL can signal that your model is heading towards reward hacking or capability degradation.</p>
<p><span class="math display">\[D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) = \mathbb{E}_{x \sim \pi_\theta}\left[\log \frac{\pi_\theta(x)}{\pi_{\text{ref}}(x)}\right]\]</span></p>
<p>The GRPO loss includes a KL penalty term scaled by <code>beta</code>:</p>
<p><span class="math display">\[\mathcal{L}_{\text{GRPO}} = \mathcal{L}_{\text{policy}} + \beta \cdot D_{\text{KL}}\]</span></p>
<p>This penalty pushes the policy back towards the reference, preventing it from drifting too far in pursuit of reward.</p>
<p><strong>What to watch for:</strong></p>
<ul>
<li><strong>KL growing steadily without bound</strong>: The policy is drifting further from the reference with each update. If your reward is also increasing, this is classic overoptimisation – the model is exploiting the reward function rather than genuinely improving. Increase <code>beta</code> or reduce learning rate.</li>
<li><strong>KL near zero throughout training</strong>: Your <code>beta</code> is too high, or the learning rate is too low. The policy is barely moving from the reference. Reduce <code>beta</code> to allow more exploration.</li>
<li><strong>KL spiking suddenly</strong>: A batch of data caused a large policy shift. Investigate what’s different about that batch. This can indicate data quality issues or reward function bugs.</li>
<li><strong>KL and reward moving in opposite directions</strong>: If KL increases while reward decreases (or vice versa), the KL penalty may be fighting the reward signal. Consider retuning <code>beta</code>.</li>
</ul>
<p>The relationship between KL and reward is subtle. Some KL increase is expected and healthy – you <em>want</em> the policy to change from the reference, that’s the whole point of training. The question is whether the change is productive (aligned with genuine improvement) or pathological (exploiting reward model weaknesses).</p>
</section>
</section>
<section id="putting-it-together-the-grpo-dashboard" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-together-the-grpo-dashboard">Putting it together: the GRPO dashboard</h2>
<p>Based on these instruments, here’s my recommended layout for a GRPO monitoring dashboard. This aligns with TRL’s documented “crucial values” while incorporating the GRPO-specific insights we’ve discussed:</p>
<p><strong>Primary panel (watch constantly):</strong></p>
<ol type="1">
<li>Reward standard deviation (your main learning signal health indicator)</li>
<li>Mean reward (primary objective, but interpret with caution)</li>
<li>KL divergence (stability indicator, crucial when beta &gt; 0)</li>
</ol>
<p><strong>Secondary panel (check regularly):</strong></p>
<ol start="4" type="1">
<li>Completion length and within-group variance</li>
<li>Clip ratios (ratio clipping asymmetry)</li>
<li>Entropy (exploration vs exploitation balance)</li>
</ol>
<p><strong>Tertiary panel (investigate on anomalies):</strong></p>
<ol start="7" type="1">
<li>Partial reward components (if using composite rewards)</li>
<li>Loss (only for detecting numerical issues – not a progress metric!)</li>
<li>Gradient statistics (standard SFT instruments)</li>
</ol>
<div id="cell-fig-grpo-dashboard" class="cell" data-fig-height="10" data-fig-width="11" data-execution_count="11">
<div class="cell-output cell-output-display">
<div id="fig-grpo-dashboard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grpo-dashboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-grpo-dashboard-output-1.png" width="886" height="810" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grpo-dashboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: A complete GRPO monitoring dashboard: reward std is primary, loss is relegated to anomaly detection.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="practical-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="practical-recommendations">Practical recommendations</h2>
<p>Let me close with concrete heuristics for GRPO monitoring:</p>
<ol type="1">
<li><p><strong>Set alerts on reward std, not loss.</strong> If reward std drops below 0.05 (or your task-specific threshold), something is wrong. Either saturation, mode collapse, or a bug.</p></li>
<li><p><strong>Monitor partial rewards separately.</strong> If using composite rewards, set individual thresholds for each component’s variance. A component with zero variance is dead weight.</p></li>
<li><p><strong>Watch for length-reward correlation.</strong> If you see completion length systematically trending, investigate whether your reward function has a length bias. Consider adding explicit length penalties or normalisation.</p></li>
<li><p><strong>Track clip ratio asymmetry.</strong> Balanced clipping (roughly equal high and low) is healthy. Strong asymmetry indicates the policy is moving decisively in one direction – which might be good (learning) or bad (collapsing).</p></li>
<li><p><strong>Sample completions regularly.</strong> No amount of metrics substitutes for actually reading what the model outputs. Schedule periodic human review of generated completions, especially when metrics show anomalies.</p></li>
<li><p><strong>Don’t trust the loss.</strong> I’ve said it before, but it bears repeating. The GRPO loss is not a progress metric. Stop watching it trend. Look at it only for sudden discontinuities that indicate numerical problems.</p></li>
</ol>
<p>GRPO represents a significant shift in how we think about policy optimisation, and that shift requires corresponding changes in how we monitor training. The intuitions from supervised learning – and even from simpler RL methods like PPO – can actively mislead you here. Build new intuitions around reward variance, completion diversity and clip dynamics, and you’ll catch problems that would otherwise only manifest at evaluation time. As in, by which time you’ve wasted a few tens of thousands of dollars on compute.</p>
<hr>
<p>I hope this has been a useful deep dive into GRPO monitoring! As always, the best way to learn is by doing – set up these instruments in your own training runs and see what insights you can uncover. Happy training!</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{csefalvay2026,
  author = {{Chris von Csefalvay}},
  title = {The Post-Training Instrument Cluster -\/- {Part} {III}},
  date = {2026-01-04},
  url = {https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/},
  langid = {en-GB}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-csefalvay2026" class="csl-entry quarto-appendix-citeas" role="listitem">
Chris von Csefalvay. 2026. <span>“The Post-Training Instrument Cluster
-- Part III.”</span> <a href="https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/">https://chrisvoncsefalvay.com/posts/post-training-instrument-cluster-grpo/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrisvoncsefalvay\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<ol start="3" type="a">
<li>Chris von Csefalvay, 2011–. <a href="disclaimer">Disclaimer</a></li>
</ol>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>